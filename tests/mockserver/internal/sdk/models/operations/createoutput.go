// Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.

package operations

import (
	"encoding/json"
	"errors"
	"fmt"
	"mockserver/internal/sdk/models/components"
	"mockserver/internal/sdk/utils"
)

type TypeSentinelOneAiSiem string

const (
	TypeSentinelOneAiSiemSentinelOneAiSiem TypeSentinelOneAiSiem = "sentinel_one_ai_siem"
)

func (e TypeSentinelOneAiSiem) ToPointer() *TypeSentinelOneAiSiem {
	return &e
}
func (e *TypeSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sentinel_one_ai_siem":
		*e = TypeSentinelOneAiSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSentinelOneAiSiem: %v", v)
	}
}

// RegionSentinelOneAiSiem - The SentinelOne region to send events to. In most cases you can find the region by either looking at your SentinelOne URL or knowing what geographic region your SentinelOne instance is contained in.
type RegionSentinelOneAiSiem string

const (
	RegionSentinelOneAiSiemUs     RegionSentinelOneAiSiem = "US"
	RegionSentinelOneAiSiemCa     RegionSentinelOneAiSiem = "CA"
	RegionSentinelOneAiSiemEmea   RegionSentinelOneAiSiem = "EMEA"
	RegionSentinelOneAiSiemAp     RegionSentinelOneAiSiem = "AP"
	RegionSentinelOneAiSiemAps    RegionSentinelOneAiSiem = "APS"
	RegionSentinelOneAiSiemAu     RegionSentinelOneAiSiem = "AU"
	RegionSentinelOneAiSiemCustom RegionSentinelOneAiSiem = "Custom"
)

func (e RegionSentinelOneAiSiem) ToPointer() *RegionSentinelOneAiSiem {
	return &e
}
func (e *RegionSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "US":
		fallthrough
	case "CA":
		fallthrough
	case "EMEA":
		fallthrough
	case "AP":
		fallthrough
	case "APS":
		fallthrough
	case "AU":
		fallthrough
	case "Custom":
		*e = RegionSentinelOneAiSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RegionSentinelOneAiSiem: %v", v)
	}
}

// AISIEMEndpointPath - Regional endpoint used to send events to, such as /services/collector/event or /services/collector/raw
type AISIEMEndpointPath string

const (
	AISIEMEndpointPathRootServicesCollectorEvent AISIEMEndpointPath = "/services/collector/event"
	AISIEMEndpointPathRootServicesCollectorRaw   AISIEMEndpointPath = "/services/collector/raw"
)

func (e AISIEMEndpointPath) ToPointer() *AISIEMEndpointPath {
	return &e
}
func (e *AISIEMEndpointPath) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "/services/collector/event":
		fallthrough
	case "/services/collector/raw":
		*e = AISIEMEndpointPath(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AISIEMEndpointPath: %v", v)
	}
}

type ExtraHTTPHeaderSentinelOneAiSiem struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderSentinelOneAiSiem) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderSentinelOneAiSiem) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeSentinelOneAiSiem - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSentinelOneAiSiem string

const (
	FailedRequestLoggingModeSentinelOneAiSiemPayload           FailedRequestLoggingModeSentinelOneAiSiem = "payload"
	FailedRequestLoggingModeSentinelOneAiSiemPayloadAndHeaders FailedRequestLoggingModeSentinelOneAiSiem = "payloadAndHeaders"
	FailedRequestLoggingModeSentinelOneAiSiemNone              FailedRequestLoggingModeSentinelOneAiSiem = "none"
)

func (e FailedRequestLoggingModeSentinelOneAiSiem) ToPointer() *FailedRequestLoggingModeSentinelOneAiSiem {
	return &e
}
func (e *FailedRequestLoggingModeSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeSentinelOneAiSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeSentinelOneAiSiem: %v", v)
	}
}

// AuthenticationMethodSentinelOneAiSiem - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodSentinelOneAiSiem string

const (
	AuthenticationMethodSentinelOneAiSiemManual AuthenticationMethodSentinelOneAiSiem = "manual"
	AuthenticationMethodSentinelOneAiSiemSecret AuthenticationMethodSentinelOneAiSiem = "secret"
)

func (e AuthenticationMethodSentinelOneAiSiem) ToPointer() *AuthenticationMethodSentinelOneAiSiem {
	return &e
}
func (e *AuthenticationMethodSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodSentinelOneAiSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodSentinelOneAiSiem: %v", v)
	}
}

type ResponseRetrySettingSentinelOneAiSiem struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSentinelOneAiSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingSentinelOneAiSiem) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingSentinelOneAiSiem) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingSentinelOneAiSiem) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingSentinelOneAiSiem) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsSentinelOneAiSiem struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSentinelOneAiSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsSentinelOneAiSiem) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsSentinelOneAiSiem) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsSentinelOneAiSiem) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsSentinelOneAiSiem) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorSentinelOneAiSiem - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSentinelOneAiSiem string

const (
	BackpressureBehaviorSentinelOneAiSiemBlock BackpressureBehaviorSentinelOneAiSiem = "block"
	BackpressureBehaviorSentinelOneAiSiemDrop  BackpressureBehaviorSentinelOneAiSiem = "drop"
	BackpressureBehaviorSentinelOneAiSiemQueue BackpressureBehaviorSentinelOneAiSiem = "queue"
)

func (e BackpressureBehaviorSentinelOneAiSiem) ToPointer() *BackpressureBehaviorSentinelOneAiSiem {
	return &e
}
func (e *BackpressureBehaviorSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSentinelOneAiSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSentinelOneAiSiem: %v", v)
	}
}

// CompressionSentinelOneAiSiem - Codec to use to compress the persisted data
type CompressionSentinelOneAiSiem string

const (
	CompressionSentinelOneAiSiemNone CompressionSentinelOneAiSiem = "none"
	CompressionSentinelOneAiSiemGzip CompressionSentinelOneAiSiem = "gzip"
)

func (e CompressionSentinelOneAiSiem) ToPointer() *CompressionSentinelOneAiSiem {
	return &e
}
func (e *CompressionSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionSentinelOneAiSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionSentinelOneAiSiem: %v", v)
	}
}

// QueueFullBehaviorSentinelOneAiSiem - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSentinelOneAiSiem string

const (
	QueueFullBehaviorSentinelOneAiSiemBlock QueueFullBehaviorSentinelOneAiSiem = "block"
	QueueFullBehaviorSentinelOneAiSiemDrop  QueueFullBehaviorSentinelOneAiSiem = "drop"
)

func (e QueueFullBehaviorSentinelOneAiSiem) ToPointer() *QueueFullBehaviorSentinelOneAiSiem {
	return &e
}
func (e *QueueFullBehaviorSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSentinelOneAiSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSentinelOneAiSiem: %v", v)
	}
}

// ModeSentinelOneAiSiem - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSentinelOneAiSiem string

const (
	ModeSentinelOneAiSiemError        ModeSentinelOneAiSiem = "error"
	ModeSentinelOneAiSiemBackpressure ModeSentinelOneAiSiem = "backpressure"
	ModeSentinelOneAiSiemAlways       ModeSentinelOneAiSiem = "always"
)

func (e ModeSentinelOneAiSiem) ToPointer() *ModeSentinelOneAiSiem {
	return &e
}
func (e *ModeSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeSentinelOneAiSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeSentinelOneAiSiem: %v", v)
	}
}

type PqControlsSentinelOneAiSiem struct {
}

type OutputSentinelOneAiSiem struct {
	// Unique ID for this output
	ID   string                 `json:"id"`
	Type *TypeSentinelOneAiSiem `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The SentinelOne region to send events to. In most cases you can find the region by either looking at your SentinelOne URL or knowing what geographic region your SentinelOne instance is contained in.
	Region *RegionSentinelOneAiSiem `default:"US" json:"region"`
	// Regional endpoint used to send events to, such as /services/collector/event or /services/collector/raw
	Endpoint *AISIEMEndpointPath `default:"/services/collector/event" json:"endpoint"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"5120" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"5" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderSentinelOneAiSiem `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSentinelOneAiSiem `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodSentinelOneAiSiem `default:"manual" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingSentinelOneAiSiem `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSentinelOneAiSiem  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSentinelOneAiSiem `default:"block" json:"onBackpressure"`
	Description    *string                                `json:"description,omitempty"`
	// In the SentinelOne Console select Policy & Settings then select the Singularity AI SIEM section, API Keys will be at the bottom. Under Log Access Keys select a Write token and copy it here
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Base URL of the endpoint used to send events to, such as https://<Your-S1-Tenant>.sentinelone.net. Must begin with http:// or https://, can include a port number, and no trailing slashes. Matches pattern: ^https?://[a-zA-Z0-9.-]+(:[0-9]+)?$.
	BaseURL *string `default:"https://<Your-S1-Tenant>.sentinelone.net" json:"baseUrl"`
	// Define serverHost for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myServer').
	HostExpression *string `default:"__e.host || C.os.hostname()" json:"hostExpression"`
	// Define logFile for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myLogFile.txt').
	SourceExpression *string `default:"__e.source || (__e.__criblMetrics ? 'metrics' : 'cribl')" json:"sourceExpression"`
	// Define the parser for events using a JavaScript expression. This value helps parse data into AI SIEM. You must enclose text constants in quotes (such as, 'dottedJson'). For custom parsers, substitute 'dottedJson' with your parser's name.
	SourceTypeExpression *string `default:"__e.sourcetype || 'dottedJson'" json:"sourceTypeExpression"`
	// Define the dataSource.category for events using a JavaScript expression. This value helps categorize data and helps enable extra features in SentinelOne AI SIEM. You must enclose text constants in quotes. The default value is 'security'.
	DataSourceCategoryExpression *string `default:"'security'" json:"dataSourceCategoryExpression"`
	// Define the dataSource.name for events using a JavaScript expression. This value should reflect the type of data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'networkActivity' or 'authLogs').
	DataSourceNameExpression *string `default:"__e.__dataSourceName || 'cribl'" json:"dataSourceNameExpression"`
	// Define the dataSource.vendor for events using a JavaScript expression. This value should reflect the vendor of the data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'Cisco' or 'Microsoft').
	DataSourceVendorExpression *string `default:"__e.__dataSourceVendor || 'cribl'" json:"dataSourceVendorExpression"`
	// Optionally, define the event.type for events using a JavaScript expression. This value acts as a label, grouping events into meaningful categories. You must enclose text constants in quotes (such as, 'Process Creation' or 'Network Connection').
	EventTypeExpression *string `default:"" json:"eventTypeExpression"`
	// Define the serverHost for events using a JavaScript expression. This value will be passed to AI SIEM. You must enclose text constants in quotes (such as, 'myServerName').
	Host *string `default:"C.os.hostname()" json:"host"`
	// Specify the logFile value to pass as a parameter to SentinelOne AI SIEM. Don't quote this value. The default is cribl.
	Source *string `default:"cribl" json:"source"`
	// Specify the sourcetype parameter for SentinelOne AI SIEM, which determines the parser. Don't quote this value. For custom parsers, substitute hecRawParser with your parser's name. The default is hecRawParser.
	SourceType *string `default:"hecRawParser" json:"sourceType"`
	// Specify the dataSource.category value to pass as a parameter to SentinelOne AI SIEM. This value helps categorize data and enables additional features. Don't quote this value. The default is security.
	DataSourceCategory *string `default:"security" json:"dataSourceCategory"`
	// Specify the dataSource.name value to pass as a parameter to AI SIEM. This value should reflect the type of data being inserted. Don't quote this value. The default is cribl.
	DataSourceName *string `default:"cribl" json:"dataSourceName"`
	// Specify the dataSource.vendorvalue to pass as a parameter to AI SIEM. This value should reflect the vendor of the data being inserted. Don't quote this value. The default is cribl.
	DataSourceVendor *string `default:"cribl" json:"dataSourceVendor"`
	// Specify the event.type value to pass as an optional parameter to AI SIEM. This value acts as a label, grouping events into meaningful categories like Process Creation, File Modification, or Network Connection. Don't quote this value. By default, this field is empty.
	EventType *string `default:"" json:"eventType"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSentinelOneAiSiem `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSentinelOneAiSiem `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeSentinelOneAiSiem       `default:"error" json:"pqMode"`
	PqControls *PqControlsSentinelOneAiSiem `json:"pqControls,omitempty"`
}

func (o OutputSentinelOneAiSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSentinelOneAiSiem) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSentinelOneAiSiem) GetType() *TypeSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputSentinelOneAiSiem) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSentinelOneAiSiem) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSentinelOneAiSiem) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSentinelOneAiSiem) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSentinelOneAiSiem) GetRegion() *RegionSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputSentinelOneAiSiem) GetEndpoint() *AISIEMEndpointPath {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputSentinelOneAiSiem) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSentinelOneAiSiem) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSentinelOneAiSiem) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSentinelOneAiSiem) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSentinelOneAiSiem) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSentinelOneAiSiem) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSentinelOneAiSiem) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSentinelOneAiSiem) GetExtraHTTPHeaders() []ExtraHTTPHeaderSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSentinelOneAiSiem) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSentinelOneAiSiem) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSentinelOneAiSiem) GetAuthType() *AuthenticationMethodSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSentinelOneAiSiem) GetResponseRetrySettings() []ResponseRetrySettingSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSentinelOneAiSiem) GetTimeoutRetrySettings() *TimeoutRetrySettingsSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSentinelOneAiSiem) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSentinelOneAiSiem) GetOnBackpressure() *BackpressureBehaviorSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSentinelOneAiSiem) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSentinelOneAiSiem) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputSentinelOneAiSiem) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputSentinelOneAiSiem) GetBaseURL() *string {
	if o == nil {
		return nil
	}
	return o.BaseURL
}

func (o *OutputSentinelOneAiSiem) GetHostExpression() *string {
	if o == nil {
		return nil
	}
	return o.HostExpression
}

func (o *OutputSentinelOneAiSiem) GetSourceExpression() *string {
	if o == nil {
		return nil
	}
	return o.SourceExpression
}

func (o *OutputSentinelOneAiSiem) GetSourceTypeExpression() *string {
	if o == nil {
		return nil
	}
	return o.SourceTypeExpression
}

func (o *OutputSentinelOneAiSiem) GetDataSourceCategoryExpression() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceCategoryExpression
}

func (o *OutputSentinelOneAiSiem) GetDataSourceNameExpression() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceNameExpression
}

func (o *OutputSentinelOneAiSiem) GetDataSourceVendorExpression() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceVendorExpression
}

func (o *OutputSentinelOneAiSiem) GetEventTypeExpression() *string {
	if o == nil {
		return nil
	}
	return o.EventTypeExpression
}

func (o *OutputSentinelOneAiSiem) GetHost() *string {
	if o == nil {
		return nil
	}
	return o.Host
}

func (o *OutputSentinelOneAiSiem) GetSource() *string {
	if o == nil {
		return nil
	}
	return o.Source
}

func (o *OutputSentinelOneAiSiem) GetSourceType() *string {
	if o == nil {
		return nil
	}
	return o.SourceType
}

func (o *OutputSentinelOneAiSiem) GetDataSourceCategory() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceCategory
}

func (o *OutputSentinelOneAiSiem) GetDataSourceName() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceName
}

func (o *OutputSentinelOneAiSiem) GetDataSourceVendor() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceVendor
}

func (o *OutputSentinelOneAiSiem) GetEventType() *string {
	if o == nil {
		return nil
	}
	return o.EventType
}

func (o *OutputSentinelOneAiSiem) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSentinelOneAiSiem) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSentinelOneAiSiem) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSentinelOneAiSiem) GetPqCompress() *CompressionSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSentinelOneAiSiem) GetPqOnBackpressure() *QueueFullBehaviorSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSentinelOneAiSiem) GetPqMode() *ModeSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSentinelOneAiSiem) GetPqControls() *PqControlsSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeDynatraceOtlp string

const (
	TypeDynatraceOtlpDynatraceOtlp TypeDynatraceOtlp = "dynatrace_otlp"
)

func (e TypeDynatraceOtlp) ToPointer() *TypeDynatraceOtlp {
	return &e
}
func (e *TypeDynatraceOtlp) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dynatrace_otlp":
		*e = TypeDynatraceOtlp(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDynatraceOtlp: %v", v)
	}
}

// ProtocolHTTP - Select a transport option for Dynatrace
type ProtocolHTTP string

const (
	ProtocolHTTPHTTP ProtocolHTTP = "http"
)

func (e ProtocolHTTP) ToPointer() *ProtocolHTTP {
	return &e
}
func (e *ProtocolHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "http":
		*e = ProtocolHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ProtocolHTTP: %v", v)
	}
}

// OTLPVersionHTTP - The version of OTLP Protobuf definitions to use when structuring data to send
type OTLPVersionHTTP string

const (
	OTLPVersionHTTPOneDot3Dot1 OTLPVersionHTTP = "1.3.1"
)

func (e OTLPVersionHTTP) ToPointer() *OTLPVersionHTTP {
	return &e
}
func (e *OTLPVersionHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "1.3.1":
		*e = OTLPVersionHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OTLPVersionHTTP: %v", v)
	}
}

// CreateOutputCompressCompressionHTTP - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type CreateOutputCompressCompressionHTTP string

const (
	CreateOutputCompressCompressionHTTPNone    CreateOutputCompressCompressionHTTP = "none"
	CreateOutputCompressCompressionHTTPDeflate CreateOutputCompressCompressionHTTP = "deflate"
	CreateOutputCompressCompressionHTTPGzip    CreateOutputCompressCompressionHTTP = "gzip"
)

func (e CreateOutputCompressCompressionHTTP) ToPointer() *CreateOutputCompressCompressionHTTP {
	return &e
}
func (e *CreateOutputCompressCompressionHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "deflate":
		fallthrough
	case "gzip":
		*e = CreateOutputCompressCompressionHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputCompressCompressionHTTP: %v", v)
	}
}

// HTTPCompressCompressionHTTP - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type HTTPCompressCompressionHTTP string

const (
	HTTPCompressCompressionHTTPNone HTTPCompressCompressionHTTP = "none"
	HTTPCompressCompressionHTTPGzip HTTPCompressCompressionHTTP = "gzip"
)

func (e HTTPCompressCompressionHTTP) ToPointer() *HTTPCompressCompressionHTTP {
	return &e
}
func (e *HTTPCompressCompressionHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = HTTPCompressCompressionHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for HTTPCompressCompressionHTTP: %v", v)
	}
}

type CreateOutputMetadatumHTTP struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (c CreateOutputMetadatumHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputMetadatumHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputMetadatumHTTP) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *CreateOutputMetadatumHTTP) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeHTTP - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeHTTP string

const (
	FailedRequestLoggingModeHTTPPayload           FailedRequestLoggingModeHTTP = "payload"
	FailedRequestLoggingModeHTTPPayloadAndHeaders FailedRequestLoggingModeHTTP = "payloadAndHeaders"
	FailedRequestLoggingModeHTTPNone              FailedRequestLoggingModeHTTP = "none"
)

func (e FailedRequestLoggingModeHTTP) ToPointer() *FailedRequestLoggingModeHTTP {
	return &e
}
func (e *FailedRequestLoggingModeHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeHTTP: %v", v)
	}
}

// EndpointType - Select the type of Dynatrace endpoint configured
type EndpointType string

const (
	EndpointTypeSaas EndpointType = "saas"
	EndpointTypeAg   EndpointType = "ag"
)

func (e EndpointType) ToPointer() *EndpointType {
	return &e
}
func (e *EndpointType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "saas":
		fallthrough
	case "ag":
		*e = EndpointType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for EndpointType: %v", v)
	}
}

// BackpressureBehaviorHTTP - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorHTTP string

const (
	BackpressureBehaviorHTTPBlock BackpressureBehaviorHTTP = "block"
	BackpressureBehaviorHTTPDrop  BackpressureBehaviorHTTP = "drop"
	BackpressureBehaviorHTTPQueue BackpressureBehaviorHTTP = "queue"
)

func (e BackpressureBehaviorHTTP) ToPointer() *BackpressureBehaviorHTTP {
	return &e
}
func (e *BackpressureBehaviorHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorHTTP: %v", v)
	}
}

type ExtraHTTPHeaderHTTP struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderHTTP) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderHTTP) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type ResponseRetrySettingHTTP struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingHTTP) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingHTTP) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingHTTP) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingHTTP) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsHTTP struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsHTTP) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsHTTP) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsHTTP) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsHTTP) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// PqCompressCompressionHTTP - Codec to use to compress the persisted data
type PqCompressCompressionHTTP string

const (
	PqCompressCompressionHTTPNone PqCompressCompressionHTTP = "none"
	PqCompressCompressionHTTPGzip PqCompressCompressionHTTP = "gzip"
)

func (e PqCompressCompressionHTTP) ToPointer() *PqCompressCompressionHTTP {
	return &e
}
func (e *PqCompressCompressionHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionHTTP: %v", v)
	}
}

// QueueFullBehaviorHTTP - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorHTTP string

const (
	QueueFullBehaviorHTTPBlock QueueFullBehaviorHTTP = "block"
	QueueFullBehaviorHTTPDrop  QueueFullBehaviorHTTP = "drop"
)

func (e QueueFullBehaviorHTTP) ToPointer() *QueueFullBehaviorHTTP {
	return &e
}
func (e *QueueFullBehaviorHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorHTTP: %v", v)
	}
}

// CreateOutputModeHTTP - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeHTTP string

const (
	CreateOutputModeHTTPError        CreateOutputModeHTTP = "error"
	CreateOutputModeHTTPBackpressure CreateOutputModeHTTP = "backpressure"
	CreateOutputModeHTTPAlways       CreateOutputModeHTTP = "always"
)

func (e CreateOutputModeHTTP) ToPointer() *CreateOutputModeHTTP {
	return &e
}
func (e *CreateOutputModeHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = CreateOutputModeHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputModeHTTP: %v", v)
	}
}

type PqControlsHTTP struct {
}

type OutputDynatraceOtlp struct {
	// Unique ID for this output
	ID   string             `json:"id"`
	Type *TypeDynatraceOtlp `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select a transport option for Dynatrace
	Protocol *ProtocolHTTP `default:"http" json:"protocol"`
	// The endpoint where Dynatrace events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)
	Endpoint *string `default:"https://{your-environment-id}.live.dynatrace.com/api/v2/otlp" json:"endpoint"`
	// The version of OTLP Protobuf definitions to use when structuring data to send
	OtlpVersion *OTLPVersionHTTP `default:"1.3.1" json:"otlpVersion"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	Compress *CreateOutputCompressCompressionHTTP `default:"gzip" json:"compress"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	HTTPCompress *HTTPCompressCompressionHTTP `default:"gzip" json:"httpCompress"`
	// If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPTracesEndpointOverride *string `json:"httpTracesEndpointOverride,omitempty"`
	// If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPMetricsEndpointOverride *string `json:"httpMetricsEndpointOverride,omitempty"`
	// If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPLogsEndpointOverride *string `json:"httpLogsEndpointOverride,omitempty"`
	// List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.
	Metadata []CreateOutputMetadatumHTTP `json:"metadata,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size (in KB) of the request body. The maximum payload size is 4 MB. If this limit is exceeded, the entire OTLP message is dropped
	MaxPayloadSizeKB *float64 `default:"2048" json:"maxPayloadSizeKB"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeHTTP `default:"none" json:"failedRequestLoggingMode"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// How often the sender should ping the peer to keep the connection open
	KeepAliveTime *float64 `default:"30" json:"keepAliveTime"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Select the type of Dynatrace endpoint configured
	EndpointType *EndpointType `default:"saas" json:"endpointType"`
	// Select or create a stored text secret
	TokenSecret   string  `json:"tokenSecret"`
	AuthTokenName *string `default:"Authorization" json:"authTokenName"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorHTTP `default:"block" json:"onBackpressure"`
	Description    *string                   `json:"description,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderHTTP `json:"extraHttpHeaders,omitempty"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingHTTP `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsHTTP  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionHTTP `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorHTTP `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *CreateOutputModeHTTP `default:"error" json:"pqMode"`
	PqControls *PqControlsHTTP       `json:"pqControls,omitempty"`
}

func (o OutputDynatraceOtlp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDynatraceOtlp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputDynatraceOtlp) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDynatraceOtlp) GetType() *TypeDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputDynatraceOtlp) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDynatraceOtlp) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDynatraceOtlp) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDynatraceOtlp) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDynatraceOtlp) GetProtocol() *ProtocolHTTP {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputDynatraceOtlp) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputDynatraceOtlp) GetOtlpVersion() *OTLPVersionHTTP {
	if o == nil {
		return nil
	}
	return o.OtlpVersion
}

func (o *OutputDynatraceOtlp) GetCompress() *CreateOutputCompressCompressionHTTP {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDynatraceOtlp) GetHTTPCompress() *HTTPCompressCompressionHTTP {
	if o == nil {
		return nil
	}
	return o.HTTPCompress
}

func (o *OutputDynatraceOtlp) GetHTTPTracesEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPTracesEndpointOverride
}

func (o *OutputDynatraceOtlp) GetHTTPMetricsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPMetricsEndpointOverride
}

func (o *OutputDynatraceOtlp) GetHTTPLogsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPLogsEndpointOverride
}

func (o *OutputDynatraceOtlp) GetMetadata() []CreateOutputMetadatumHTTP {
	if o == nil {
		return nil
	}
	return o.Metadata
}

func (o *OutputDynatraceOtlp) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputDynatraceOtlp) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputDynatraceOtlp) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDynatraceOtlp) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputDynatraceOtlp) GetFailedRequestLoggingMode() *FailedRequestLoggingModeHTTP {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputDynatraceOtlp) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputDynatraceOtlp) GetKeepAliveTime() *float64 {
	if o == nil {
		return nil
	}
	return o.KeepAliveTime
}

func (o *OutputDynatraceOtlp) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputDynatraceOtlp) GetEndpointType() *EndpointType {
	if o == nil {
		return nil
	}
	return o.EndpointType
}

func (o *OutputDynatraceOtlp) GetTokenSecret() string {
	if o == nil {
		return ""
	}
	return o.TokenSecret
}

func (o *OutputDynatraceOtlp) GetAuthTokenName() *string {
	if o == nil {
		return nil
	}
	return o.AuthTokenName
}

func (o *OutputDynatraceOtlp) GetOnBackpressure() *BackpressureBehaviorHTTP {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDynatraceOtlp) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDynatraceOtlp) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDynatraceOtlp) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputDynatraceOtlp) GetExtraHTTPHeaders() []ExtraHTTPHeaderHTTP {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputDynatraceOtlp) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputDynatraceOtlp) GetResponseRetrySettings() []ResponseRetrySettingHTTP {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputDynatraceOtlp) GetTimeoutRetrySettings() *TimeoutRetrySettingsHTTP {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputDynatraceOtlp) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputDynatraceOtlp) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputDynatraceOtlp) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputDynatraceOtlp) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputDynatraceOtlp) GetPqCompress() *PqCompressCompressionHTTP {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputDynatraceOtlp) GetPqOnBackpressure() *QueueFullBehaviorHTTP {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputDynatraceOtlp) GetPqMode() *CreateOutputModeHTTP {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputDynatraceOtlp) GetPqControls() *PqControlsHTTP {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeDynatraceHTTP string

const (
	TypeDynatraceHTTPDynatraceHTTP TypeDynatraceHTTP = "dynatrace_http"
)

func (e TypeDynatraceHTTP) ToPointer() *TypeDynatraceHTTP {
	return &e
}
func (e *TypeDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dynatrace_http":
		*e = TypeDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDynatraceHTTP: %v", v)
	}
}

// Method - The method to use when sending events
type Method string

const (
	MethodPost  Method = "POST"
	MethodPut   Method = "PUT"
	MethodPatch Method = "PATCH"
)

func (e Method) ToPointer() *Method {
	return &e
}
func (e *Method) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "POST":
		fallthrough
	case "PUT":
		fallthrough
	case "PATCH":
		*e = Method(v)
		return nil
	default:
		return fmt.Errorf("invalid value for Method: %v", v)
	}
}

type ExtraHTTPHeaderDynatraceHTTP struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderDynatraceHTTP) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderDynatraceHTTP) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeDynatraceHTTP - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeDynatraceHTTP string

const (
	FailedRequestLoggingModeDynatraceHTTPPayload           FailedRequestLoggingModeDynatraceHTTP = "payload"
	FailedRequestLoggingModeDynatraceHTTPPayloadAndHeaders FailedRequestLoggingModeDynatraceHTTP = "payloadAndHeaders"
	FailedRequestLoggingModeDynatraceHTTPNone              FailedRequestLoggingModeDynatraceHTTP = "none"
)

func (e FailedRequestLoggingModeDynatraceHTTP) ToPointer() *FailedRequestLoggingModeDynatraceHTTP {
	return &e
}
func (e *FailedRequestLoggingModeDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeDynatraceHTTP: %v", v)
	}
}

type ResponseRetrySettingDynatraceHTTP struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingDynatraceHTTP) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingDynatraceHTTP) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingDynatraceHTTP) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingDynatraceHTTP) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsDynatraceHTTP struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsDynatraceHTTP) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsDynatraceHTTP) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsDynatraceHTTP) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsDynatraceHTTP) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorDynatraceHTTP - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorDynatraceHTTP string

const (
	BackpressureBehaviorDynatraceHTTPBlock BackpressureBehaviorDynatraceHTTP = "block"
	BackpressureBehaviorDynatraceHTTPDrop  BackpressureBehaviorDynatraceHTTP = "drop"
	BackpressureBehaviorDynatraceHTTPQueue BackpressureBehaviorDynatraceHTTP = "queue"
)

func (e BackpressureBehaviorDynatraceHTTP) ToPointer() *BackpressureBehaviorDynatraceHTTP {
	return &e
}
func (e *BackpressureBehaviorDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorDynatraceHTTP: %v", v)
	}
}

type AuthenticationTypeDynatraceHTTP string

const (
	AuthenticationTypeDynatraceHTTPToken      AuthenticationTypeDynatraceHTTP = "token"
	AuthenticationTypeDynatraceHTTPTextSecret AuthenticationTypeDynatraceHTTP = "textSecret"
)

func (e AuthenticationTypeDynatraceHTTP) ToPointer() *AuthenticationTypeDynatraceHTTP {
	return &e
}
func (e *AuthenticationTypeDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "token":
		fallthrough
	case "textSecret":
		*e = AuthenticationTypeDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationTypeDynatraceHTTP: %v", v)
	}
}

// FormatDynatraceHTTP - How to format events before sending. Defaults to JSON. Plaintext is not currently supported.
type FormatDynatraceHTTP string

const (
	FormatDynatraceHTTPJSONArray FormatDynatraceHTTP = "json_array"
	FormatDynatraceHTTPPlaintext FormatDynatraceHTTP = "plaintext"
)

func (e FormatDynatraceHTTP) ToPointer() *FormatDynatraceHTTP {
	return &e
}
func (e *FormatDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json_array":
		fallthrough
	case "plaintext":
		*e = FormatDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FormatDynatraceHTTP: %v", v)
	}
}

type Endpoint string

const (
	EndpointCloud      Endpoint = "cloud"
	EndpointActiveGate Endpoint = "activeGate"
	EndpointManual     Endpoint = "manual"
)

func (e Endpoint) ToPointer() *Endpoint {
	return &e
}
func (e *Endpoint) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cloud":
		fallthrough
	case "activeGate":
		fallthrough
	case "manual":
		*e = Endpoint(v)
		return nil
	default:
		return fmt.Errorf("invalid value for Endpoint: %v", v)
	}
}

type TelemetryType string

const (
	TelemetryTypeLogs    TelemetryType = "logs"
	TelemetryTypeMetrics TelemetryType = "metrics"
)

func (e TelemetryType) ToPointer() *TelemetryType {
	return &e
}
func (e *TelemetryType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "logs":
		fallthrough
	case "metrics":
		*e = TelemetryType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TelemetryType: %v", v)
	}
}

// CompressionDynatraceHTTP - Codec to use to compress the persisted data
type CompressionDynatraceHTTP string

const (
	CompressionDynatraceHTTPNone CompressionDynatraceHTTP = "none"
	CompressionDynatraceHTTPGzip CompressionDynatraceHTTP = "gzip"
)

func (e CompressionDynatraceHTTP) ToPointer() *CompressionDynatraceHTTP {
	return &e
}
func (e *CompressionDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionDynatraceHTTP: %v", v)
	}
}

// QueueFullBehaviorDynatraceHTTP - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorDynatraceHTTP string

const (
	QueueFullBehaviorDynatraceHTTPBlock QueueFullBehaviorDynatraceHTTP = "block"
	QueueFullBehaviorDynatraceHTTPDrop  QueueFullBehaviorDynatraceHTTP = "drop"
)

func (e QueueFullBehaviorDynatraceHTTP) ToPointer() *QueueFullBehaviorDynatraceHTTP {
	return &e
}
func (e *QueueFullBehaviorDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorDynatraceHTTP: %v", v)
	}
}

// ModeDynatraceHTTP - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeDynatraceHTTP string

const (
	ModeDynatraceHTTPError        ModeDynatraceHTTP = "error"
	ModeDynatraceHTTPBackpressure ModeDynatraceHTTP = "backpressure"
	ModeDynatraceHTTPAlways       ModeDynatraceHTTP = "always"
)

func (e ModeDynatraceHTTP) ToPointer() *ModeDynatraceHTTP {
	return &e
}
func (e *ModeDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeDynatraceHTTP: %v", v)
	}
}

type PqControlsDynatraceHTTP struct {
}

type OutputDynatraceHTTP struct {
	// Unique ID for this output
	ID   string             `json:"id"`
	Type *TypeDynatraceHTTP `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The method to use when sending events
	Method *Method `default:"POST" json:"method"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).
	ExtraHTTPHeaders []ExtraHTTPHeaderDynatraceHTTP `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeDynatraceHTTP `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingDynatraceHTTP `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsDynatraceHTTP  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorDynatraceHTTP `default:"block" json:"onBackpressure"`
	AuthType       *AuthenticationTypeDynatraceHTTP   `default:"token" json:"authType"`
	// How to format events before sending. Defaults to JSON. Plaintext is not currently supported.
	Format        *FormatDynatraceHTTP `default:"json_array" json:"format"`
	Endpoint      *Endpoint            `default:"cloud" json:"endpoint"`
	TelemetryType *TelemetryType       `default:"logs" json:"telemetryType"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionDynatraceHTTP `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorDynatraceHTTP `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeDynatraceHTTP       `default:"error" json:"pqMode"`
	PqControls *PqControlsDynatraceHTTP `json:"pqControls,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// ID of the environment to send to
	EnvironmentID *string `json:"environmentId,omitempty"`
	// ActiveGate domain with Log analytics collector module enabled. For example https://{activeGate-domain}:9999/e/{environment-id}/api/v2/logs/ingest.
	ActiveGateDomain *string `json:"activeGateDomain,omitempty"`
	// URL to send events to. Can be overwritten by an event's __url field.
	URL *string `json:"url,omitempty"`
}

func (o OutputDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputDynatraceHTTP) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDynatraceHTTP) GetType() *TypeDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputDynatraceHTTP) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDynatraceHTTP) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDynatraceHTTP) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDynatraceHTTP) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDynatraceHTTP) GetMethod() *Method {
	if o == nil {
		return nil
	}
	return o.Method
}

func (o *OutputDynatraceHTTP) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputDynatraceHTTP) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputDynatraceHTTP) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputDynatraceHTTP) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputDynatraceHTTP) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDynatraceHTTP) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDynatraceHTTP) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDynatraceHTTP) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputDynatraceHTTP) GetExtraHTTPHeaders() []ExtraHTTPHeaderDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputDynatraceHTTP) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputDynatraceHTTP) GetFailedRequestLoggingMode() *FailedRequestLoggingModeDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputDynatraceHTTP) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputDynatraceHTTP) GetResponseRetrySettings() []ResponseRetrySettingDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputDynatraceHTTP) GetTimeoutRetrySettings() *TimeoutRetrySettingsDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputDynatraceHTTP) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputDynatraceHTTP) GetOnBackpressure() *BackpressureBehaviorDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDynatraceHTTP) GetAuthType() *AuthenticationTypeDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputDynatraceHTTP) GetFormat() *FormatDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDynatraceHTTP) GetEndpoint() *Endpoint {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputDynatraceHTTP) GetTelemetryType() *TelemetryType {
	if o == nil {
		return nil
	}
	return o.TelemetryType
}

func (o *OutputDynatraceHTTP) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputDynatraceHTTP) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDynatraceHTTP) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputDynatraceHTTP) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputDynatraceHTTP) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputDynatraceHTTP) GetPqCompress() *CompressionDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputDynatraceHTTP) GetPqOnBackpressure() *QueueFullBehaviorDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputDynatraceHTTP) GetPqMode() *ModeDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputDynatraceHTTP) GetPqControls() *PqControlsDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputDynatraceHTTP) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputDynatraceHTTP) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputDynatraceHTTP) GetEnvironmentID() *string {
	if o == nil {
		return nil
	}
	return o.EnvironmentID
}

func (o *OutputDynatraceHTTP) GetActiveGateDomain() *string {
	if o == nil {
		return nil
	}
	return o.ActiveGateDomain
}

func (o *OutputDynatraceHTTP) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

type CreateOutputTypeNetflow string

const (
	CreateOutputTypeNetflowNetflow CreateOutputTypeNetflow = "netflow"
)

func (e CreateOutputTypeNetflow) ToPointer() *CreateOutputTypeNetflow {
	return &e
}
func (e *CreateOutputTypeNetflow) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "netflow":
		*e = CreateOutputTypeNetflow(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeNetflow: %v", v)
	}
}

type HostNetflow struct {
	// Destination host
	Host string `json:"host"`
	// Destination port, default is 2055
	Port *float64 `default:"2055" json:"port"`
}

func (h HostNetflow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostNetflow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *HostNetflow) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *HostNetflow) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

type OutputNetflow struct {
	// Unique ID for this output
	ID   string                  `json:"id"`
	Type CreateOutputTypeNetflow `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// One or more NetFlow destinations to forward events to
	Hosts []HostNetflow `json:"hosts"`
	// How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every datagram sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string  `json:"description,omitempty"`
}

func (o OutputNetflow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputNetflow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputNetflow) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputNetflow) GetType() CreateOutputTypeNetflow {
	if o == nil {
		return CreateOutputTypeNetflow("")
	}
	return o.Type
}

func (o *OutputNetflow) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputNetflow) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputNetflow) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputNetflow) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputNetflow) GetHosts() []HostNetflow {
	if o == nil {
		return []HostNetflow{}
	}
	return o.Hosts
}

func (o *OutputNetflow) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputNetflow) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

type TypeClickHouse string

const (
	TypeClickHouseClickHouse TypeClickHouse = "click_house"
)

func (e TypeClickHouse) ToPointer() *TypeClickHouse {
	return &e
}
func (e *TypeClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "click_house":
		*e = TypeClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeClickHouse: %v", v)
	}
}

type AuthenticationTypeClickHouse string

const (
	AuthenticationTypeClickHouseNone               AuthenticationTypeClickHouse = "none"
	AuthenticationTypeClickHouseBasic              AuthenticationTypeClickHouse = "basic"
	AuthenticationTypeClickHouseCredentialsSecret  AuthenticationTypeClickHouse = "credentialsSecret"
	AuthenticationTypeClickHouseSslUserCertificate AuthenticationTypeClickHouse = "sslUserCertificate"
	AuthenticationTypeClickHouseToken              AuthenticationTypeClickHouse = "token"
	AuthenticationTypeClickHouseTextSecret         AuthenticationTypeClickHouse = "textSecret"
	AuthenticationTypeClickHouseOauth              AuthenticationTypeClickHouse = "oauth"
)

func (e AuthenticationTypeClickHouse) ToPointer() *AuthenticationTypeClickHouse {
	return &e
}
func (e *AuthenticationTypeClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "basic":
		fallthrough
	case "credentialsSecret":
		fallthrough
	case "sslUserCertificate":
		fallthrough
	case "token":
		fallthrough
	case "textSecret":
		fallthrough
	case "oauth":
		*e = AuthenticationTypeClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationTypeClickHouse: %v", v)
	}
}

// FormatClickHouse - Data format to use when sending data to ClickHouse. Defaults to JSON Compact.
type FormatClickHouse string

const (
	FormatClickHouseJSONCompactEachRowWithNames FormatClickHouse = "json-compact-each-row-with-names"
	FormatClickHouseJSONEachRow                 FormatClickHouse = "json-each-row"
)

func (e FormatClickHouse) ToPointer() *FormatClickHouse {
	return &e
}
func (e *FormatClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json-compact-each-row-with-names":
		fallthrough
	case "json-each-row":
		*e = FormatClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FormatClickHouse: %v", v)
	}
}

// MappingType - How event fields are mapped to ClickHouse columns.
type MappingType string

const (
	MappingTypeAutomatic MappingType = "automatic"
	MappingTypeCustom    MappingType = "custom"
)

func (e MappingType) ToPointer() *MappingType {
	return &e
}
func (e *MappingType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "automatic":
		fallthrough
	case "custom":
		*e = MappingType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MappingType: %v", v)
	}
}

type MinimumTLSVersionClickHouse string

const (
	MinimumTLSVersionClickHouseTlSv1  MinimumTLSVersionClickHouse = "TLSv1"
	MinimumTLSVersionClickHouseTlSv11 MinimumTLSVersionClickHouse = "TLSv1.1"
	MinimumTLSVersionClickHouseTlSv12 MinimumTLSVersionClickHouse = "TLSv1.2"
	MinimumTLSVersionClickHouseTlSv13 MinimumTLSVersionClickHouse = "TLSv1.3"
)

func (e MinimumTLSVersionClickHouse) ToPointer() *MinimumTLSVersionClickHouse {
	return &e
}
func (e *MinimumTLSVersionClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MinimumTLSVersionClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MinimumTLSVersionClickHouse: %v", v)
	}
}

type MaximumTLSVersionClickHouse string

const (
	MaximumTLSVersionClickHouseTlSv1  MaximumTLSVersionClickHouse = "TLSv1"
	MaximumTLSVersionClickHouseTlSv11 MaximumTLSVersionClickHouse = "TLSv1.1"
	MaximumTLSVersionClickHouseTlSv12 MaximumTLSVersionClickHouse = "TLSv1.2"
	MaximumTLSVersionClickHouseTlSv13 MaximumTLSVersionClickHouse = "TLSv1.3"
)

func (e MaximumTLSVersionClickHouse) ToPointer() *MaximumTLSVersionClickHouse {
	return &e
}
func (e *MaximumTLSVersionClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MaximumTLSVersionClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MaximumTLSVersionClickHouse: %v", v)
	}
}

type TLSSettingsClientSideClickHouse struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                      `json:"passphrase,omitempty"`
	MinVersion *MinimumTLSVersionClickHouse `json:"minVersion,omitempty"`
	MaxVersion *MaximumTLSVersionClickHouse `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSideClickHouse) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSideClickHouse) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *TLSSettingsClientSideClickHouse) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *TLSSettingsClientSideClickHouse) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *TLSSettingsClientSideClickHouse) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *TLSSettingsClientSideClickHouse) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *TLSSettingsClientSideClickHouse) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *TLSSettingsClientSideClickHouse) GetMinVersion() *MinimumTLSVersionClickHouse {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *TLSSettingsClientSideClickHouse) GetMaxVersion() *MaximumTLSVersionClickHouse {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

type ExtraHTTPHeaderClickHouse struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderClickHouse) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderClickHouse) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeClickHouse - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeClickHouse string

const (
	FailedRequestLoggingModeClickHousePayload           FailedRequestLoggingModeClickHouse = "payload"
	FailedRequestLoggingModeClickHousePayloadAndHeaders FailedRequestLoggingModeClickHouse = "payloadAndHeaders"
	FailedRequestLoggingModeClickHouseNone              FailedRequestLoggingModeClickHouse = "none"
)

func (e FailedRequestLoggingModeClickHouse) ToPointer() *FailedRequestLoggingModeClickHouse {
	return &e
}
func (e *FailedRequestLoggingModeClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeClickHouse: %v", v)
	}
}

type ResponseRetrySettingClickHouse struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingClickHouse) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingClickHouse) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingClickHouse) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingClickHouse) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsClickHouse struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsClickHouse) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsClickHouse) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsClickHouse) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsClickHouse) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorClickHouse - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorClickHouse string

const (
	BackpressureBehaviorClickHouseBlock BackpressureBehaviorClickHouse = "block"
	BackpressureBehaviorClickHouseDrop  BackpressureBehaviorClickHouse = "drop"
	BackpressureBehaviorClickHouseQueue BackpressureBehaviorClickHouse = "queue"
)

func (e BackpressureBehaviorClickHouse) ToPointer() *BackpressureBehaviorClickHouse {
	return &e
}
func (e *BackpressureBehaviorClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorClickHouse: %v", v)
	}
}

type OauthParamClickHouse struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o *OauthParamClickHouse) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthParamClickHouse) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OauthHeaderClickHouse struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o *OauthHeaderClickHouse) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthHeaderClickHouse) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type ColumnMapping struct {
	// Name of the column in ClickHouse that will store field value
	ColumnName string `json:"columnName"`
	// Type of the column in the ClickHouse database
	ColumnType *string `json:"columnType,omitempty"`
	// JavaScript expression to compute value to be inserted into ClickHouse table
	ColumnValueExpression string `json:"columnValueExpression"`
}

func (o *ColumnMapping) GetColumnName() string {
	if o == nil {
		return ""
	}
	return o.ColumnName
}

func (o *ColumnMapping) GetColumnType() *string {
	if o == nil {
		return nil
	}
	return o.ColumnType
}

func (o *ColumnMapping) GetColumnValueExpression() string {
	if o == nil {
		return ""
	}
	return o.ColumnValueExpression
}

// CompressionClickHouse - Codec to use to compress the persisted data
type CompressionClickHouse string

const (
	CompressionClickHouseNone CompressionClickHouse = "none"
	CompressionClickHouseGzip CompressionClickHouse = "gzip"
)

func (e CompressionClickHouse) ToPointer() *CompressionClickHouse {
	return &e
}
func (e *CompressionClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionClickHouse: %v", v)
	}
}

// QueueFullBehaviorClickHouse - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorClickHouse string

const (
	QueueFullBehaviorClickHouseBlock QueueFullBehaviorClickHouse = "block"
	QueueFullBehaviorClickHouseDrop  QueueFullBehaviorClickHouse = "drop"
)

func (e QueueFullBehaviorClickHouse) ToPointer() *QueueFullBehaviorClickHouse {
	return &e
}
func (e *QueueFullBehaviorClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorClickHouse: %v", v)
	}
}

// ModeClickHouse - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeClickHouse string

const (
	ModeClickHouseError        ModeClickHouse = "error"
	ModeClickHouseBackpressure ModeClickHouse = "backpressure"
	ModeClickHouseAlways       ModeClickHouse = "always"
)

func (e ModeClickHouse) ToPointer() *ModeClickHouse {
	return &e
}
func (e *ModeClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeClickHouse: %v", v)
	}
}

type PqControlsClickHouse struct {
}

type OutputClickHouse struct {
	// Unique ID for this output
	ID   string          `json:"id"`
	Type *TypeClickHouse `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL of the ClickHouse instance. Example: http://localhost:8123/
	URL      string                        `json:"url"`
	AuthType *AuthenticationTypeClickHouse `default:"none" json:"authType"`
	Database string                        `json:"database"`
	// Name of the ClickHouse table where data will be inserted. Name can contain letters (A-Z, a-z), numbers (0-9), and the character "_", and must start with either a letter or the character "_".
	TableName string `json:"tableName"`
	// Data format to use when sending data to ClickHouse. Defaults to JSON Compact.
	Format *FormatClickHouse `default:"json-compact-each-row-with-names" json:"format"`
	// How event fields are mapped to ClickHouse columns.
	MappingType *MappingType `default:"automatic" json:"mappingType"`
	// Collect data into batches for later processing. Disable to write to a ClickHouse table immediately.
	AsyncInserts *bool                            `default:"false" json:"asyncInserts"`
	TLS          *TLSSettingsClientSideClickHouse `json:"tls,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderClickHouse `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeClickHouse `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingClickHouse `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsClickHouse  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Log the most recent event that fails to match the table schema
	DumpFormatErrorsToDisk *bool `default:"false" json:"dumpFormatErrorsToDisk"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorClickHouse `default:"block" json:"onBackpressure"`
	Description    *string                         `json:"description,omitempty"`
	Username       *string                         `json:"username,omitempty"`
	Password       *string                         `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OauthParamClickHouse `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []OauthHeaderClickHouse `json:"oauthHeaders,omitempty"`
	// Username for certificate authentication
	SQLUsername *string `json:"sqlUsername,omitempty"`
	// Cribl will wait for confirmation that data has been fully inserted into the ClickHouse database before proceeding. Disabling this option can increase throughput, but Cribl wont be able to verify data has been completely inserted.
	WaitForAsyncInserts *bool `default:"true" json:"waitForAsyncInserts"`
	// Fields to exclude from sending to ClickHouse
	ExcludeMappingFields []string `json:"excludeMappingFields,omitempty"`
	// Retrieves the table schema from ClickHouse and populates the Column Mapping table
	DescribeTable  *string         `json:"describeTable,omitempty"`
	ColumnMappings []ColumnMapping `json:"columnMappings,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionClickHouse `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorClickHouse `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeClickHouse       `default:"error" json:"pqMode"`
	PqControls *PqControlsClickHouse `json:"pqControls,omitempty"`
}

func (o OutputClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputClickHouse) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputClickHouse) GetType() *TypeClickHouse {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputClickHouse) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputClickHouse) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputClickHouse) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputClickHouse) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputClickHouse) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputClickHouse) GetAuthType() *AuthenticationTypeClickHouse {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputClickHouse) GetDatabase() string {
	if o == nil {
		return ""
	}
	return o.Database
}

func (o *OutputClickHouse) GetTableName() string {
	if o == nil {
		return ""
	}
	return o.TableName
}

func (o *OutputClickHouse) GetFormat() *FormatClickHouse {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputClickHouse) GetMappingType() *MappingType {
	if o == nil {
		return nil
	}
	return o.MappingType
}

func (o *OutputClickHouse) GetAsyncInserts() *bool {
	if o == nil {
		return nil
	}
	return o.AsyncInserts
}

func (o *OutputClickHouse) GetTLS() *TLSSettingsClientSideClickHouse {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputClickHouse) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputClickHouse) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputClickHouse) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputClickHouse) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputClickHouse) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputClickHouse) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputClickHouse) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputClickHouse) GetExtraHTTPHeaders() []ExtraHTTPHeaderClickHouse {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputClickHouse) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputClickHouse) GetFailedRequestLoggingMode() *FailedRequestLoggingModeClickHouse {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputClickHouse) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputClickHouse) GetResponseRetrySettings() []ResponseRetrySettingClickHouse {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputClickHouse) GetTimeoutRetrySettings() *TimeoutRetrySettingsClickHouse {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputClickHouse) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputClickHouse) GetDumpFormatErrorsToDisk() *bool {
	if o == nil {
		return nil
	}
	return o.DumpFormatErrorsToDisk
}

func (o *OutputClickHouse) GetOnBackpressure() *BackpressureBehaviorClickHouse {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputClickHouse) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputClickHouse) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputClickHouse) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputClickHouse) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputClickHouse) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputClickHouse) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputClickHouse) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputClickHouse) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputClickHouse) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputClickHouse) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputClickHouse) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputClickHouse) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputClickHouse) GetOauthParams() []OauthParamClickHouse {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputClickHouse) GetOauthHeaders() []OauthHeaderClickHouse {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

func (o *OutputClickHouse) GetSQLUsername() *string {
	if o == nil {
		return nil
	}
	return o.SQLUsername
}

func (o *OutputClickHouse) GetWaitForAsyncInserts() *bool {
	if o == nil {
		return nil
	}
	return o.WaitForAsyncInserts
}

func (o *OutputClickHouse) GetExcludeMappingFields() []string {
	if o == nil {
		return nil
	}
	return o.ExcludeMappingFields
}

func (o *OutputClickHouse) GetDescribeTable() *string {
	if o == nil {
		return nil
	}
	return o.DescribeTable
}

func (o *OutputClickHouse) GetColumnMappings() []ColumnMapping {
	if o == nil {
		return nil
	}
	return o.ColumnMappings
}

func (o *OutputClickHouse) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputClickHouse) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputClickHouse) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputClickHouse) GetPqCompress() *CompressionClickHouse {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputClickHouse) GetPqOnBackpressure() *QueueFullBehaviorClickHouse {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputClickHouse) GetPqMode() *ModeClickHouse {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputClickHouse) GetPqControls() *PqControlsClickHouse {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeSecurityLake string

const (
	CreateOutputTypeSecurityLakeSecurityLake CreateOutputTypeSecurityLake = "security_lake"
)

func (e CreateOutputTypeSecurityLake) ToPointer() *CreateOutputTypeSecurityLake {
	return &e
}
func (e *CreateOutputTypeSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "security_lake":
		*e = CreateOutputTypeSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeSecurityLake: %v", v)
	}
}

// CreateOutputAuthenticationMethodSecurityLake - AWS authentication method. Choose Auto to use IAM roles.
type CreateOutputAuthenticationMethodSecurityLake string

const (
	CreateOutputAuthenticationMethodSecurityLakeAuto   CreateOutputAuthenticationMethodSecurityLake = "auto"
	CreateOutputAuthenticationMethodSecurityLakeManual CreateOutputAuthenticationMethodSecurityLake = "manual"
	CreateOutputAuthenticationMethodSecurityLakeSecret CreateOutputAuthenticationMethodSecurityLake = "secret"
)

func (e CreateOutputAuthenticationMethodSecurityLake) ToPointer() *CreateOutputAuthenticationMethodSecurityLake {
	return &e
}
func (e *CreateOutputAuthenticationMethodSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = CreateOutputAuthenticationMethodSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputAuthenticationMethodSecurityLake: %v", v)
	}
}

// CreateOutputSignatureVersionSecurityLake - Signature version to use for signing Amazon Security Lake requests
type CreateOutputSignatureVersionSecurityLake string

const (
	CreateOutputSignatureVersionSecurityLakeV2 CreateOutputSignatureVersionSecurityLake = "v2"
	CreateOutputSignatureVersionSecurityLakeV4 CreateOutputSignatureVersionSecurityLake = "v4"
)

func (e CreateOutputSignatureVersionSecurityLake) ToPointer() *CreateOutputSignatureVersionSecurityLake {
	return &e
}
func (e *CreateOutputSignatureVersionSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = CreateOutputSignatureVersionSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputSignatureVersionSecurityLake: %v", v)
	}
}

// ObjectACLSecurityLake - Object ACL to assign to uploaded objects
type ObjectACLSecurityLake string

const (
	ObjectACLSecurityLakePrivate                ObjectACLSecurityLake = "private"
	ObjectACLSecurityLakePublicRead             ObjectACLSecurityLake = "public-read"
	ObjectACLSecurityLakePublicReadWrite        ObjectACLSecurityLake = "public-read-write"
	ObjectACLSecurityLakeAuthenticatedRead      ObjectACLSecurityLake = "authenticated-read"
	ObjectACLSecurityLakeAwsExecRead            ObjectACLSecurityLake = "aws-exec-read"
	ObjectACLSecurityLakeBucketOwnerRead        ObjectACLSecurityLake = "bucket-owner-read"
	ObjectACLSecurityLakeBucketOwnerFullControl ObjectACLSecurityLake = "bucket-owner-full-control"
)

func (e ObjectACLSecurityLake) ToPointer() *ObjectACLSecurityLake {
	return &e
}
func (e *ObjectACLSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "private":
		fallthrough
	case "public-read":
		fallthrough
	case "public-read-write":
		fallthrough
	case "authenticated-read":
		fallthrough
	case "aws-exec-read":
		fallthrough
	case "bucket-owner-read":
		fallthrough
	case "bucket-owner-full-control":
		*e = ObjectACLSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ObjectACLSecurityLake: %v", v)
	}
}

// StorageClassSecurityLake - Storage class to select for uploaded objects
type StorageClassSecurityLake string

const (
	StorageClassSecurityLakeStandard           StorageClassSecurityLake = "STANDARD"
	StorageClassSecurityLakeReducedRedundancy  StorageClassSecurityLake = "REDUCED_REDUNDANCY"
	StorageClassSecurityLakeStandardIa         StorageClassSecurityLake = "STANDARD_IA"
	StorageClassSecurityLakeOnezoneIa          StorageClassSecurityLake = "ONEZONE_IA"
	StorageClassSecurityLakeIntelligentTiering StorageClassSecurityLake = "INTELLIGENT_TIERING"
	StorageClassSecurityLakeGlacier            StorageClassSecurityLake = "GLACIER"
	StorageClassSecurityLakeGlacierIr          StorageClassSecurityLake = "GLACIER_IR"
	StorageClassSecurityLakeDeepArchive        StorageClassSecurityLake = "DEEP_ARCHIVE"
)

func (e StorageClassSecurityLake) ToPointer() *StorageClassSecurityLake {
	return &e
}
func (e *StorageClassSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "STANDARD":
		fallthrough
	case "REDUCED_REDUNDANCY":
		fallthrough
	case "STANDARD_IA":
		fallthrough
	case "ONEZONE_IA":
		fallthrough
	case "INTELLIGENT_TIERING":
		fallthrough
	case "GLACIER":
		fallthrough
	case "GLACIER_IR":
		fallthrough
	case "DEEP_ARCHIVE":
		*e = StorageClassSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for StorageClassSecurityLake: %v", v)
	}
}

type ServerSideEncryptionForUploadedObjectsSecurityLake string

const (
	ServerSideEncryptionForUploadedObjectsSecurityLakeAes256 ServerSideEncryptionForUploadedObjectsSecurityLake = "AES256"
	ServerSideEncryptionForUploadedObjectsSecurityLakeAwsKms ServerSideEncryptionForUploadedObjectsSecurityLake = "aws:kms"
)

func (e ServerSideEncryptionForUploadedObjectsSecurityLake) ToPointer() *ServerSideEncryptionForUploadedObjectsSecurityLake {
	return &e
}
func (e *ServerSideEncryptionForUploadedObjectsSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "AES256":
		fallthrough
	case "aws:kms":
		*e = ServerSideEncryptionForUploadedObjectsSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ServerSideEncryptionForUploadedObjectsSecurityLake: %v", v)
	}
}

// BackpressureBehaviorSecurityLake - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSecurityLake string

const (
	BackpressureBehaviorSecurityLakeBlock BackpressureBehaviorSecurityLake = "block"
	BackpressureBehaviorSecurityLakeDrop  BackpressureBehaviorSecurityLake = "drop"
)

func (e BackpressureBehaviorSecurityLake) ToPointer() *BackpressureBehaviorSecurityLake {
	return &e
}
func (e *BackpressureBehaviorSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSecurityLake: %v", v)
	}
}

// DiskSpaceProtectionSecurityLake - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionSecurityLake string

const (
	DiskSpaceProtectionSecurityLakeBlock DiskSpaceProtectionSecurityLake = "block"
	DiskSpaceProtectionSecurityLakeDrop  DiskSpaceProtectionSecurityLake = "drop"
)

func (e DiskSpaceProtectionSecurityLake) ToPointer() *DiskSpaceProtectionSecurityLake {
	return &e
}
func (e *DiskSpaceProtectionSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionSecurityLake: %v", v)
	}
}

// ParquetVersionSecurityLake - Determines which data types are supported and how they are represented
type ParquetVersionSecurityLake string

const (
	ParquetVersionSecurityLakeParquet10 ParquetVersionSecurityLake = "PARQUET_1_0"
	ParquetVersionSecurityLakeParquet24 ParquetVersionSecurityLake = "PARQUET_2_4"
	ParquetVersionSecurityLakeParquet26 ParquetVersionSecurityLake = "PARQUET_2_6"
)

func (e ParquetVersionSecurityLake) ToPointer() *ParquetVersionSecurityLake {
	return &e
}
func (e *ParquetVersionSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PARQUET_1_0":
		fallthrough
	case "PARQUET_2_4":
		fallthrough
	case "PARQUET_2_6":
		*e = ParquetVersionSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ParquetVersionSecurityLake: %v", v)
	}
}

// DataPageVersionSecurityLake - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionSecurityLake string

const (
	DataPageVersionSecurityLakeDataPageV1 DataPageVersionSecurityLake = "DATA_PAGE_V1"
	DataPageVersionSecurityLakeDataPageV2 DataPageVersionSecurityLake = "DATA_PAGE_V2"
)

func (e DataPageVersionSecurityLake) ToPointer() *DataPageVersionSecurityLake {
	return &e
}
func (e *DataPageVersionSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "DATA_PAGE_V1":
		fallthrough
	case "DATA_PAGE_V2":
		*e = DataPageVersionSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataPageVersionSecurityLake: %v", v)
	}
}

type KeyValueMetadatumSecurityLake struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumSecurityLake) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumSecurityLake) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *KeyValueMetadatumSecurityLake) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *KeyValueMetadatumSecurityLake) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputSecurityLake struct {
	// Unique ID for this output
	ID   string                        `json:"id"`
	Type *CreateOutputTypeSecurityLake `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket string `json:"bucket"`
	// Region where the Amazon Security Lake is located.
	Region       string  `json:"region"`
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *CreateOutputAuthenticationMethodSecurityLake `default:"auto" json:"awsAuthenticationMethod"`
	// Amazon Security Lake service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Amazon Security Lake-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing Amazon Security Lake requests
	SignatureVersion *CreateOutputSignatureVersionSecurityLake `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access S3
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn string `json:"assumeRoleArn"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLSecurityLake `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass         *StorageClassSecurityLake                           `json:"storageClass,omitempty"`
	ServerSideEncryption *ServerSideEncryptionForUploadedObjectsSecurityLake `json:"serverSideEncryption,omitempty"`
	// ID or ARN of the KMS customer-managed key to use for encryption
	KmsKeyID *string `json:"kmsKeyId,omitempty"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSecurityLake `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionSecurityLake `default:"block" json:"onDiskFullBackpressure"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Maximum number of files that can be waiting for upload before backpressure is applied
	MaxClosingFilesToBackpressure *float64 `default:"100" json:"maxClosingFilesToBackpressure"`
	// ID of the AWS account whose data the Destination will write to Security Lake. This should have been configured when creating the Amazon Security Lake custom source.
	AccountID string `json:"accountId"`
	// Name of the custom source configured in Amazon Security Lake
	CustomSource string `json:"customSource"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionSecurityLake `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionSecurityLake `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumSecurityLake `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool   `default:"false" json:"enablePageChecksum"`
	Description        *string `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputSecurityLake) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSecurityLake) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSecurityLake) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSecurityLake) GetType() *CreateOutputTypeSecurityLake {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputSecurityLake) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSecurityLake) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSecurityLake) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSecurityLake) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSecurityLake) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputSecurityLake) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputSecurityLake) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputSecurityLake) GetAwsAuthenticationMethod() *CreateOutputAuthenticationMethodSecurityLake {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputSecurityLake) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputSecurityLake) GetSignatureVersion() *CreateOutputSignatureVersionSecurityLake {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputSecurityLake) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputSecurityLake) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSecurityLake) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputSecurityLake) GetAssumeRoleArn() string {
	if o == nil {
		return ""
	}
	return o.AssumeRoleArn
}

func (o *OutputSecurityLake) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputSecurityLake) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputSecurityLake) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputSecurityLake) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputSecurityLake) GetObjectACL() *ObjectACLSecurityLake {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputSecurityLake) GetStorageClass() *StorageClassSecurityLake {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputSecurityLake) GetServerSideEncryption() *ServerSideEncryptionForUploadedObjectsSecurityLake {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputSecurityLake) GetKmsKeyID() *string {
	if o == nil {
		return nil
	}
	return o.KmsKeyID
}

func (o *OutputSecurityLake) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputSecurityLake) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputSecurityLake) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputSecurityLake) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputSecurityLake) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputSecurityLake) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputSecurityLake) GetOnBackpressure() *BackpressureBehaviorSecurityLake {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSecurityLake) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputSecurityLake) GetOnDiskFullBackpressure() *DiskSpaceProtectionSecurityLake {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputSecurityLake) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputSecurityLake) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputSecurityLake) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputSecurityLake) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputSecurityLake) GetMaxClosingFilesToBackpressure() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxClosingFilesToBackpressure
}

func (o *OutputSecurityLake) GetAccountID() string {
	if o == nil {
		return ""
	}
	return o.AccountID
}

func (o *OutputSecurityLake) GetCustomSource() string {
	if o == nil {
		return ""
	}
	return o.CustomSource
}

func (o *OutputSecurityLake) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputSecurityLake) GetParquetVersion() *ParquetVersionSecurityLake {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputSecurityLake) GetParquetDataPageVersion() *DataPageVersionSecurityLake {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputSecurityLake) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputSecurityLake) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputSecurityLake) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputSecurityLake) GetKeyValueMetadata() []KeyValueMetadatumSecurityLake {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputSecurityLake) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputSecurityLake) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputSecurityLake) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputSecurityLake) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSecurityLake) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputSecurityLake) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputSecurityLake) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputSecurityLake) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputSecurityLake) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputSecurityLake) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type TypeDlS3 string

const (
	TypeDlS3DlS3 TypeDlS3 = "dl_s3"
)

func (e TypeDlS3) ToPointer() *TypeDlS3 {
	return &e
}
func (e *TypeDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dl_s3":
		*e = TypeDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDlS3: %v", v)
	}
}

// AuthenticationMethodDlS3 - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodDlS3 string

const (
	AuthenticationMethodDlS3Auto   AuthenticationMethodDlS3 = "auto"
	AuthenticationMethodDlS3Manual AuthenticationMethodDlS3 = "manual"
	AuthenticationMethodDlS3Secret AuthenticationMethodDlS3 = "secret"
)

func (e AuthenticationMethodDlS3) ToPointer() *AuthenticationMethodDlS3 {
	return &e
}
func (e *AuthenticationMethodDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodDlS3: %v", v)
	}
}

// SignatureVersionDlS3 - Signature version to use for signing S3 requests
type SignatureVersionDlS3 string

const (
	SignatureVersionDlS3V2 SignatureVersionDlS3 = "v2"
	SignatureVersionDlS3V4 SignatureVersionDlS3 = "v4"
)

func (e SignatureVersionDlS3) ToPointer() *SignatureVersionDlS3 {
	return &e
}
func (e *SignatureVersionDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = SignatureVersionDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SignatureVersionDlS3: %v", v)
	}
}

// ObjectACLDlS3 - Object ACL to assign to uploaded objects
type ObjectACLDlS3 string

const (
	ObjectACLDlS3Private                ObjectACLDlS3 = "private"
	ObjectACLDlS3PublicRead             ObjectACLDlS3 = "public-read"
	ObjectACLDlS3PublicReadWrite        ObjectACLDlS3 = "public-read-write"
	ObjectACLDlS3AuthenticatedRead      ObjectACLDlS3 = "authenticated-read"
	ObjectACLDlS3AwsExecRead            ObjectACLDlS3 = "aws-exec-read"
	ObjectACLDlS3BucketOwnerRead        ObjectACLDlS3 = "bucket-owner-read"
	ObjectACLDlS3BucketOwnerFullControl ObjectACLDlS3 = "bucket-owner-full-control"
)

func (e ObjectACLDlS3) ToPointer() *ObjectACLDlS3 {
	return &e
}
func (e *ObjectACLDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "private":
		fallthrough
	case "public-read":
		fallthrough
	case "public-read-write":
		fallthrough
	case "authenticated-read":
		fallthrough
	case "aws-exec-read":
		fallthrough
	case "bucket-owner-read":
		fallthrough
	case "bucket-owner-full-control":
		*e = ObjectACLDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ObjectACLDlS3: %v", v)
	}
}

// StorageClassDlS3 - Storage class to select for uploaded objects
type StorageClassDlS3 string

const (
	StorageClassDlS3Standard           StorageClassDlS3 = "STANDARD"
	StorageClassDlS3ReducedRedundancy  StorageClassDlS3 = "REDUCED_REDUNDANCY"
	StorageClassDlS3StandardIa         StorageClassDlS3 = "STANDARD_IA"
	StorageClassDlS3OnezoneIa          StorageClassDlS3 = "ONEZONE_IA"
	StorageClassDlS3IntelligentTiering StorageClassDlS3 = "INTELLIGENT_TIERING"
	StorageClassDlS3Glacier            StorageClassDlS3 = "GLACIER"
	StorageClassDlS3GlacierIr          StorageClassDlS3 = "GLACIER_IR"
	StorageClassDlS3DeepArchive        StorageClassDlS3 = "DEEP_ARCHIVE"
)

func (e StorageClassDlS3) ToPointer() *StorageClassDlS3 {
	return &e
}
func (e *StorageClassDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "STANDARD":
		fallthrough
	case "REDUCED_REDUNDANCY":
		fallthrough
	case "STANDARD_IA":
		fallthrough
	case "ONEZONE_IA":
		fallthrough
	case "INTELLIGENT_TIERING":
		fallthrough
	case "GLACIER":
		fallthrough
	case "GLACIER_IR":
		fallthrough
	case "DEEP_ARCHIVE":
		*e = StorageClassDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for StorageClassDlS3: %v", v)
	}
}

type ServerSideEncryptionForUploadedObjectsDlS3 string

const (
	ServerSideEncryptionForUploadedObjectsDlS3Aes256 ServerSideEncryptionForUploadedObjectsDlS3 = "AES256"
	ServerSideEncryptionForUploadedObjectsDlS3AwsKms ServerSideEncryptionForUploadedObjectsDlS3 = "aws:kms"
)

func (e ServerSideEncryptionForUploadedObjectsDlS3) ToPointer() *ServerSideEncryptionForUploadedObjectsDlS3 {
	return &e
}
func (e *ServerSideEncryptionForUploadedObjectsDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "AES256":
		fallthrough
	case "aws:kms":
		*e = ServerSideEncryptionForUploadedObjectsDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ServerSideEncryptionForUploadedObjectsDlS3: %v", v)
	}
}

// DataFormatDlS3 - Format of the output data
type DataFormatDlS3 string

const (
	DataFormatDlS3JSON    DataFormatDlS3 = "json"
	DataFormatDlS3Raw     DataFormatDlS3 = "raw"
	DataFormatDlS3Parquet DataFormatDlS3 = "parquet"
)

func (e DataFormatDlS3) ToPointer() *DataFormatDlS3 {
	return &e
}
func (e *DataFormatDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "parquet":
		*e = DataFormatDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataFormatDlS3: %v", v)
	}
}

// BackpressureBehaviorDlS3 - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorDlS3 string

const (
	BackpressureBehaviorDlS3Block BackpressureBehaviorDlS3 = "block"
	BackpressureBehaviorDlS3Drop  BackpressureBehaviorDlS3 = "drop"
)

func (e BackpressureBehaviorDlS3) ToPointer() *BackpressureBehaviorDlS3 {
	return &e
}
func (e *BackpressureBehaviorDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorDlS3: %v", v)
	}
}

// DiskSpaceProtectionDlS3 - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionDlS3 string

const (
	DiskSpaceProtectionDlS3Block DiskSpaceProtectionDlS3 = "block"
	DiskSpaceProtectionDlS3Drop  DiskSpaceProtectionDlS3 = "drop"
)

func (e DiskSpaceProtectionDlS3) ToPointer() *DiskSpaceProtectionDlS3 {
	return &e
}
func (e *DiskSpaceProtectionDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionDlS3: %v", v)
	}
}

// CompressionDlS3 - Data compression format to apply to HTTP content before it is delivered
type CompressionDlS3 string

const (
	CompressionDlS3None CompressionDlS3 = "none"
	CompressionDlS3Gzip CompressionDlS3 = "gzip"
)

func (e CompressionDlS3) ToPointer() *CompressionDlS3 {
	return &e
}
func (e *CompressionDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionDlS3: %v", v)
	}
}

// CompressionLevelDlS3 - Compression level to apply before moving files to final destination
type CompressionLevelDlS3 string

const (
	CompressionLevelDlS3BestSpeed       CompressionLevelDlS3 = "best_speed"
	CompressionLevelDlS3Normal          CompressionLevelDlS3 = "normal"
	CompressionLevelDlS3BestCompression CompressionLevelDlS3 = "best_compression"
)

func (e CompressionLevelDlS3) ToPointer() *CompressionLevelDlS3 {
	return &e
}
func (e *CompressionLevelDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "best_speed":
		fallthrough
	case "normal":
		fallthrough
	case "best_compression":
		*e = CompressionLevelDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionLevelDlS3: %v", v)
	}
}

// ParquetVersionDlS3 - Determines which data types are supported and how they are represented
type ParquetVersionDlS3 string

const (
	ParquetVersionDlS3Parquet10 ParquetVersionDlS3 = "PARQUET_1_0"
	ParquetVersionDlS3Parquet24 ParquetVersionDlS3 = "PARQUET_2_4"
	ParquetVersionDlS3Parquet26 ParquetVersionDlS3 = "PARQUET_2_6"
)

func (e ParquetVersionDlS3) ToPointer() *ParquetVersionDlS3 {
	return &e
}
func (e *ParquetVersionDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PARQUET_1_0":
		fallthrough
	case "PARQUET_2_4":
		fallthrough
	case "PARQUET_2_6":
		*e = ParquetVersionDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ParquetVersionDlS3: %v", v)
	}
}

// DataPageVersionDlS3 - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionDlS3 string

const (
	DataPageVersionDlS3DataPageV1 DataPageVersionDlS3 = "DATA_PAGE_V1"
	DataPageVersionDlS3DataPageV2 DataPageVersionDlS3 = "DATA_PAGE_V2"
)

func (e DataPageVersionDlS3) ToPointer() *DataPageVersionDlS3 {
	return &e
}
func (e *DataPageVersionDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "DATA_PAGE_V1":
		fallthrough
	case "DATA_PAGE_V2":
		*e = DataPageVersionDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataPageVersionDlS3: %v", v)
	}
}

type KeyValueMetadatumDlS3 struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumDlS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumDlS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *KeyValueMetadatumDlS3) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *KeyValueMetadatumDlS3) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputDlS3 struct {
	// Unique ID for this output
	ID   string    `json:"id"`
	Type *TypeDlS3 `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket string `json:"bucket"`
	// Region where the S3 bucket is located
	Region *string `json:"region,omitempty"`
	// Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodDlS3 `default:"auto" json:"awsAuthenticationMethod"`
	// S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing S3 requests
	SignatureVersion *SignatureVersionDlS3 `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access S3
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Prefix to append to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`
	DestPath *string `default:"" json:"destPath"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLDlS3 `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass         *StorageClassDlS3                           `json:"storageClass,omitempty"`
	ServerSideEncryption *ServerSideEncryptionForUploadedObjectsDlS3 `json:"serverSideEncryption,omitempty"`
	// ID or ARN of the KMS customer-managed key to use for encryption
	KmsKeyID *string `json:"kmsKeyId,omitempty"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// Format of the output data
	Format *DataFormatDlS3 `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorDlS3 `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionDlS3 `default:"block" json:"onDiskFullBackpressure"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Maximum number of files that can be waiting for upload before backpressure is applied
	MaxClosingFilesToBackpressure *float64 `default:"100" json:"maxClosingFilesToBackpressure"`
	// List of fields to partition the path by, in addition to time, which is included automatically. The effective partition will be YYYY/MM/DD/HH/<list/of/fields>.
	PartitioningFields []string `json:"partitioningFields,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressionDlS3 `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelDlS3 `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionDlS3 `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionDlS3 `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumDlS3 `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputDlS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDlS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputDlS3) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDlS3) GetType() *TypeDlS3 {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputDlS3) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDlS3) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDlS3) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDlS3) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDlS3) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputDlS3) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputDlS3) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputDlS3) GetAwsAuthenticationMethod() *AuthenticationMethodDlS3 {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputDlS3) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputDlS3) GetSignatureVersion() *SignatureVersionDlS3 {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputDlS3) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputDlS3) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDlS3) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputDlS3) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputDlS3) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputDlS3) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputDlS3) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputDlS3) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputDlS3) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputDlS3) GetObjectACL() *ObjectACLDlS3 {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputDlS3) GetStorageClass() *StorageClassDlS3 {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputDlS3) GetServerSideEncryption() *ServerSideEncryptionForUploadedObjectsDlS3 {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputDlS3) GetKmsKeyID() *string {
	if o == nil {
		return nil
	}
	return o.KmsKeyID
}

func (o *OutputDlS3) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputDlS3) GetFormat() *DataFormatDlS3 {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDlS3) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputDlS3) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputDlS3) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputDlS3) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputDlS3) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputDlS3) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputDlS3) GetOnBackpressure() *BackpressureBehaviorDlS3 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDlS3) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputDlS3) GetOnDiskFullBackpressure() *DiskSpaceProtectionDlS3 {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputDlS3) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputDlS3) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputDlS3) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputDlS3) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputDlS3) GetMaxClosingFilesToBackpressure() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxClosingFilesToBackpressure
}

func (o *OutputDlS3) GetPartitioningFields() []string {
	if o == nil {
		return nil
	}
	return o.PartitioningFields
}

func (o *OutputDlS3) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDlS3) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputDlS3) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputDlS3) GetCompress() *CompressionDlS3 {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDlS3) GetCompressionLevel() *CompressionLevelDlS3 {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputDlS3) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputDlS3) GetParquetVersion() *ParquetVersionDlS3 {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputDlS3) GetParquetDataPageVersion() *DataPageVersionDlS3 {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputDlS3) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputDlS3) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputDlS3) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputDlS3) GetKeyValueMetadata() []KeyValueMetadatumDlS3 {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputDlS3) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputDlS3) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputDlS3) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputDlS3) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputDlS3) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputDlS3) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type TypeCrowdstrikeNextGenSiem string

const (
	TypeCrowdstrikeNextGenSiemCrowdstrikeNextGenSiem TypeCrowdstrikeNextGenSiem = "crowdstrike_next_gen_siem"
)

func (e TypeCrowdstrikeNextGenSiem) ToPointer() *TypeCrowdstrikeNextGenSiem {
	return &e
}
func (e *TypeCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "crowdstrike_next_gen_siem":
		*e = TypeCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeCrowdstrikeNextGenSiem: %v", v)
	}
}

type ExtraHTTPHeaderCrowdstrikeNextGenSiem struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderCrowdstrikeNextGenSiem) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderCrowdstrikeNextGenSiem) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeCrowdstrikeNextGenSiem - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeCrowdstrikeNextGenSiem string

const (
	FailedRequestLoggingModeCrowdstrikeNextGenSiemPayload           FailedRequestLoggingModeCrowdstrikeNextGenSiem = "payload"
	FailedRequestLoggingModeCrowdstrikeNextGenSiemPayloadAndHeaders FailedRequestLoggingModeCrowdstrikeNextGenSiem = "payloadAndHeaders"
	FailedRequestLoggingModeCrowdstrikeNextGenSiemNone              FailedRequestLoggingModeCrowdstrikeNextGenSiem = "none"
)

func (e FailedRequestLoggingModeCrowdstrikeNextGenSiem) ToPointer() *FailedRequestLoggingModeCrowdstrikeNextGenSiem {
	return &e
}
func (e *FailedRequestLoggingModeCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeCrowdstrikeNextGenSiem: %v", v)
	}
}

// RequestFormatCrowdstrikeNextGenSiem - When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
type RequestFormatCrowdstrikeNextGenSiem string

const (
	RequestFormatCrowdstrikeNextGenSiemJSON RequestFormatCrowdstrikeNextGenSiem = "JSON"
	RequestFormatCrowdstrikeNextGenSiemRaw  RequestFormatCrowdstrikeNextGenSiem = "raw"
)

func (e RequestFormatCrowdstrikeNextGenSiem) ToPointer() *RequestFormatCrowdstrikeNextGenSiem {
	return &e
}
func (e *RequestFormatCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "JSON":
		fallthrough
	case "raw":
		*e = RequestFormatCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RequestFormatCrowdstrikeNextGenSiem: %v", v)
	}
}

// AuthenticationMethodCrowdstrikeNextGenSiem - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodCrowdstrikeNextGenSiem string

const (
	AuthenticationMethodCrowdstrikeNextGenSiemManual AuthenticationMethodCrowdstrikeNextGenSiem = "manual"
	AuthenticationMethodCrowdstrikeNextGenSiemSecret AuthenticationMethodCrowdstrikeNextGenSiem = "secret"
)

func (e AuthenticationMethodCrowdstrikeNextGenSiem) ToPointer() *AuthenticationMethodCrowdstrikeNextGenSiem {
	return &e
}
func (e *AuthenticationMethodCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodCrowdstrikeNextGenSiem: %v", v)
	}
}

type ResponseRetrySettingCrowdstrikeNextGenSiem struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingCrowdstrikeNextGenSiem) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingCrowdstrikeNextGenSiem) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingCrowdstrikeNextGenSiem) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingCrowdstrikeNextGenSiem) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsCrowdstrikeNextGenSiem struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorCrowdstrikeNextGenSiem - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorCrowdstrikeNextGenSiem string

const (
	BackpressureBehaviorCrowdstrikeNextGenSiemBlock BackpressureBehaviorCrowdstrikeNextGenSiem = "block"
	BackpressureBehaviorCrowdstrikeNextGenSiemDrop  BackpressureBehaviorCrowdstrikeNextGenSiem = "drop"
	BackpressureBehaviorCrowdstrikeNextGenSiemQueue BackpressureBehaviorCrowdstrikeNextGenSiem = "queue"
)

func (e BackpressureBehaviorCrowdstrikeNextGenSiem) ToPointer() *BackpressureBehaviorCrowdstrikeNextGenSiem {
	return &e
}
func (e *BackpressureBehaviorCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorCrowdstrikeNextGenSiem: %v", v)
	}
}

// CompressionCrowdstrikeNextGenSiem - Codec to use to compress the persisted data
type CompressionCrowdstrikeNextGenSiem string

const (
	CompressionCrowdstrikeNextGenSiemNone CompressionCrowdstrikeNextGenSiem = "none"
	CompressionCrowdstrikeNextGenSiemGzip CompressionCrowdstrikeNextGenSiem = "gzip"
)

func (e CompressionCrowdstrikeNextGenSiem) ToPointer() *CompressionCrowdstrikeNextGenSiem {
	return &e
}
func (e *CompressionCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionCrowdstrikeNextGenSiem: %v", v)
	}
}

// QueueFullBehaviorCrowdstrikeNextGenSiem - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorCrowdstrikeNextGenSiem string

const (
	QueueFullBehaviorCrowdstrikeNextGenSiemBlock QueueFullBehaviorCrowdstrikeNextGenSiem = "block"
	QueueFullBehaviorCrowdstrikeNextGenSiemDrop  QueueFullBehaviorCrowdstrikeNextGenSiem = "drop"
)

func (e QueueFullBehaviorCrowdstrikeNextGenSiem) ToPointer() *QueueFullBehaviorCrowdstrikeNextGenSiem {
	return &e
}
func (e *QueueFullBehaviorCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorCrowdstrikeNextGenSiem: %v", v)
	}
}

// ModeCrowdstrikeNextGenSiem - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeCrowdstrikeNextGenSiem string

const (
	ModeCrowdstrikeNextGenSiemError        ModeCrowdstrikeNextGenSiem = "error"
	ModeCrowdstrikeNextGenSiemBackpressure ModeCrowdstrikeNextGenSiem = "backpressure"
	ModeCrowdstrikeNextGenSiemAlways       ModeCrowdstrikeNextGenSiem = "always"
)

func (e ModeCrowdstrikeNextGenSiem) ToPointer() *ModeCrowdstrikeNextGenSiem {
	return &e
}
func (e *ModeCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeCrowdstrikeNextGenSiem: %v", v)
	}
}

type PqControlsCrowdstrikeNextGenSiem struct {
}

type OutputCrowdstrikeNextGenSiem struct {
	// Unique ID for this output
	ID   string                      `json:"id"`
	Type *TypeCrowdstrikeNextGenSiem `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL provided from a CrowdStrike data connector.
	// Example: https://ingest.<region>.crowdstrike.com/api/ingest/hec/<connection-id>/v1/services/collector
	URL string `json:"url"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderCrowdstrikeNextGenSiem `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"true" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeCrowdstrikeNextGenSiem `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
	Format *RequestFormatCrowdstrikeNextGenSiem `default:"raw" json:"format"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodCrowdstrikeNextGenSiem `default:"manual" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingCrowdstrikeNextGenSiem `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsCrowdstrikeNextGenSiem  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCrowdstrikeNextGenSiem `default:"block" json:"onBackpressure"`
	Description    *string                                     `json:"description,omitempty"`
	Token          *string                                     `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionCrowdstrikeNextGenSiem `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorCrowdstrikeNextGenSiem `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeCrowdstrikeNextGenSiem       `default:"error" json:"pqMode"`
	PqControls *PqControlsCrowdstrikeNextGenSiem `json:"pqControls,omitempty"`
}

func (o OutputCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputCrowdstrikeNextGenSiem) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputCrowdstrikeNextGenSiem) GetType() *TypeCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputCrowdstrikeNextGenSiem) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCrowdstrikeNextGenSiem) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCrowdstrikeNextGenSiem) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCrowdstrikeNextGenSiem) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCrowdstrikeNextGenSiem) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputCrowdstrikeNextGenSiem) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputCrowdstrikeNextGenSiem) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputCrowdstrikeNextGenSiem) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputCrowdstrikeNextGenSiem) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputCrowdstrikeNextGenSiem) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCrowdstrikeNextGenSiem) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputCrowdstrikeNextGenSiem) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputCrowdstrikeNextGenSiem) GetExtraHTTPHeaders() []ExtraHTTPHeaderCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputCrowdstrikeNextGenSiem) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputCrowdstrikeNextGenSiem) GetFailedRequestLoggingMode() *FailedRequestLoggingModeCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputCrowdstrikeNextGenSiem) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputCrowdstrikeNextGenSiem) GetFormat() *RequestFormatCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputCrowdstrikeNextGenSiem) GetAuthType() *AuthenticationMethodCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputCrowdstrikeNextGenSiem) GetResponseRetrySettings() []ResponseRetrySettingCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputCrowdstrikeNextGenSiem) GetTimeoutRetrySettings() *TimeoutRetrySettingsCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputCrowdstrikeNextGenSiem) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputCrowdstrikeNextGenSiem) GetOnBackpressure() *BackpressureBehaviorCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCrowdstrikeNextGenSiem) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCrowdstrikeNextGenSiem) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputCrowdstrikeNextGenSiem) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqCompress() *CompressionCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqOnBackpressure() *QueueFullBehaviorCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMode() *ModeCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqControls() *PqControlsCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeHumioHec string

const (
	TypeHumioHecHumioHec TypeHumioHec = "humio_hec"
)

func (e TypeHumioHec) ToPointer() *TypeHumioHec {
	return &e
}
func (e *TypeHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "humio_hec":
		*e = TypeHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeHumioHec: %v", v)
	}
}

type ExtraHTTPHeaderHumioHec struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderHumioHec) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderHumioHec) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeHumioHec - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeHumioHec string

const (
	FailedRequestLoggingModeHumioHecPayload           FailedRequestLoggingModeHumioHec = "payload"
	FailedRequestLoggingModeHumioHecPayloadAndHeaders FailedRequestLoggingModeHumioHec = "payloadAndHeaders"
	FailedRequestLoggingModeHumioHecNone              FailedRequestLoggingModeHumioHec = "none"
)

func (e FailedRequestLoggingModeHumioHec) ToPointer() *FailedRequestLoggingModeHumioHec {
	return &e
}
func (e *FailedRequestLoggingModeHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeHumioHec: %v", v)
	}
}

// RequestFormatHumioHec - When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
type RequestFormatHumioHec string

const (
	RequestFormatHumioHecJSON RequestFormatHumioHec = "JSON"
	RequestFormatHumioHecRaw  RequestFormatHumioHec = "raw"
)

func (e RequestFormatHumioHec) ToPointer() *RequestFormatHumioHec {
	return &e
}
func (e *RequestFormatHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "JSON":
		fallthrough
	case "raw":
		*e = RequestFormatHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RequestFormatHumioHec: %v", v)
	}
}

// AuthenticationMethodHumioHec - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodHumioHec string

const (
	AuthenticationMethodHumioHecManual AuthenticationMethodHumioHec = "manual"
	AuthenticationMethodHumioHecSecret AuthenticationMethodHumioHec = "secret"
)

func (e AuthenticationMethodHumioHec) ToPointer() *AuthenticationMethodHumioHec {
	return &e
}
func (e *AuthenticationMethodHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodHumioHec: %v", v)
	}
}

type ResponseRetrySettingHumioHec struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingHumioHec) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingHumioHec) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingHumioHec) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingHumioHec) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsHumioHec struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsHumioHec) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsHumioHec) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsHumioHec) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsHumioHec) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorHumioHec - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorHumioHec string

const (
	BackpressureBehaviorHumioHecBlock BackpressureBehaviorHumioHec = "block"
	BackpressureBehaviorHumioHecDrop  BackpressureBehaviorHumioHec = "drop"
	BackpressureBehaviorHumioHecQueue BackpressureBehaviorHumioHec = "queue"
)

func (e BackpressureBehaviorHumioHec) ToPointer() *BackpressureBehaviorHumioHec {
	return &e
}
func (e *BackpressureBehaviorHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorHumioHec: %v", v)
	}
}

// CompressionHumioHec - Codec to use to compress the persisted data
type CompressionHumioHec string

const (
	CompressionHumioHecNone CompressionHumioHec = "none"
	CompressionHumioHecGzip CompressionHumioHec = "gzip"
)

func (e CompressionHumioHec) ToPointer() *CompressionHumioHec {
	return &e
}
func (e *CompressionHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionHumioHec: %v", v)
	}
}

// QueueFullBehaviorHumioHec - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorHumioHec string

const (
	QueueFullBehaviorHumioHecBlock QueueFullBehaviorHumioHec = "block"
	QueueFullBehaviorHumioHecDrop  QueueFullBehaviorHumioHec = "drop"
)

func (e QueueFullBehaviorHumioHec) ToPointer() *QueueFullBehaviorHumioHec {
	return &e
}
func (e *QueueFullBehaviorHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorHumioHec: %v", v)
	}
}

// ModeHumioHec - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeHumioHec string

const (
	ModeHumioHecError        ModeHumioHec = "error"
	ModeHumioHecBackpressure ModeHumioHec = "backpressure"
	ModeHumioHecAlways       ModeHumioHec = "always"
)

func (e ModeHumioHec) ToPointer() *ModeHumioHec {
	return &e
}
func (e *ModeHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeHumioHec: %v", v)
	}
}

type PqControlsHumioHec struct {
}

type OutputHumioHec struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type *TypeHumioHec `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL to a CrowdStrike Falcon LogScale endpoint to send events to. Examples: https://cloud.us.humio.com/api/v1/ingest/hec for JSON and https://cloud.us.humio.com/api/v1/ingest/hec/raw for raw
	URL *string `default:"https://cloud.us.humio.com/api/v1/ingest/hec" json:"url"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderHumioHec `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"true" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeHumioHec `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
	Format *RequestFormatHumioHec `default:"JSON" json:"format"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodHumioHec `default:"manual" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingHumioHec `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsHumioHec  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorHumioHec `default:"block" json:"onBackpressure"`
	Description    *string                       `json:"description,omitempty"`
	// CrowdStrike Falcon LogScale authentication token
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionHumioHec `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorHumioHec `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeHumioHec       `default:"error" json:"pqMode"`
	PqControls *PqControlsHumioHec `json:"pqControls,omitempty"`
}

func (o OutputHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputHumioHec) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputHumioHec) GetType() *TypeHumioHec {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputHumioHec) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputHumioHec) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputHumioHec) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputHumioHec) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputHumioHec) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputHumioHec) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputHumioHec) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputHumioHec) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputHumioHec) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputHumioHec) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputHumioHec) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputHumioHec) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputHumioHec) GetExtraHTTPHeaders() []ExtraHTTPHeaderHumioHec {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputHumioHec) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputHumioHec) GetFailedRequestLoggingMode() *FailedRequestLoggingModeHumioHec {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputHumioHec) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputHumioHec) GetFormat() *RequestFormatHumioHec {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputHumioHec) GetAuthType() *AuthenticationMethodHumioHec {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputHumioHec) GetResponseRetrySettings() []ResponseRetrySettingHumioHec {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputHumioHec) GetTimeoutRetrySettings() *TimeoutRetrySettingsHumioHec {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputHumioHec) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputHumioHec) GetOnBackpressure() *BackpressureBehaviorHumioHec {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputHumioHec) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputHumioHec) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputHumioHec) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputHumioHec) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputHumioHec) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputHumioHec) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputHumioHec) GetPqCompress() *CompressionHumioHec {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputHumioHec) GetPqOnBackpressure() *QueueFullBehaviorHumioHec {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputHumioHec) GetPqMode() *ModeHumioHec {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputHumioHec) GetPqControls() *PqControlsHumioHec {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeServiceNow string

const (
	TypeServiceNowServiceNow TypeServiceNow = "service_now"
)

func (e TypeServiceNow) ToPointer() *TypeServiceNow {
	return &e
}
func (e *TypeServiceNow) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "service_now":
		*e = TypeServiceNow(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeServiceNow: %v", v)
	}
}

// OTLPVersion131 - The version of OTLP Protobuf definitions to use when structuring data to send
type OTLPVersion131 string

const (
	OTLPVersion131OneDot3Dot1 OTLPVersion131 = "1.3.1"
)

func (e OTLPVersion131) ToPointer() *OTLPVersion131 {
	return &e
}
func (e *OTLPVersion131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "1.3.1":
		*e = OTLPVersion131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OTLPVersion131: %v", v)
	}
}

// Protocol131 - Select a transport option for OpenTelemetry
type Protocol131 string

const (
	Protocol131Grpc Protocol131 = "grpc"
	Protocol131HTTP Protocol131 = "http"
)

func (e Protocol131) ToPointer() *Protocol131 {
	return &e
}
func (e *Protocol131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "grpc":
		fallthrough
	case "http":
		*e = Protocol131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for Protocol131: %v", v)
	}
}

// CompressCompression131 - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type CompressCompression131 string

const (
	CompressCompression131None    CompressCompression131 = "none"
	CompressCompression131Deflate CompressCompression131 = "deflate"
	CompressCompression131Gzip    CompressCompression131 = "gzip"
)

func (e CompressCompression131) ToPointer() *CompressCompression131 {
	return &e
}
func (e *CompressCompression131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "deflate":
		fallthrough
	case "gzip":
		*e = CompressCompression131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressCompression131: %v", v)
	}
}

// HTTPCompressCompression131 - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type HTTPCompressCompression131 string

const (
	HTTPCompressCompression131None HTTPCompressCompression131 = "none"
	HTTPCompressCompression131Gzip HTTPCompressCompression131 = "gzip"
)

func (e HTTPCompressCompression131) ToPointer() *HTTPCompressCompression131 {
	return &e
}
func (e *HTTPCompressCompression131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = HTTPCompressCompression131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for HTTPCompressCompression131: %v", v)
	}
}

type Metadatum131 struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (m Metadatum131) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(m, "", false)
}

func (m *Metadatum131) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &m, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *Metadatum131) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *Metadatum131) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingMode131 - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingMode131 string

const (
	FailedRequestLoggingMode131Payload           FailedRequestLoggingMode131 = "payload"
	FailedRequestLoggingMode131PayloadAndHeaders FailedRequestLoggingMode131 = "payloadAndHeaders"
	FailedRequestLoggingMode131None              FailedRequestLoggingMode131 = "none"
)

func (e FailedRequestLoggingMode131) ToPointer() *FailedRequestLoggingMode131 {
	return &e
}
func (e *FailedRequestLoggingMode131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingMode131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingMode131: %v", v)
	}
}

// BackpressureBehavior131 - How to handle events when all receivers are exerting backpressure
type BackpressureBehavior131 string

const (
	BackpressureBehavior131Block BackpressureBehavior131 = "block"
	BackpressureBehavior131Drop  BackpressureBehavior131 = "drop"
	BackpressureBehavior131Queue BackpressureBehavior131 = "queue"
)

func (e BackpressureBehavior131) ToPointer() *BackpressureBehavior131 {
	return &e
}
func (e *BackpressureBehavior131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehavior131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehavior131: %v", v)
	}
}

type ExtraHTTPHeader131 struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeader131) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeader131) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type ResponseRetrySetting131 struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySetting131) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySetting131) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySetting131) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySetting131) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySetting131) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySetting131) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettings131 struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettings131) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettings131) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettings131) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettings131) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettings131) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettings131) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type MinimumTLSVersion131 string

const (
	MinimumTLSVersion131TlSv1  MinimumTLSVersion131 = "TLSv1"
	MinimumTLSVersion131TlSv11 MinimumTLSVersion131 = "TLSv1.1"
	MinimumTLSVersion131TlSv12 MinimumTLSVersion131 = "TLSv1.2"
	MinimumTLSVersion131TlSv13 MinimumTLSVersion131 = "TLSv1.3"
)

func (e MinimumTLSVersion131) ToPointer() *MinimumTLSVersion131 {
	return &e
}
func (e *MinimumTLSVersion131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MinimumTLSVersion131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MinimumTLSVersion131: %v", v)
	}
}

type MaximumTLSVersion131 string

const (
	MaximumTLSVersion131TlSv1  MaximumTLSVersion131 = "TLSv1"
	MaximumTLSVersion131TlSv11 MaximumTLSVersion131 = "TLSv1.1"
	MaximumTLSVersion131TlSv12 MaximumTLSVersion131 = "TLSv1.2"
	MaximumTLSVersion131TlSv13 MaximumTLSVersion131 = "TLSv1.3"
)

func (e MaximumTLSVersion131) ToPointer() *MaximumTLSVersion131 {
	return &e
}
func (e *MaximumTLSVersion131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MaximumTLSVersion131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MaximumTLSVersion131: %v", v)
	}
}

type TLSSettingsClientSide131 struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string               `json:"passphrase,omitempty"`
	MinVersion *MinimumTLSVersion131 `json:"minVersion,omitempty"`
	MaxVersion *MaximumTLSVersion131 `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSide131) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSide131) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSide131) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSide131) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *TLSSettingsClientSide131) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *TLSSettingsClientSide131) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *TLSSettingsClientSide131) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *TLSSettingsClientSide131) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *TLSSettingsClientSide131) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *TLSSettingsClientSide131) GetMinVersion() *MinimumTLSVersion131 {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *TLSSettingsClientSide131) GetMaxVersion() *MaximumTLSVersion131 {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// PqCompressCompression131 - Codec to use to compress the persisted data
type PqCompressCompression131 string

const (
	PqCompressCompression131None PqCompressCompression131 = "none"
	PqCompressCompression131Gzip PqCompressCompression131 = "gzip"
)

func (e PqCompressCompression131) ToPointer() *PqCompressCompression131 {
	return &e
}
func (e *PqCompressCompression131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompression131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompression131: %v", v)
	}
}

// QueueFullBehavior131 - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehavior131 string

const (
	QueueFullBehavior131Block QueueFullBehavior131 = "block"
	QueueFullBehavior131Drop  QueueFullBehavior131 = "drop"
)

func (e QueueFullBehavior131) ToPointer() *QueueFullBehavior131 {
	return &e
}
func (e *QueueFullBehavior131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehavior131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehavior131: %v", v)
	}
}

// Mode131 - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type Mode131 string

const (
	Mode131Error        Mode131 = "error"
	Mode131Backpressure Mode131 = "backpressure"
	Mode131Always       Mode131 = "always"
)

func (e Mode131) ToPointer() *Mode131 {
	return &e
}
func (e *Mode131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = Mode131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for Mode131: %v", v)
	}
}

type PqControls131 struct {
}

type OutputServiceNow struct {
	// Unique ID for this output
	ID   string          `json:"id"`
	Type *TypeServiceNow `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint where ServiceNow events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)
	Endpoint *string `default:"ingest.lightstep.com:443" json:"endpoint"`
	// Select or create a stored text secret
	TokenSecret   string  `json:"tokenSecret"`
	AuthTokenName *string `default:"lightstep-access-token" json:"authTokenName"`
	// The version of OTLP Protobuf definitions to use when structuring data to send
	OtlpVersion *OTLPVersion131 `default:"1.3.1" json:"otlpVersion"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"2048" json:"maxPayloadSizeKB"`
	// Select a transport option for OpenTelemetry
	Protocol *Protocol131 `default:"grpc" json:"protocol"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	Compress *CompressCompression131 `default:"gzip" json:"compress"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	HTTPCompress *HTTPCompressCompression131 `default:"gzip" json:"httpCompress"`
	// If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPTracesEndpointOverride *string `json:"httpTracesEndpointOverride,omitempty"`
	// If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPMetricsEndpointOverride *string `json:"httpMetricsEndpointOverride,omitempty"`
	// If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPLogsEndpointOverride *string `json:"httpLogsEndpointOverride,omitempty"`
	// List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.
	Metadata []Metadatum131 `json:"metadata,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingMode131 `default:"none" json:"failedRequestLoggingMode"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// How often the sender should ping the peer to keep the connection open
	KeepAliveTime *float64 `default:"30" json:"keepAliveTime"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehavior131 `default:"block" json:"onBackpressure"`
	Description    *string                  `json:"description,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeader131 `json:"extraHttpHeaders,omitempty"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySetting131 `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettings131  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool                     `default:"false" json:"responseHonorRetryAfterHeader"`
	TLS                           *TLSSettingsClientSide131 `json:"tls,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompression131 `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehavior131 `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *Mode131       `default:"error" json:"pqMode"`
	PqControls *PqControls131 `json:"pqControls,omitempty"`
}

func (o OutputServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputServiceNow) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputServiceNow) GetType() *TypeServiceNow {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputServiceNow) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputServiceNow) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputServiceNow) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputServiceNow) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputServiceNow) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputServiceNow) GetTokenSecret() string {
	if o == nil {
		return ""
	}
	return o.TokenSecret
}

func (o *OutputServiceNow) GetAuthTokenName() *string {
	if o == nil {
		return nil
	}
	return o.AuthTokenName
}

func (o *OutputServiceNow) GetOtlpVersion() *OTLPVersion131 {
	if o == nil {
		return nil
	}
	return o.OtlpVersion
}

func (o *OutputServiceNow) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputServiceNow) GetProtocol() *Protocol131 {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputServiceNow) GetCompress() *CompressCompression131 {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputServiceNow) GetHTTPCompress() *HTTPCompressCompression131 {
	if o == nil {
		return nil
	}
	return o.HTTPCompress
}

func (o *OutputServiceNow) GetHTTPTracesEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPTracesEndpointOverride
}

func (o *OutputServiceNow) GetHTTPMetricsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPMetricsEndpointOverride
}

func (o *OutputServiceNow) GetHTTPLogsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPLogsEndpointOverride
}

func (o *OutputServiceNow) GetMetadata() []Metadatum131 {
	if o == nil {
		return nil
	}
	return o.Metadata
}

func (o *OutputServiceNow) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputServiceNow) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputServiceNow) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputServiceNow) GetFailedRequestLoggingMode() *FailedRequestLoggingMode131 {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputServiceNow) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputServiceNow) GetKeepAliveTime() *float64 {
	if o == nil {
		return nil
	}
	return o.KeepAliveTime
}

func (o *OutputServiceNow) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputServiceNow) GetOnBackpressure() *BackpressureBehavior131 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputServiceNow) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputServiceNow) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputServiceNow) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputServiceNow) GetExtraHTTPHeaders() []ExtraHTTPHeader131 {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputServiceNow) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputServiceNow) GetResponseRetrySettings() []ResponseRetrySetting131 {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputServiceNow) GetTimeoutRetrySettings() *TimeoutRetrySettings131 {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputServiceNow) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputServiceNow) GetTLS() *TLSSettingsClientSide131 {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputServiceNow) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputServiceNow) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputServiceNow) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputServiceNow) GetPqCompress() *PqCompressCompression131 {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputServiceNow) GetPqOnBackpressure() *QueueFullBehavior131 {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputServiceNow) GetPqMode() *Mode131 {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputServiceNow) GetPqControls() *PqControls131 {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeOpenTelemetry string

const (
	CreateOutputTypeOpenTelemetryOpenTelemetry CreateOutputTypeOpenTelemetry = "open_telemetry"
)

func (e CreateOutputTypeOpenTelemetry) ToPointer() *CreateOutputTypeOpenTelemetry {
	return &e
}
func (e *CreateOutputTypeOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "open_telemetry":
		*e = CreateOutputTypeOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeOpenTelemetry: %v", v)
	}
}

// CreateOutputProtocolOpenTelemetry - Select a transport option for OpenTelemetry
type CreateOutputProtocolOpenTelemetry string

const (
	CreateOutputProtocolOpenTelemetryGrpc CreateOutputProtocolOpenTelemetry = "grpc"
	CreateOutputProtocolOpenTelemetryHTTP CreateOutputProtocolOpenTelemetry = "http"
)

func (e CreateOutputProtocolOpenTelemetry) ToPointer() *CreateOutputProtocolOpenTelemetry {
	return &e
}
func (e *CreateOutputProtocolOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "grpc":
		fallthrough
	case "http":
		*e = CreateOutputProtocolOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputProtocolOpenTelemetry: %v", v)
	}
}

// CreateOutputOTLPVersionOpenTelemetry - The version of OTLP Protobuf definitions to use when structuring data to send
type CreateOutputOTLPVersionOpenTelemetry string

const (
	CreateOutputOTLPVersionOpenTelemetryZeroDot10Dot0 CreateOutputOTLPVersionOpenTelemetry = "0.10.0"
	CreateOutputOTLPVersionOpenTelemetryOneDot3Dot1   CreateOutputOTLPVersionOpenTelemetry = "1.3.1"
)

func (e CreateOutputOTLPVersionOpenTelemetry) ToPointer() *CreateOutputOTLPVersionOpenTelemetry {
	return &e
}
func (e *CreateOutputOTLPVersionOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "0.10.0":
		fallthrough
	case "1.3.1":
		*e = CreateOutputOTLPVersionOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputOTLPVersionOpenTelemetry: %v", v)
	}
}

// CreateOutputCompressCompressionOpenTelemetry - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type CreateOutputCompressCompressionOpenTelemetry string

const (
	CreateOutputCompressCompressionOpenTelemetryNone    CreateOutputCompressCompressionOpenTelemetry = "none"
	CreateOutputCompressCompressionOpenTelemetryDeflate CreateOutputCompressCompressionOpenTelemetry = "deflate"
	CreateOutputCompressCompressionOpenTelemetryGzip    CreateOutputCompressCompressionOpenTelemetry = "gzip"
)

func (e CreateOutputCompressCompressionOpenTelemetry) ToPointer() *CreateOutputCompressCompressionOpenTelemetry {
	return &e
}
func (e *CreateOutputCompressCompressionOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "deflate":
		fallthrough
	case "gzip":
		*e = CreateOutputCompressCompressionOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputCompressCompressionOpenTelemetry: %v", v)
	}
}

// HTTPCompressCompressionOpenTelemetry - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type HTTPCompressCompressionOpenTelemetry string

const (
	HTTPCompressCompressionOpenTelemetryNone HTTPCompressCompressionOpenTelemetry = "none"
	HTTPCompressCompressionOpenTelemetryGzip HTTPCompressCompressionOpenTelemetry = "gzip"
)

func (e HTTPCompressCompressionOpenTelemetry) ToPointer() *HTTPCompressCompressionOpenTelemetry {
	return &e
}
func (e *HTTPCompressCompressionOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = HTTPCompressCompressionOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for HTTPCompressCompressionOpenTelemetry: %v", v)
	}
}

// CreateOutputAuthenticationTypeOpenTelemetry - OpenTelemetry authentication type
type CreateOutputAuthenticationTypeOpenTelemetry string

const (
	CreateOutputAuthenticationTypeOpenTelemetryNone              CreateOutputAuthenticationTypeOpenTelemetry = "none"
	CreateOutputAuthenticationTypeOpenTelemetryBasic             CreateOutputAuthenticationTypeOpenTelemetry = "basic"
	CreateOutputAuthenticationTypeOpenTelemetryCredentialsSecret CreateOutputAuthenticationTypeOpenTelemetry = "credentialsSecret"
	CreateOutputAuthenticationTypeOpenTelemetryToken             CreateOutputAuthenticationTypeOpenTelemetry = "token"
	CreateOutputAuthenticationTypeOpenTelemetryTextSecret        CreateOutputAuthenticationTypeOpenTelemetry = "textSecret"
	CreateOutputAuthenticationTypeOpenTelemetryOauth             CreateOutputAuthenticationTypeOpenTelemetry = "oauth"
)

func (e CreateOutputAuthenticationTypeOpenTelemetry) ToPointer() *CreateOutputAuthenticationTypeOpenTelemetry {
	return &e
}
func (e *CreateOutputAuthenticationTypeOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "basic":
		fallthrough
	case "credentialsSecret":
		fallthrough
	case "token":
		fallthrough
	case "textSecret":
		fallthrough
	case "oauth":
		*e = CreateOutputAuthenticationTypeOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputAuthenticationTypeOpenTelemetry: %v", v)
	}
}

type CreateOutputMetadatumOpenTelemetry struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (c CreateOutputMetadatumOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputMetadatumOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputMetadatumOpenTelemetry) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *CreateOutputMetadatumOpenTelemetry) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeOpenTelemetry - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeOpenTelemetry string

const (
	FailedRequestLoggingModeOpenTelemetryPayload           FailedRequestLoggingModeOpenTelemetry = "payload"
	FailedRequestLoggingModeOpenTelemetryPayloadAndHeaders FailedRequestLoggingModeOpenTelemetry = "payloadAndHeaders"
	FailedRequestLoggingModeOpenTelemetryNone              FailedRequestLoggingModeOpenTelemetry = "none"
)

func (e FailedRequestLoggingModeOpenTelemetry) ToPointer() *FailedRequestLoggingModeOpenTelemetry {
	return &e
}
func (e *FailedRequestLoggingModeOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeOpenTelemetry: %v", v)
	}
}

// BackpressureBehaviorOpenTelemetry - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorOpenTelemetry string

const (
	BackpressureBehaviorOpenTelemetryBlock BackpressureBehaviorOpenTelemetry = "block"
	BackpressureBehaviorOpenTelemetryDrop  BackpressureBehaviorOpenTelemetry = "drop"
	BackpressureBehaviorOpenTelemetryQueue BackpressureBehaviorOpenTelemetry = "queue"
)

func (e BackpressureBehaviorOpenTelemetry) ToPointer() *BackpressureBehaviorOpenTelemetry {
	return &e
}
func (e *BackpressureBehaviorOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorOpenTelemetry: %v", v)
	}
}

type CreateOutputOauthParamOpenTelemetry struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o *CreateOutputOauthParamOpenTelemetry) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *CreateOutputOauthParamOpenTelemetry) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type CreateOutputOauthHeaderOpenTelemetry struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o *CreateOutputOauthHeaderOpenTelemetry) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *CreateOutputOauthHeaderOpenTelemetry) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type ExtraHTTPHeaderOpenTelemetry struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderOpenTelemetry) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderOpenTelemetry) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type ResponseRetrySettingOpenTelemetry struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingOpenTelemetry) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingOpenTelemetry) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingOpenTelemetry) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingOpenTelemetry) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsOpenTelemetry struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsOpenTelemetry) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsOpenTelemetry) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsOpenTelemetry) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsOpenTelemetry) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type CreateOutputMinimumTLSVersionOpenTelemetry string

const (
	CreateOutputMinimumTLSVersionOpenTelemetryTlSv1  CreateOutputMinimumTLSVersionOpenTelemetry = "TLSv1"
	CreateOutputMinimumTLSVersionOpenTelemetryTlSv11 CreateOutputMinimumTLSVersionOpenTelemetry = "TLSv1.1"
	CreateOutputMinimumTLSVersionOpenTelemetryTlSv12 CreateOutputMinimumTLSVersionOpenTelemetry = "TLSv1.2"
	CreateOutputMinimumTLSVersionOpenTelemetryTlSv13 CreateOutputMinimumTLSVersionOpenTelemetry = "TLSv1.3"
)

func (e CreateOutputMinimumTLSVersionOpenTelemetry) ToPointer() *CreateOutputMinimumTLSVersionOpenTelemetry {
	return &e
}
func (e *CreateOutputMinimumTLSVersionOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputMinimumTLSVersionOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputMinimumTLSVersionOpenTelemetry: %v", v)
	}
}

type CreateOutputMaximumTLSVersionOpenTelemetry string

const (
	CreateOutputMaximumTLSVersionOpenTelemetryTlSv1  CreateOutputMaximumTLSVersionOpenTelemetry = "TLSv1"
	CreateOutputMaximumTLSVersionOpenTelemetryTlSv11 CreateOutputMaximumTLSVersionOpenTelemetry = "TLSv1.1"
	CreateOutputMaximumTLSVersionOpenTelemetryTlSv12 CreateOutputMaximumTLSVersionOpenTelemetry = "TLSv1.2"
	CreateOutputMaximumTLSVersionOpenTelemetryTlSv13 CreateOutputMaximumTLSVersionOpenTelemetry = "TLSv1.3"
)

func (e CreateOutputMaximumTLSVersionOpenTelemetry) ToPointer() *CreateOutputMaximumTLSVersionOpenTelemetry {
	return &e
}
func (e *CreateOutputMaximumTLSVersionOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputMaximumTLSVersionOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputMaximumTLSVersionOpenTelemetry: %v", v)
	}
}

type TLSSettingsClientSideOpenTelemetry struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                     `json:"passphrase,omitempty"`
	MinVersion *CreateOutputMinimumTLSVersionOpenTelemetry `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputMaximumTLSVersionOpenTelemetry `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSideOpenTelemetry) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSideOpenTelemetry) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *TLSSettingsClientSideOpenTelemetry) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *TLSSettingsClientSideOpenTelemetry) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *TLSSettingsClientSideOpenTelemetry) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *TLSSettingsClientSideOpenTelemetry) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *TLSSettingsClientSideOpenTelemetry) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *TLSSettingsClientSideOpenTelemetry) GetMinVersion() *CreateOutputMinimumTLSVersionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *TLSSettingsClientSideOpenTelemetry) GetMaxVersion() *CreateOutputMaximumTLSVersionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// PqCompressCompressionOpenTelemetry - Codec to use to compress the persisted data
type PqCompressCompressionOpenTelemetry string

const (
	PqCompressCompressionOpenTelemetryNone PqCompressCompressionOpenTelemetry = "none"
	PqCompressCompressionOpenTelemetryGzip PqCompressCompressionOpenTelemetry = "gzip"
)

func (e PqCompressCompressionOpenTelemetry) ToPointer() *PqCompressCompressionOpenTelemetry {
	return &e
}
func (e *PqCompressCompressionOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionOpenTelemetry: %v", v)
	}
}

// QueueFullBehaviorOpenTelemetry - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorOpenTelemetry string

const (
	QueueFullBehaviorOpenTelemetryBlock QueueFullBehaviorOpenTelemetry = "block"
	QueueFullBehaviorOpenTelemetryDrop  QueueFullBehaviorOpenTelemetry = "drop"
)

func (e QueueFullBehaviorOpenTelemetry) ToPointer() *QueueFullBehaviorOpenTelemetry {
	return &e
}
func (e *QueueFullBehaviorOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorOpenTelemetry: %v", v)
	}
}

// CreateOutputModeOpenTelemetry - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeOpenTelemetry string

const (
	CreateOutputModeOpenTelemetryError        CreateOutputModeOpenTelemetry = "error"
	CreateOutputModeOpenTelemetryBackpressure CreateOutputModeOpenTelemetry = "backpressure"
	CreateOutputModeOpenTelemetryAlways       CreateOutputModeOpenTelemetry = "always"
)

func (e CreateOutputModeOpenTelemetry) ToPointer() *CreateOutputModeOpenTelemetry {
	return &e
}
func (e *CreateOutputModeOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = CreateOutputModeOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputModeOpenTelemetry: %v", v)
	}
}

type PqControlsOpenTelemetry struct {
}

type OutputOpenTelemetry struct {
	// Unique ID for this output
	ID   string                        `json:"id"`
	Type CreateOutputTypeOpenTelemetry `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select a transport option for OpenTelemetry
	Protocol *CreateOutputProtocolOpenTelemetry `default:"grpc" json:"protocol"`
	// The endpoint where OTel events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets). Unspecified ports will default to 4317, unless the endpoint is an HTTPS-based URL or TLS is enabled, in which case 443 will be used.
	Endpoint string `json:"endpoint"`
	// The version of OTLP Protobuf definitions to use when structuring data to send
	OtlpVersion *CreateOutputOTLPVersionOpenTelemetry `default:"0.10.0" json:"otlpVersion"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	Compress *CreateOutputCompressCompressionOpenTelemetry `default:"gzip" json:"compress"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	HTTPCompress *HTTPCompressCompressionOpenTelemetry `default:"gzip" json:"httpCompress"`
	// OpenTelemetry authentication type
	AuthType *CreateOutputAuthenticationTypeOpenTelemetry `default:"none" json:"authType"`
	// If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPTracesEndpointOverride *string `json:"httpTracesEndpointOverride,omitempty"`
	// If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPMetricsEndpointOverride *string `json:"httpMetricsEndpointOverride,omitempty"`
	// If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPLogsEndpointOverride *string `json:"httpLogsEndpointOverride,omitempty"`
	// List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.
	Metadata []CreateOutputMetadatumOpenTelemetry `json:"metadata,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeOpenTelemetry `default:"none" json:"failedRequestLoggingMode"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// How often the sender should ping the peer to keep the connection open
	KeepAliveTime *float64 `default:"30" json:"keepAliveTime"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorOpenTelemetry `default:"block" json:"onBackpressure"`
	Description    *string                            `json:"description,omitempty"`
	Username       *string                            `json:"username,omitempty"`
	Password       *string                            `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []CreateOutputOauthParamOpenTelemetry `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []CreateOutputOauthHeaderOpenTelemetry `json:"oauthHeaders,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderOpenTelemetry `json:"extraHttpHeaders,omitempty"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingOpenTelemetry `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsOpenTelemetry  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool                               `default:"false" json:"responseHonorRetryAfterHeader"`
	TLS                           *TLSSettingsClientSideOpenTelemetry `json:"tls,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionOpenTelemetry `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorOpenTelemetry `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *CreateOutputModeOpenTelemetry `default:"error" json:"pqMode"`
	PqControls *PqControlsOpenTelemetry       `json:"pqControls,omitempty"`
}

func (o OutputOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputOpenTelemetry) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputOpenTelemetry) GetType() CreateOutputTypeOpenTelemetry {
	if o == nil {
		return CreateOutputTypeOpenTelemetry("")
	}
	return o.Type
}

func (o *OutputOpenTelemetry) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputOpenTelemetry) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputOpenTelemetry) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputOpenTelemetry) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputOpenTelemetry) GetProtocol() *CreateOutputProtocolOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputOpenTelemetry) GetEndpoint() string {
	if o == nil {
		return ""
	}
	return o.Endpoint
}

func (o *OutputOpenTelemetry) GetOtlpVersion() *CreateOutputOTLPVersionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OtlpVersion
}

func (o *OutputOpenTelemetry) GetCompress() *CreateOutputCompressCompressionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputOpenTelemetry) GetHTTPCompress() *HTTPCompressCompressionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.HTTPCompress
}

func (o *OutputOpenTelemetry) GetAuthType() *CreateOutputAuthenticationTypeOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputOpenTelemetry) GetHTTPTracesEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPTracesEndpointOverride
}

func (o *OutputOpenTelemetry) GetHTTPMetricsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPMetricsEndpointOverride
}

func (o *OutputOpenTelemetry) GetHTTPLogsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPLogsEndpointOverride
}

func (o *OutputOpenTelemetry) GetMetadata() []CreateOutputMetadatumOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.Metadata
}

func (o *OutputOpenTelemetry) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputOpenTelemetry) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputOpenTelemetry) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputOpenTelemetry) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputOpenTelemetry) GetFailedRequestLoggingMode() *FailedRequestLoggingModeOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputOpenTelemetry) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputOpenTelemetry) GetKeepAliveTime() *float64 {
	if o == nil {
		return nil
	}
	return o.KeepAliveTime
}

func (o *OutputOpenTelemetry) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputOpenTelemetry) GetOnBackpressure() *BackpressureBehaviorOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputOpenTelemetry) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputOpenTelemetry) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputOpenTelemetry) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputOpenTelemetry) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputOpenTelemetry) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputOpenTelemetry) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputOpenTelemetry) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputOpenTelemetry) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputOpenTelemetry) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputOpenTelemetry) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputOpenTelemetry) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputOpenTelemetry) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputOpenTelemetry) GetOauthParams() []CreateOutputOauthParamOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputOpenTelemetry) GetOauthHeaders() []CreateOutputOauthHeaderOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

func (o *OutputOpenTelemetry) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputOpenTelemetry) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputOpenTelemetry) GetExtraHTTPHeaders() []ExtraHTTPHeaderOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputOpenTelemetry) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputOpenTelemetry) GetResponseRetrySettings() []ResponseRetrySettingOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputOpenTelemetry) GetTimeoutRetrySettings() *TimeoutRetrySettingsOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputOpenTelemetry) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputOpenTelemetry) GetTLS() *TLSSettingsClientSideOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputOpenTelemetry) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputOpenTelemetry) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputOpenTelemetry) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputOpenTelemetry) GetPqCompress() *PqCompressCompressionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputOpenTelemetry) GetPqOnBackpressure() *QueueFullBehaviorOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputOpenTelemetry) GetPqMode() *CreateOutputModeOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputOpenTelemetry) GetPqControls() *PqControlsOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypePrometheus string

const (
	CreateOutputTypePrometheusPrometheus CreateOutputTypePrometheus = "prometheus"
)

func (e CreateOutputTypePrometheus) ToPointer() *CreateOutputTypePrometheus {
	return &e
}
func (e *CreateOutputTypePrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "prometheus":
		*e = CreateOutputTypePrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypePrometheus: %v", v)
	}
}

type ExtraHTTPHeaderPrometheus struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderPrometheus) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderPrometheus) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModePrometheus - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModePrometheus string

const (
	FailedRequestLoggingModePrometheusPayload           FailedRequestLoggingModePrometheus = "payload"
	FailedRequestLoggingModePrometheusPayloadAndHeaders FailedRequestLoggingModePrometheus = "payloadAndHeaders"
	FailedRequestLoggingModePrometheusNone              FailedRequestLoggingModePrometheus = "none"
)

func (e FailedRequestLoggingModePrometheus) ToPointer() *FailedRequestLoggingModePrometheus {
	return &e
}
func (e *FailedRequestLoggingModePrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModePrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModePrometheus: %v", v)
	}
}

type ResponseRetrySettingPrometheus struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingPrometheus) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingPrometheus) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingPrometheus) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingPrometheus) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsPrometheus struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsPrometheus) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsPrometheus) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsPrometheus) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsPrometheus) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorPrometheus - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorPrometheus string

const (
	BackpressureBehaviorPrometheusBlock BackpressureBehaviorPrometheus = "block"
	BackpressureBehaviorPrometheusDrop  BackpressureBehaviorPrometheus = "drop"
	BackpressureBehaviorPrometheusQueue BackpressureBehaviorPrometheus = "queue"
)

func (e BackpressureBehaviorPrometheus) ToPointer() *BackpressureBehaviorPrometheus {
	return &e
}
func (e *BackpressureBehaviorPrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorPrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorPrometheus: %v", v)
	}
}

// AuthenticationTypePrometheus - Remote Write authentication type
type AuthenticationTypePrometheus string

const (
	AuthenticationTypePrometheusNone              AuthenticationTypePrometheus = "none"
	AuthenticationTypePrometheusBasic             AuthenticationTypePrometheus = "basic"
	AuthenticationTypePrometheusCredentialsSecret AuthenticationTypePrometheus = "credentialsSecret"
	AuthenticationTypePrometheusToken             AuthenticationTypePrometheus = "token"
	AuthenticationTypePrometheusTextSecret        AuthenticationTypePrometheus = "textSecret"
	AuthenticationTypePrometheusOauth             AuthenticationTypePrometheus = "oauth"
)

func (e AuthenticationTypePrometheus) ToPointer() *AuthenticationTypePrometheus {
	return &e
}
func (e *AuthenticationTypePrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "basic":
		fallthrough
	case "credentialsSecret":
		fallthrough
	case "token":
		fallthrough
	case "textSecret":
		fallthrough
	case "oauth":
		*e = AuthenticationTypePrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationTypePrometheus: %v", v)
	}
}

// PqCompressCompressionPrometheus - Codec to use to compress the persisted data
type PqCompressCompressionPrometheus string

const (
	PqCompressCompressionPrometheusNone PqCompressCompressionPrometheus = "none"
	PqCompressCompressionPrometheusGzip PqCompressCompressionPrometheus = "gzip"
)

func (e PqCompressCompressionPrometheus) ToPointer() *PqCompressCompressionPrometheus {
	return &e
}
func (e *PqCompressCompressionPrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionPrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionPrometheus: %v", v)
	}
}

// QueueFullBehaviorPrometheus - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorPrometheus string

const (
	QueueFullBehaviorPrometheusBlock QueueFullBehaviorPrometheus = "block"
	QueueFullBehaviorPrometheusDrop  QueueFullBehaviorPrometheus = "drop"
)

func (e QueueFullBehaviorPrometheus) ToPointer() *QueueFullBehaviorPrometheus {
	return &e
}
func (e *QueueFullBehaviorPrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorPrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorPrometheus: %v", v)
	}
}

// CreateOutputModePrometheus - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModePrometheus string

const (
	CreateOutputModePrometheusError        CreateOutputModePrometheus = "error"
	CreateOutputModePrometheusBackpressure CreateOutputModePrometheus = "backpressure"
	CreateOutputModePrometheusAlways       CreateOutputModePrometheus = "always"
)

func (e CreateOutputModePrometheus) ToPointer() *CreateOutputModePrometheus {
	return &e
}
func (e *CreateOutputModePrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = CreateOutputModePrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputModePrometheus: %v", v)
	}
}

type PqControlsPrometheus struct {
}

type OauthParamPrometheus struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o *OauthParamPrometheus) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthParamPrometheus) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OauthHeaderPrometheus struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o *OauthHeaderPrometheus) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthHeaderPrometheus) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputPrometheus struct {
	// Unique ID for this output
	ID   string                     `json:"id"`
	Type CreateOutputTypePrometheus `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions to generated metrics.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint to send metrics to
	URL string `json:"url"`
	// JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.
	MetricRenameExpr *string `default:"name.replace(/[^a-zA-Z0-9_]/g, '_')" json:"metricRenameExpr"`
	// Generate and send metadata (`type` and `metricFamilyName`) requests
	SendMetadata *bool `default:"true" json:"sendMetadata"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderPrometheus `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModePrometheus `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingPrometheus `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsPrometheus  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorPrometheus `default:"block" json:"onBackpressure"`
	// Remote Write authentication type
	AuthType    *AuthenticationTypePrometheus `default:"none" json:"authType"`
	Description *string                       `json:"description,omitempty"`
	// How frequently metrics metadata is sent out. Value cannot be smaller than the base Flush period set above.
	MetricsFlushPeriodSec *float64 `default:"60" json:"metricsFlushPeriodSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionPrometheus `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorPrometheus `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *CreateOutputModePrometheus `default:"error" json:"pqMode"`
	PqControls *PqControlsPrometheus       `json:"pqControls,omitempty"`
	Username   *string                     `json:"username,omitempty"`
	Password   *string                     `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OauthParamPrometheus `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []OauthHeaderPrometheus `json:"oauthHeaders,omitempty"`
}

func (o OutputPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputPrometheus) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputPrometheus) GetType() CreateOutputTypePrometheus {
	if o == nil {
		return CreateOutputTypePrometheus("")
	}
	return o.Type
}

func (o *OutputPrometheus) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputPrometheus) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputPrometheus) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputPrometheus) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputPrometheus) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputPrometheus) GetMetricRenameExpr() *string {
	if o == nil {
		return nil
	}
	return o.MetricRenameExpr
}

func (o *OutputPrometheus) GetSendMetadata() *bool {
	if o == nil {
		return nil
	}
	return o.SendMetadata
}

func (o *OutputPrometheus) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputPrometheus) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputPrometheus) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputPrometheus) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputPrometheus) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputPrometheus) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputPrometheus) GetExtraHTTPHeaders() []ExtraHTTPHeaderPrometheus {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputPrometheus) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputPrometheus) GetFailedRequestLoggingMode() *FailedRequestLoggingModePrometheus {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputPrometheus) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputPrometheus) GetResponseRetrySettings() []ResponseRetrySettingPrometheus {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputPrometheus) GetTimeoutRetrySettings() *TimeoutRetrySettingsPrometheus {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputPrometheus) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputPrometheus) GetOnBackpressure() *BackpressureBehaviorPrometheus {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputPrometheus) GetAuthType() *AuthenticationTypePrometheus {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputPrometheus) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputPrometheus) GetMetricsFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MetricsFlushPeriodSec
}

func (o *OutputPrometheus) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputPrometheus) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputPrometheus) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputPrometheus) GetPqCompress() *PqCompressCompressionPrometheus {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputPrometheus) GetPqOnBackpressure() *QueueFullBehaviorPrometheus {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputPrometheus) GetPqMode() *CreateOutputModePrometheus {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputPrometheus) GetPqControls() *PqControlsPrometheus {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputPrometheus) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputPrometheus) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputPrometheus) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputPrometheus) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputPrometheus) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputPrometheus) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputPrometheus) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputPrometheus) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputPrometheus) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputPrometheus) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputPrometheus) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputPrometheus) GetOauthParams() []OauthParamPrometheus {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputPrometheus) GetOauthHeaders() []OauthHeaderPrometheus {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

type CreateOutputTypeLoki string

const (
	CreateOutputTypeLokiLoki CreateOutputTypeLoki = "loki"
)

func (e CreateOutputTypeLoki) ToPointer() *CreateOutputTypeLoki {
	return &e
}
func (e *CreateOutputTypeLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "loki":
		*e = CreateOutputTypeLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeLoki: %v", v)
	}
}

// MessageFormat - Format to use when sending logs to Loki (Protobuf or JSON)
type MessageFormat string

const (
	MessageFormatProtobuf MessageFormat = "protobuf"
	MessageFormatJSON     MessageFormat = "json"
)

func (e MessageFormat) ToPointer() *MessageFormat {
	return &e
}
func (e *MessageFormat) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "protobuf":
		fallthrough
	case "json":
		*e = MessageFormat(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MessageFormat: %v", v)
	}
}

type Label struct {
	Name  *string `default:"" json:"name"`
	Value string  `json:"value"`
}

func (l Label) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(l, "", false)
}

func (l *Label) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &l, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *Label) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *Label) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type CreateOutputAuthenticationTypeLoki string

const (
	CreateOutputAuthenticationTypeLokiNone              CreateOutputAuthenticationTypeLoki = "none"
	CreateOutputAuthenticationTypeLokiToken             CreateOutputAuthenticationTypeLoki = "token"
	CreateOutputAuthenticationTypeLokiTextSecret        CreateOutputAuthenticationTypeLoki = "textSecret"
	CreateOutputAuthenticationTypeLokiBasic             CreateOutputAuthenticationTypeLoki = "basic"
	CreateOutputAuthenticationTypeLokiCredentialsSecret CreateOutputAuthenticationTypeLoki = "credentialsSecret"
)

func (e CreateOutputAuthenticationTypeLoki) ToPointer() *CreateOutputAuthenticationTypeLoki {
	return &e
}
func (e *CreateOutputAuthenticationTypeLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "token":
		fallthrough
	case "textSecret":
		fallthrough
	case "basic":
		fallthrough
	case "credentialsSecret":
		*e = CreateOutputAuthenticationTypeLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputAuthenticationTypeLoki: %v", v)
	}
}

type ExtraHTTPHeaderLoki struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderLoki) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderLoki) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeLoki - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeLoki string

const (
	FailedRequestLoggingModeLokiPayload           FailedRequestLoggingModeLoki = "payload"
	FailedRequestLoggingModeLokiPayloadAndHeaders FailedRequestLoggingModeLoki = "payloadAndHeaders"
	FailedRequestLoggingModeLokiNone              FailedRequestLoggingModeLoki = "none"
)

func (e FailedRequestLoggingModeLoki) ToPointer() *FailedRequestLoggingModeLoki {
	return &e
}
func (e *FailedRequestLoggingModeLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeLoki: %v", v)
	}
}

type ResponseRetrySettingLoki struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingLoki) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingLoki) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingLoki) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingLoki) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsLoki struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsLoki) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsLoki) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsLoki) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsLoki) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorLoki - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorLoki string

const (
	BackpressureBehaviorLokiBlock BackpressureBehaviorLoki = "block"
	BackpressureBehaviorLokiDrop  BackpressureBehaviorLoki = "drop"
	BackpressureBehaviorLokiQueue BackpressureBehaviorLoki = "queue"
)

func (e BackpressureBehaviorLoki) ToPointer() *BackpressureBehaviorLoki {
	return &e
}
func (e *BackpressureBehaviorLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorLoki: %v", v)
	}
}

// PqCompressCompressionLoki - Codec to use to compress the persisted data
type PqCompressCompressionLoki string

const (
	PqCompressCompressionLokiNone PqCompressCompressionLoki = "none"
	PqCompressCompressionLokiGzip PqCompressCompressionLoki = "gzip"
)

func (e PqCompressCompressionLoki) ToPointer() *PqCompressCompressionLoki {
	return &e
}
func (e *PqCompressCompressionLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionLoki: %v", v)
	}
}

// QueueFullBehaviorLoki - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorLoki string

const (
	QueueFullBehaviorLokiBlock QueueFullBehaviorLoki = "block"
	QueueFullBehaviorLokiDrop  QueueFullBehaviorLoki = "drop"
)

func (e QueueFullBehaviorLoki) ToPointer() *QueueFullBehaviorLoki {
	return &e
}
func (e *QueueFullBehaviorLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorLoki: %v", v)
	}
}

// CreateOutputModeLoki - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeLoki string

const (
	CreateOutputModeLokiError        CreateOutputModeLoki = "error"
	CreateOutputModeLokiBackpressure CreateOutputModeLoki = "backpressure"
	CreateOutputModeLokiAlways       CreateOutputModeLoki = "always"
)

func (e CreateOutputModeLoki) ToPointer() *CreateOutputModeLoki {
	return &e
}
func (e *CreateOutputModeLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = CreateOutputModeLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputModeLoki: %v", v)
	}
}

type PqControlsLoki struct {
}

type OutputLoki struct {
	// Unique ID for this output
	ID   string               `json:"id"`
	Type CreateOutputTypeLoki `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as labels to generated logs.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint to send logs to
	URL string `json:"url"`
	// Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
	Message *string `json:"message,omitempty"`
	// Format to use when sending logs to Loki (Protobuf or JSON)
	MessageFormat *MessageFormat `default:"protobuf" json:"messageFormat"`
	// List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'
	Labels   []Label                             `json:"labels,omitempty"`
	AuthType *CreateOutputAuthenticationTypeLoki `default:"none" json:"authType"`
	// Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki to complain about entries being delivered out of order.
	Concurrency *float64 `default:"1" json:"concurrency"`
	// Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Defaults to 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.
	FlushPeriodSec *float64 `default:"15" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderLoki `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeLoki `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingLoki `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsLoki  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorLoki `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionLoki `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorLoki `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *CreateOutputModeLoki `default:"error" json:"pqMode"`
	PqControls *PqControlsLoki       `json:"pqControls,omitempty"`
}

func (o OutputLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputLoki) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputLoki) GetType() CreateOutputTypeLoki {
	if o == nil {
		return CreateOutputTypeLoki("")
	}
	return o.Type
}

func (o *OutputLoki) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputLoki) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputLoki) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputLoki) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputLoki) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputLoki) GetMessage() *string {
	if o == nil {
		return nil
	}
	return o.Message
}

func (o *OutputLoki) GetMessageFormat() *MessageFormat {
	if o == nil {
		return nil
	}
	return o.MessageFormat
}

func (o *OutputLoki) GetLabels() []Label {
	if o == nil {
		return nil
	}
	return o.Labels
}

func (o *OutputLoki) GetAuthType() *CreateOutputAuthenticationTypeLoki {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputLoki) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputLoki) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputLoki) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputLoki) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputLoki) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputLoki) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputLoki) GetExtraHTTPHeaders() []ExtraHTTPHeaderLoki {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputLoki) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputLoki) GetFailedRequestLoggingMode() *FailedRequestLoggingModeLoki {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputLoki) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputLoki) GetResponseRetrySettings() []ResponseRetrySettingLoki {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputLoki) GetTimeoutRetrySettings() *TimeoutRetrySettingsLoki {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputLoki) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputLoki) GetOnBackpressure() *BackpressureBehaviorLoki {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputLoki) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputLoki) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputLoki) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputLoki) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputLoki) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputLoki) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputLoki) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputLoki) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputLoki) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputLoki) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputLoki) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputLoki) GetPqCompress() *PqCompressCompressionLoki {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputLoki) GetPqOnBackpressure() *QueueFullBehaviorLoki {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputLoki) GetPqMode() *CreateOutputModeLoki {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputLoki) GetPqControls() *PqControlsLoki {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeSumoLogic string

const (
	TypeSumoLogicSumoLogic TypeSumoLogic = "sumo_logic"
)

func (e TypeSumoLogic) ToPointer() *TypeSumoLogic {
	return &e
}
func (e *TypeSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sumo_logic":
		*e = TypeSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSumoLogic: %v", v)
	}
}

// DataFormatSumoLogic - Preserve the raw event format instead of JSONifying it
type DataFormatSumoLogic string

const (
	DataFormatSumoLogicJSON DataFormatSumoLogic = "json"
	DataFormatSumoLogicRaw  DataFormatSumoLogic = "raw"
)

func (e DataFormatSumoLogic) ToPointer() *DataFormatSumoLogic {
	return &e
}
func (e *DataFormatSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		*e = DataFormatSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataFormatSumoLogic: %v", v)
	}
}

type ExtraHTTPHeaderSumoLogic struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderSumoLogic) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderSumoLogic) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeSumoLogic - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSumoLogic string

const (
	FailedRequestLoggingModeSumoLogicPayload           FailedRequestLoggingModeSumoLogic = "payload"
	FailedRequestLoggingModeSumoLogicPayloadAndHeaders FailedRequestLoggingModeSumoLogic = "payloadAndHeaders"
	FailedRequestLoggingModeSumoLogicNone              FailedRequestLoggingModeSumoLogic = "none"
)

func (e FailedRequestLoggingModeSumoLogic) ToPointer() *FailedRequestLoggingModeSumoLogic {
	return &e
}
func (e *FailedRequestLoggingModeSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeSumoLogic: %v", v)
	}
}

type ResponseRetrySettingSumoLogic struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingSumoLogic) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingSumoLogic) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingSumoLogic) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingSumoLogic) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsSumoLogic struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsSumoLogic) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsSumoLogic) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsSumoLogic) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsSumoLogic) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorSumoLogic - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSumoLogic string

const (
	BackpressureBehaviorSumoLogicBlock BackpressureBehaviorSumoLogic = "block"
	BackpressureBehaviorSumoLogicDrop  BackpressureBehaviorSumoLogic = "drop"
	BackpressureBehaviorSumoLogicQueue BackpressureBehaviorSumoLogic = "queue"
)

func (e BackpressureBehaviorSumoLogic) ToPointer() *BackpressureBehaviorSumoLogic {
	return &e
}
func (e *BackpressureBehaviorSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSumoLogic: %v", v)
	}
}

// CompressionSumoLogic - Codec to use to compress the persisted data
type CompressionSumoLogic string

const (
	CompressionSumoLogicNone CompressionSumoLogic = "none"
	CompressionSumoLogicGzip CompressionSumoLogic = "gzip"
)

func (e CompressionSumoLogic) ToPointer() *CompressionSumoLogic {
	return &e
}
func (e *CompressionSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionSumoLogic: %v", v)
	}
}

// QueueFullBehaviorSumoLogic - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSumoLogic string

const (
	QueueFullBehaviorSumoLogicBlock QueueFullBehaviorSumoLogic = "block"
	QueueFullBehaviorSumoLogicDrop  QueueFullBehaviorSumoLogic = "drop"
)

func (e QueueFullBehaviorSumoLogic) ToPointer() *QueueFullBehaviorSumoLogic {
	return &e
}
func (e *QueueFullBehaviorSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSumoLogic: %v", v)
	}
}

// ModeSumoLogic - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSumoLogic string

const (
	ModeSumoLogicError        ModeSumoLogic = "error"
	ModeSumoLogicBackpressure ModeSumoLogic = "backpressure"
	ModeSumoLogicAlways       ModeSumoLogic = "always"
)

func (e ModeSumoLogic) ToPointer() *ModeSumoLogic {
	return &e
}
func (e *ModeSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeSumoLogic: %v", v)
	}
}

type PqControlsSumoLogic struct {
}

type OutputSumoLogic struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type TypeSumoLogic `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Sumo Logic HTTP collector URL to which events should be sent
	URL string `json:"url"`
	// Override the source name configured on the SumoLogic HTTP collector. This can also be overridden at the event level with the __sourceName field.
	CustomSource *string `json:"customSource,omitempty"`
	// Override the source category configured on the SumoLogic HTTP collector. This can also be overridden at the event level with the __sourceCategory field.
	CustomCategory *string `json:"customCategory,omitempty"`
	// Preserve the raw event format instead of JSONifying it
	Format *DataFormatSumoLogic `default:"json" json:"format"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderSumoLogic `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSumoLogic `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingSumoLogic `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSumoLogic  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSumoLogic `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSumoLogic `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSumoLogic `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeSumoLogic       `default:"error" json:"pqMode"`
	PqControls *PqControlsSumoLogic `json:"pqControls,omitempty"`
}

func (o OutputSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSumoLogic) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSumoLogic) GetType() TypeSumoLogic {
	if o == nil {
		return TypeSumoLogic("")
	}
	return o.Type
}

func (o *OutputSumoLogic) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSumoLogic) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSumoLogic) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSumoLogic) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSumoLogic) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputSumoLogic) GetCustomSource() *string {
	if o == nil {
		return nil
	}
	return o.CustomSource
}

func (o *OutputSumoLogic) GetCustomCategory() *string {
	if o == nil {
		return nil
	}
	return o.CustomCategory
}

func (o *OutputSumoLogic) GetFormat() *DataFormatSumoLogic {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputSumoLogic) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSumoLogic) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSumoLogic) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSumoLogic) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSumoLogic) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSumoLogic) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSumoLogic) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSumoLogic) GetExtraHTTPHeaders() []ExtraHTTPHeaderSumoLogic {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSumoLogic) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputSumoLogic) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSumoLogic {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSumoLogic) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSumoLogic) GetResponseRetrySettings() []ResponseRetrySettingSumoLogic {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSumoLogic) GetTimeoutRetrySettings() *TimeoutRetrySettingsSumoLogic {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSumoLogic) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSumoLogic) GetOnBackpressure() *BackpressureBehaviorSumoLogic {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSumoLogic) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputSumoLogic) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSumoLogic) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSumoLogic) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSumoLogic) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSumoLogic) GetPqCompress() *CompressionSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSumoLogic) GetPqOnBackpressure() *QueueFullBehaviorSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSumoLogic) GetPqMode() *ModeSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSumoLogic) GetPqControls() *PqControlsSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeSnmp string

const (
	CreateOutputTypeSnmpSnmp CreateOutputTypeSnmp = "snmp"
)

func (e CreateOutputTypeSnmp) ToPointer() *CreateOutputTypeSnmp {
	return &e
}
func (e *CreateOutputTypeSnmp) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "snmp":
		*e = CreateOutputTypeSnmp(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeSnmp: %v", v)
	}
}

type HostSnmp struct {
	// Destination host
	Host string `json:"host"`
	// Destination port, default is 162
	Port *float64 `default:"162" json:"port"`
}

func (h HostSnmp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostSnmp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *HostSnmp) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *HostSnmp) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

type OutputSnmp struct {
	// Unique ID for this output
	ID   string               `json:"id"`
	Type CreateOutputTypeSnmp `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// One or more SNMP destinations to forward traps to
	Hosts []HostSnmp `json:"hosts"`
	// How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every trap sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string  `json:"description,omitempty"`
}

func (o OutputSnmp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSnmp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSnmp) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSnmp) GetType() CreateOutputTypeSnmp {
	if o == nil {
		return CreateOutputTypeSnmp("")
	}
	return o.Type
}

func (o *OutputSnmp) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSnmp) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSnmp) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSnmp) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSnmp) GetHosts() []HostSnmp {
	if o == nil {
		return []HostSnmp{}
	}
	return o.Hosts
}

func (o *OutputSnmp) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputSnmp) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

type CreateOutputTypeSqs string

const (
	CreateOutputTypeSqsSqs CreateOutputTypeSqs = "sqs"
)

func (e CreateOutputTypeSqs) ToPointer() *CreateOutputTypeSqs {
	return &e
}
func (e *CreateOutputTypeSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sqs":
		*e = CreateOutputTypeSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeSqs: %v", v)
	}
}

// CreateOutputQueueType - The queue type used (or created). Defaults to Standard.
type CreateOutputQueueType string

const (
	CreateOutputQueueTypeStandard CreateOutputQueueType = "standard"
	CreateOutputQueueTypeFifo     CreateOutputQueueType = "fifo"
)

func (e CreateOutputQueueType) ToPointer() *CreateOutputQueueType {
	return &e
}
func (e *CreateOutputQueueType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "standard":
		fallthrough
	case "fifo":
		*e = CreateOutputQueueType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputQueueType: %v", v)
	}
}

// CreateOutputAuthenticationMethodSqs - AWS authentication method. Choose Auto to use IAM roles.
type CreateOutputAuthenticationMethodSqs string

const (
	CreateOutputAuthenticationMethodSqsAuto   CreateOutputAuthenticationMethodSqs = "auto"
	CreateOutputAuthenticationMethodSqsManual CreateOutputAuthenticationMethodSqs = "manual"
	CreateOutputAuthenticationMethodSqsSecret CreateOutputAuthenticationMethodSqs = "secret"
)

func (e CreateOutputAuthenticationMethodSqs) ToPointer() *CreateOutputAuthenticationMethodSqs {
	return &e
}
func (e *CreateOutputAuthenticationMethodSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = CreateOutputAuthenticationMethodSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputAuthenticationMethodSqs: %v", v)
	}
}

// CreateOutputSignatureVersionSqs - Signature version to use for signing SQS requests
type CreateOutputSignatureVersionSqs string

const (
	CreateOutputSignatureVersionSqsV2 CreateOutputSignatureVersionSqs = "v2"
	CreateOutputSignatureVersionSqsV4 CreateOutputSignatureVersionSqs = "v4"
)

func (e CreateOutputSignatureVersionSqs) ToPointer() *CreateOutputSignatureVersionSqs {
	return &e
}
func (e *CreateOutputSignatureVersionSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = CreateOutputSignatureVersionSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputSignatureVersionSqs: %v", v)
	}
}

// BackpressureBehaviorSqs - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSqs string

const (
	BackpressureBehaviorSqsBlock BackpressureBehaviorSqs = "block"
	BackpressureBehaviorSqsDrop  BackpressureBehaviorSqs = "drop"
	BackpressureBehaviorSqsQueue BackpressureBehaviorSqs = "queue"
)

func (e BackpressureBehaviorSqs) ToPointer() *BackpressureBehaviorSqs {
	return &e
}
func (e *BackpressureBehaviorSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSqs: %v", v)
	}
}

// PqCompressCompressionSqs - Codec to use to compress the persisted data
type PqCompressCompressionSqs string

const (
	PqCompressCompressionSqsNone PqCompressCompressionSqs = "none"
	PqCompressCompressionSqsGzip PqCompressCompressionSqs = "gzip"
)

func (e PqCompressCompressionSqs) ToPointer() *PqCompressCompressionSqs {
	return &e
}
func (e *PqCompressCompressionSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionSqs: %v", v)
	}
}

// QueueFullBehaviorSqs - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSqs string

const (
	QueueFullBehaviorSqsBlock QueueFullBehaviorSqs = "block"
	QueueFullBehaviorSqsDrop  QueueFullBehaviorSqs = "drop"
)

func (e QueueFullBehaviorSqs) ToPointer() *QueueFullBehaviorSqs {
	return &e
}
func (e *QueueFullBehaviorSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSqs: %v", v)
	}
}

// CreateOutputModeSqs - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeSqs string

const (
	CreateOutputModeSqsError        CreateOutputModeSqs = "error"
	CreateOutputModeSqsBackpressure CreateOutputModeSqs = "backpressure"
	CreateOutputModeSqsAlways       CreateOutputModeSqs = "always"
)

func (e CreateOutputModeSqs) ToPointer() *CreateOutputModeSqs {
	return &e
}
func (e *CreateOutputModeSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = CreateOutputModeSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputModeSqs: %v", v)
	}
}

type PqControlsSqs struct {
}

type OutputSqs struct {
	// Unique ID for this output
	ID   string               `json:"id"`
	Type *CreateOutputTypeSqs `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The name, URL, or ARN of the SQS queue to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
	QueueName string `json:"queueName"`
	// The queue type used (or created). Defaults to Standard.
	QueueType *CreateOutputQueueType `default:"standard" json:"queueType"`
	// SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
	AwsAccountID *string `json:"awsAccountId,omitempty"`
	// This parameter applies only to FIFO queues. The tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are processed in a FIFO manner. Use event field __messageGroupId to override this value.
	MessageGroupID *string `default:"cribl" json:"messageGroupId"`
	// Create queue if it does not exist.
	CreateQueue *bool `default:"true" json:"createQueue"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *CreateOutputAuthenticationMethodSqs `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                              `json:"awsSecretKey,omitempty"`
	// AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.
	Region *string `json:"region,omitempty"`
	// SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing SQS requests
	SignatureVersion *CreateOutputSignatureVersionSqs `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access SQS
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Maximum number of queued batches before blocking.
	MaxQueueSize *float64 `default:"100" json:"maxQueueSize"`
	// Maximum size (KB) of batches to send. Per the SQS spec, the max allowed value is 256 KB.
	MaxRecordSizeKB *float64 `default:"256" json:"maxRecordSizeKB"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// The maximum number of in-progress API requests before backpressure is applied.
	MaxInProgress *float64 `default:"10" json:"maxInProgress"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSqs `default:"block" json:"onBackpressure"`
	Description    *string                  `json:"description,omitempty"`
	AwsAPIKey      *string                  `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionSqs `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSqs `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *CreateOutputModeSqs `default:"error" json:"pqMode"`
	PqControls *PqControlsSqs       `json:"pqControls,omitempty"`
}

func (o OutputSqs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSqs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSqs) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSqs) GetType() *CreateOutputTypeSqs {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputSqs) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSqs) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSqs) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSqs) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSqs) GetQueueName() string {
	if o == nil {
		return ""
	}
	return o.QueueName
}

func (o *OutputSqs) GetQueueType() *CreateOutputQueueType {
	if o == nil {
		return nil
	}
	return o.QueueType
}

func (o *OutputSqs) GetAwsAccountID() *string {
	if o == nil {
		return nil
	}
	return o.AwsAccountID
}

func (o *OutputSqs) GetMessageGroupID() *string {
	if o == nil {
		return nil
	}
	return o.MessageGroupID
}

func (o *OutputSqs) GetCreateQueue() *bool {
	if o == nil {
		return nil
	}
	return o.CreateQueue
}

func (o *OutputSqs) GetAwsAuthenticationMethod() *CreateOutputAuthenticationMethodSqs {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputSqs) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputSqs) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputSqs) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputSqs) GetSignatureVersion() *CreateOutputSignatureVersionSqs {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputSqs) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputSqs) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSqs) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputSqs) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputSqs) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputSqs) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputSqs) GetMaxQueueSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxQueueSize
}

func (o *OutputSqs) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputSqs) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSqs) GetMaxInProgress() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxInProgress
}

func (o *OutputSqs) GetOnBackpressure() *BackpressureBehaviorSqs {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSqs) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSqs) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputSqs) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputSqs) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSqs) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSqs) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSqs) GetPqCompress() *PqCompressCompressionSqs {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSqs) GetPqOnBackpressure() *QueueFullBehaviorSqs {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSqs) GetPqMode() *CreateOutputModeSqs {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSqs) GetPqControls() *PqControlsSqs {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeSns string

const (
	TypeSnsSns TypeSns = "sns"
)

func (e TypeSns) ToPointer() *TypeSns {
	return &e
}
func (e *TypeSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sns":
		*e = TypeSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSns: %v", v)
	}
}

// AuthenticationMethodSns - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodSns string

const (
	AuthenticationMethodSnsAuto   AuthenticationMethodSns = "auto"
	AuthenticationMethodSnsManual AuthenticationMethodSns = "manual"
	AuthenticationMethodSnsSecret AuthenticationMethodSns = "secret"
)

func (e AuthenticationMethodSns) ToPointer() *AuthenticationMethodSns {
	return &e
}
func (e *AuthenticationMethodSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodSns: %v", v)
	}
}

// SignatureVersionSns - Signature version to use for signing SNS requests
type SignatureVersionSns string

const (
	SignatureVersionSnsV2 SignatureVersionSns = "v2"
	SignatureVersionSnsV4 SignatureVersionSns = "v4"
)

func (e SignatureVersionSns) ToPointer() *SignatureVersionSns {
	return &e
}
func (e *SignatureVersionSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = SignatureVersionSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SignatureVersionSns: %v", v)
	}
}

// BackpressureBehaviorSns - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSns string

const (
	BackpressureBehaviorSnsBlock BackpressureBehaviorSns = "block"
	BackpressureBehaviorSnsDrop  BackpressureBehaviorSns = "drop"
	BackpressureBehaviorSnsQueue BackpressureBehaviorSns = "queue"
)

func (e BackpressureBehaviorSns) ToPointer() *BackpressureBehaviorSns {
	return &e
}
func (e *BackpressureBehaviorSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSns: %v", v)
	}
}

// CompressionSns - Codec to use to compress the persisted data
type CompressionSns string

const (
	CompressionSnsNone CompressionSns = "none"
	CompressionSnsGzip CompressionSns = "gzip"
)

func (e CompressionSns) ToPointer() *CompressionSns {
	return &e
}
func (e *CompressionSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionSns: %v", v)
	}
}

// QueueFullBehaviorSns - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSns string

const (
	QueueFullBehaviorSnsBlock QueueFullBehaviorSns = "block"
	QueueFullBehaviorSnsDrop  QueueFullBehaviorSns = "drop"
)

func (e QueueFullBehaviorSns) ToPointer() *QueueFullBehaviorSns {
	return &e
}
func (e *QueueFullBehaviorSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSns: %v", v)
	}
}

// ModeSns - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSns string

const (
	ModeSnsError        ModeSns = "error"
	ModeSnsBackpressure ModeSns = "backpressure"
	ModeSnsAlways       ModeSns = "always"
)

func (e ModeSns) ToPointer() *ModeSns {
	return &e
}
func (e *ModeSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeSns: %v", v)
	}
}

type PqControlsSns struct {
}

type OutputSns struct {
	// Unique ID for this output
	ID   string   `json:"id"`
	Type *TypeSns `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The ARN of the SNS topic to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. E.g., 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`
	TopicArn string `json:"topicArn"`
	// Messages in the same group are processed in a FIFO manner. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
	MessageGroupID string `json:"messageGroupId"`
	// Maximum number of retries before the output returns an error. Note that not all errors are retryable. The retries use an exponential backoff policy.
	MaxRetries *float64 `json:"maxRetries,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodSns `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                  `json:"awsSecretKey,omitempty"`
	// Region where the SNS is located
	Region *string `json:"region,omitempty"`
	// SNS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SNS-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing SNS requests
	SignatureVersion *SignatureVersionSns `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access SNS
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSns `default:"block" json:"onBackpressure"`
	Description    *string                  `json:"description,omitempty"`
	AwsAPIKey      *string                  `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSns `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSns `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeSns       `default:"error" json:"pqMode"`
	PqControls *PqControlsSns `json:"pqControls,omitempty"`
}

func (o OutputSns) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSns) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSns) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSns) GetType() *TypeSns {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputSns) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSns) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSns) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSns) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSns) GetTopicArn() string {
	if o == nil {
		return ""
	}
	return o.TopicArn
}

func (o *OutputSns) GetMessageGroupID() string {
	if o == nil {
		return ""
	}
	return o.MessageGroupID
}

func (o *OutputSns) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputSns) GetAwsAuthenticationMethod() *AuthenticationMethodSns {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputSns) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputSns) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputSns) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputSns) GetSignatureVersion() *SignatureVersionSns {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputSns) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputSns) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSns) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputSns) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputSns) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputSns) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputSns) GetOnBackpressure() *BackpressureBehaviorSns {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSns) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSns) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputSns) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputSns) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSns) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSns) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSns) GetPqCompress() *CompressionSns {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSns) GetPqOnBackpressure() *QueueFullBehaviorSns {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSns) GetPqMode() *ModeSns {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSns) GetPqControls() *PqControlsSns {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeRouter string

const (
	TypeRouterRouter TypeRouter = "router"
)

func (e TypeRouter) ToPointer() *TypeRouter {
	return &e
}
func (e *TypeRouter) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "router":
		*e = TypeRouter(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeRouter: %v", v)
	}
}

type CreateOutputRule struct {
	// JavaScript expression to select events to send to output
	Filter string `json:"filter"`
	// Output to send matching events to
	Output string `json:"output"`
	// Description of this rule's purpose
	Description *string `json:"description,omitempty"`
	// Flag to control whether to stop the event from being checked against other rules
	Final *bool `default:"true" json:"final"`
}

func (c CreateOutputRule) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputRule) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputRule) GetFilter() string {
	if o == nil {
		return ""
	}
	return o.Filter
}

func (o *CreateOutputRule) GetOutput() string {
	if o == nil {
		return ""
	}
	return o.Output
}

func (o *CreateOutputRule) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *CreateOutputRule) GetFinal() *bool {
	if o == nil {
		return nil
	}
	return o.Final
}

type OutputRouter struct {
	// Unique ID for this output
	ID   string     `json:"id"`
	Type TypeRouter `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Event routing rules
	Rules       []CreateOutputRule `json:"rules"`
	Description *string            `json:"description,omitempty"`
}

func (o *OutputRouter) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputRouter) GetType() TypeRouter {
	if o == nil {
		return TypeRouter("")
	}
	return o.Type
}

func (o *OutputRouter) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputRouter) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputRouter) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputRouter) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputRouter) GetRules() []CreateOutputRule {
	if o == nil {
		return []CreateOutputRule{}
	}
	return o.Rules
}

func (o *OutputRouter) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

type TypeGraphite string

const (
	TypeGraphiteGraphite TypeGraphite = "graphite"
)

func (e TypeGraphite) ToPointer() *TypeGraphite {
	return &e
}
func (e *TypeGraphite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "graphite":
		*e = TypeGraphite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGraphite: %v", v)
	}
}

// DestinationProtocolGraphite - Protocol to use when communicating with the destination.
type DestinationProtocolGraphite string

const (
	DestinationProtocolGraphiteUDP DestinationProtocolGraphite = "udp"
	DestinationProtocolGraphiteTCP DestinationProtocolGraphite = "tcp"
)

func (e DestinationProtocolGraphite) ToPointer() *DestinationProtocolGraphite {
	return &e
}
func (e *DestinationProtocolGraphite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "udp":
		fallthrough
	case "tcp":
		*e = DestinationProtocolGraphite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationProtocolGraphite: %v", v)
	}
}

// BackpressureBehaviorGraphite - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorGraphite string

const (
	BackpressureBehaviorGraphiteBlock BackpressureBehaviorGraphite = "block"
	BackpressureBehaviorGraphiteDrop  BackpressureBehaviorGraphite = "drop"
	BackpressureBehaviorGraphiteQueue BackpressureBehaviorGraphite = "queue"
)

func (e BackpressureBehaviorGraphite) ToPointer() *BackpressureBehaviorGraphite {
	return &e
}
func (e *BackpressureBehaviorGraphite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorGraphite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorGraphite: %v", v)
	}
}

// CompressionGraphite - Codec to use to compress the persisted data
type CompressionGraphite string

const (
	CompressionGraphiteNone CompressionGraphite = "none"
	CompressionGraphiteGzip CompressionGraphite = "gzip"
)

func (e CompressionGraphite) ToPointer() *CompressionGraphite {
	return &e
}
func (e *CompressionGraphite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionGraphite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionGraphite: %v", v)
	}
}

// QueueFullBehaviorGraphite - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGraphite string

const (
	QueueFullBehaviorGraphiteBlock QueueFullBehaviorGraphite = "block"
	QueueFullBehaviorGraphiteDrop  QueueFullBehaviorGraphite = "drop"
)

func (e QueueFullBehaviorGraphite) ToPointer() *QueueFullBehaviorGraphite {
	return &e
}
func (e *QueueFullBehaviorGraphite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorGraphite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorGraphite: %v", v)
	}
}

// ModeGraphite - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeGraphite string

const (
	ModeGraphiteError        ModeGraphite = "error"
	ModeGraphiteBackpressure ModeGraphite = "backpressure"
	ModeGraphiteAlways       ModeGraphite = "always"
)

func (e ModeGraphite) ToPointer() *ModeGraphite {
	return &e
}
func (e *ModeGraphite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeGraphite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeGraphite: %v", v)
	}
}

type PqControlsGraphite struct {
}

type OutputGraphite struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type *TypeGraphite `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Protocol to use when communicating with the destination.
	Protocol *DestinationProtocolGraphite `default:"udp" json:"protocol"`
	// The hostname of the destination.
	Host string `json:"host"`
	// Destination port.
	Port *float64 `default:"8125" json:"port"`
	// When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.
	Mtu *float64 `default:"512" json:"mtu"`
	// When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string  `json:"description,omitempty"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGraphite `default:"block" json:"onBackpressure"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionGraphite `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorGraphite `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeGraphite       `default:"error" json:"pqMode"`
	PqControls *PqControlsGraphite `json:"pqControls,omitempty"`
}

func (o OutputGraphite) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGraphite) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputGraphite) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputGraphite) GetType() *TypeGraphite {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputGraphite) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGraphite) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGraphite) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGraphite) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGraphite) GetProtocol() *DestinationProtocolGraphite {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputGraphite) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputGraphite) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputGraphite) GetMtu() *float64 {
	if o == nil {
		return nil
	}
	return o.Mtu
}

func (o *OutputGraphite) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGraphite) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputGraphite) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGraphite) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputGraphite) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputGraphite) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputGraphite) GetOnBackpressure() *BackpressureBehaviorGraphite {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGraphite) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGraphite) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGraphite) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGraphite) GetPqCompress() *CompressionGraphite {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGraphite) GetPqOnBackpressure() *QueueFullBehaviorGraphite {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGraphite) GetPqMode() *ModeGraphite {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGraphite) GetPqControls() *PqControlsGraphite {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeStatsdExt string

const (
	TypeStatsdExtStatsdExt TypeStatsdExt = "statsd_ext"
)

func (e TypeStatsdExt) ToPointer() *TypeStatsdExt {
	return &e
}
func (e *TypeStatsdExt) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "statsd_ext":
		*e = TypeStatsdExt(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeStatsdExt: %v", v)
	}
}

// DestinationProtocolStatsdExt - Protocol to use when communicating with the destination.
type DestinationProtocolStatsdExt string

const (
	DestinationProtocolStatsdExtUDP DestinationProtocolStatsdExt = "udp"
	DestinationProtocolStatsdExtTCP DestinationProtocolStatsdExt = "tcp"
)

func (e DestinationProtocolStatsdExt) ToPointer() *DestinationProtocolStatsdExt {
	return &e
}
func (e *DestinationProtocolStatsdExt) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "udp":
		fallthrough
	case "tcp":
		*e = DestinationProtocolStatsdExt(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationProtocolStatsdExt: %v", v)
	}
}

// BackpressureBehaviorStatsdExt - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorStatsdExt string

const (
	BackpressureBehaviorStatsdExtBlock BackpressureBehaviorStatsdExt = "block"
	BackpressureBehaviorStatsdExtDrop  BackpressureBehaviorStatsdExt = "drop"
	BackpressureBehaviorStatsdExtQueue BackpressureBehaviorStatsdExt = "queue"
)

func (e BackpressureBehaviorStatsdExt) ToPointer() *BackpressureBehaviorStatsdExt {
	return &e
}
func (e *BackpressureBehaviorStatsdExt) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorStatsdExt(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorStatsdExt: %v", v)
	}
}

// CompressionStatsdExt - Codec to use to compress the persisted data
type CompressionStatsdExt string

const (
	CompressionStatsdExtNone CompressionStatsdExt = "none"
	CompressionStatsdExtGzip CompressionStatsdExt = "gzip"
)

func (e CompressionStatsdExt) ToPointer() *CompressionStatsdExt {
	return &e
}
func (e *CompressionStatsdExt) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionStatsdExt(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionStatsdExt: %v", v)
	}
}

// QueueFullBehaviorStatsdExt - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorStatsdExt string

const (
	QueueFullBehaviorStatsdExtBlock QueueFullBehaviorStatsdExt = "block"
	QueueFullBehaviorStatsdExtDrop  QueueFullBehaviorStatsdExt = "drop"
)

func (e QueueFullBehaviorStatsdExt) ToPointer() *QueueFullBehaviorStatsdExt {
	return &e
}
func (e *QueueFullBehaviorStatsdExt) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorStatsdExt(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorStatsdExt: %v", v)
	}
}

// ModeStatsdExt - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeStatsdExt string

const (
	ModeStatsdExtError        ModeStatsdExt = "error"
	ModeStatsdExtBackpressure ModeStatsdExt = "backpressure"
	ModeStatsdExtAlways       ModeStatsdExt = "always"
)

func (e ModeStatsdExt) ToPointer() *ModeStatsdExt {
	return &e
}
func (e *ModeStatsdExt) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeStatsdExt(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeStatsdExt: %v", v)
	}
}

type PqControlsStatsdExt struct {
}

type OutputStatsdExt struct {
	// Unique ID for this output
	ID   string         `json:"id"`
	Type *TypeStatsdExt `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Protocol to use when communicating with the destination.
	Protocol *DestinationProtocolStatsdExt `default:"udp" json:"protocol"`
	// The hostname of the destination.
	Host string `json:"host"`
	// Destination port.
	Port *float64 `default:"8125" json:"port"`
	// When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.
	Mtu *float64 `default:"512" json:"mtu"`
	// When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string  `json:"description,omitempty"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorStatsdExt `default:"block" json:"onBackpressure"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionStatsdExt `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorStatsdExt `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeStatsdExt       `default:"error" json:"pqMode"`
	PqControls *PqControlsStatsdExt `json:"pqControls,omitempty"`
}

func (o OutputStatsdExt) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputStatsdExt) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputStatsdExt) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputStatsdExt) GetType() *TypeStatsdExt {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputStatsdExt) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputStatsdExt) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputStatsdExt) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputStatsdExt) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputStatsdExt) GetProtocol() *DestinationProtocolStatsdExt {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputStatsdExt) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputStatsdExt) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputStatsdExt) GetMtu() *float64 {
	if o == nil {
		return nil
	}
	return o.Mtu
}

func (o *OutputStatsdExt) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputStatsdExt) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputStatsdExt) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputStatsdExt) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputStatsdExt) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputStatsdExt) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputStatsdExt) GetOnBackpressure() *BackpressureBehaviorStatsdExt {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputStatsdExt) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputStatsdExt) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputStatsdExt) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputStatsdExt) GetPqCompress() *CompressionStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputStatsdExt) GetPqOnBackpressure() *QueueFullBehaviorStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputStatsdExt) GetPqMode() *ModeStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputStatsdExt) GetPqControls() *PqControlsStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeStatsd string

const (
	TypeStatsdStatsd TypeStatsd = "statsd"
)

func (e TypeStatsd) ToPointer() *TypeStatsd {
	return &e
}
func (e *TypeStatsd) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "statsd":
		*e = TypeStatsd(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeStatsd: %v", v)
	}
}

// DestinationProtocolStatsd - Protocol to use when communicating with the destination.
type DestinationProtocolStatsd string

const (
	DestinationProtocolStatsdUDP DestinationProtocolStatsd = "udp"
	DestinationProtocolStatsdTCP DestinationProtocolStatsd = "tcp"
)

func (e DestinationProtocolStatsd) ToPointer() *DestinationProtocolStatsd {
	return &e
}
func (e *DestinationProtocolStatsd) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "udp":
		fallthrough
	case "tcp":
		*e = DestinationProtocolStatsd(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationProtocolStatsd: %v", v)
	}
}

// BackpressureBehaviorStatsd - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorStatsd string

const (
	BackpressureBehaviorStatsdBlock BackpressureBehaviorStatsd = "block"
	BackpressureBehaviorStatsdDrop  BackpressureBehaviorStatsd = "drop"
	BackpressureBehaviorStatsdQueue BackpressureBehaviorStatsd = "queue"
)

func (e BackpressureBehaviorStatsd) ToPointer() *BackpressureBehaviorStatsd {
	return &e
}
func (e *BackpressureBehaviorStatsd) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorStatsd(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorStatsd: %v", v)
	}
}

// CompressionStatsd - Codec to use to compress the persisted data
type CompressionStatsd string

const (
	CompressionStatsdNone CompressionStatsd = "none"
	CompressionStatsdGzip CompressionStatsd = "gzip"
)

func (e CompressionStatsd) ToPointer() *CompressionStatsd {
	return &e
}
func (e *CompressionStatsd) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionStatsd(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionStatsd: %v", v)
	}
}

// QueueFullBehaviorStatsd - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorStatsd string

const (
	QueueFullBehaviorStatsdBlock QueueFullBehaviorStatsd = "block"
	QueueFullBehaviorStatsdDrop  QueueFullBehaviorStatsd = "drop"
)

func (e QueueFullBehaviorStatsd) ToPointer() *QueueFullBehaviorStatsd {
	return &e
}
func (e *QueueFullBehaviorStatsd) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorStatsd(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorStatsd: %v", v)
	}
}

// ModeStatsd - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeStatsd string

const (
	ModeStatsdError        ModeStatsd = "error"
	ModeStatsdBackpressure ModeStatsd = "backpressure"
	ModeStatsdAlways       ModeStatsd = "always"
)

func (e ModeStatsd) ToPointer() *ModeStatsd {
	return &e
}
func (e *ModeStatsd) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeStatsd(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeStatsd: %v", v)
	}
}

type PqControlsStatsd struct {
}

type OutputStatsd struct {
	// Unique ID for this output
	ID   string      `json:"id"`
	Type *TypeStatsd `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Protocol to use when communicating with the destination.
	Protocol *DestinationProtocolStatsd `default:"udp" json:"protocol"`
	// The hostname of the destination.
	Host string `json:"host"`
	// Destination port.
	Port *float64 `default:"8125" json:"port"`
	// When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.
	Mtu *float64 `default:"512" json:"mtu"`
	// When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string  `json:"description,omitempty"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorStatsd `default:"block" json:"onBackpressure"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionStatsd `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorStatsd `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeStatsd       `default:"error" json:"pqMode"`
	PqControls *PqControlsStatsd `json:"pqControls,omitempty"`
}

func (o OutputStatsd) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputStatsd) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputStatsd) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputStatsd) GetType() *TypeStatsd {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputStatsd) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputStatsd) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputStatsd) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputStatsd) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputStatsd) GetProtocol() *DestinationProtocolStatsd {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputStatsd) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputStatsd) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputStatsd) GetMtu() *float64 {
	if o == nil {
		return nil
	}
	return o.Mtu
}

func (o *OutputStatsd) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputStatsd) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputStatsd) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputStatsd) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputStatsd) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputStatsd) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputStatsd) GetOnBackpressure() *BackpressureBehaviorStatsd {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputStatsd) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputStatsd) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputStatsd) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputStatsd) GetPqCompress() *CompressionStatsd {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputStatsd) GetPqOnBackpressure() *QueueFullBehaviorStatsd {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputStatsd) GetPqMode() *ModeStatsd {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputStatsd) GetPqControls() *PqControlsStatsd {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type OutputMinioType string

const (
	OutputMinioTypeMinio OutputMinioType = "minio"
)

func (e OutputMinioType) ToPointer() *OutputMinioType {
	return &e
}
func (e *OutputMinioType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "minio":
		*e = OutputMinioType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioType: %v", v)
	}
}

// OutputMinioAuthenticationMethod - AWS authentication method. Choose Auto to use IAM roles.
type OutputMinioAuthenticationMethod string

const (
	OutputMinioAuthenticationMethodAuto   OutputMinioAuthenticationMethod = "auto"
	OutputMinioAuthenticationMethodManual OutputMinioAuthenticationMethod = "manual"
	OutputMinioAuthenticationMethodSecret OutputMinioAuthenticationMethod = "secret"
)

func (e OutputMinioAuthenticationMethod) ToPointer() *OutputMinioAuthenticationMethod {
	return &e
}
func (e *OutputMinioAuthenticationMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = OutputMinioAuthenticationMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioAuthenticationMethod: %v", v)
	}
}

// OutputMinioSignatureVersion - Signature version to use for signing MinIO requests
type OutputMinioSignatureVersion string

const (
	OutputMinioSignatureVersionV2 OutputMinioSignatureVersion = "v2"
	OutputMinioSignatureVersionV4 OutputMinioSignatureVersion = "v4"
)

func (e OutputMinioSignatureVersion) ToPointer() *OutputMinioSignatureVersion {
	return &e
}
func (e *OutputMinioSignatureVersion) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = OutputMinioSignatureVersion(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioSignatureVersion: %v", v)
	}
}

// OutputMinioObjectACL - Object ACL to assign to uploaded objects
type OutputMinioObjectACL string

const (
	OutputMinioObjectACLPrivate                OutputMinioObjectACL = "private"
	OutputMinioObjectACLPublicRead             OutputMinioObjectACL = "public-read"
	OutputMinioObjectACLPublicReadWrite        OutputMinioObjectACL = "public-read-write"
	OutputMinioObjectACLAuthenticatedRead      OutputMinioObjectACL = "authenticated-read"
	OutputMinioObjectACLAwsExecRead            OutputMinioObjectACL = "aws-exec-read"
	OutputMinioObjectACLBucketOwnerRead        OutputMinioObjectACL = "bucket-owner-read"
	OutputMinioObjectACLBucketOwnerFullControl OutputMinioObjectACL = "bucket-owner-full-control"
)

func (e OutputMinioObjectACL) ToPointer() *OutputMinioObjectACL {
	return &e
}
func (e *OutputMinioObjectACL) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "private":
		fallthrough
	case "public-read":
		fallthrough
	case "public-read-write":
		fallthrough
	case "authenticated-read":
		fallthrough
	case "aws-exec-read":
		fallthrough
	case "bucket-owner-read":
		fallthrough
	case "bucket-owner-full-control":
		*e = OutputMinioObjectACL(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioObjectACL: %v", v)
	}
}

// OutputMinioStorageClass - Storage class to select for uploaded objects
type OutputMinioStorageClass string

const (
	OutputMinioStorageClassStandard          OutputMinioStorageClass = "STANDARD"
	OutputMinioStorageClassReducedRedundancy OutputMinioStorageClass = "REDUCED_REDUNDANCY"
)

func (e OutputMinioStorageClass) ToPointer() *OutputMinioStorageClass {
	return &e
}
func (e *OutputMinioStorageClass) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "STANDARD":
		fallthrough
	case "REDUCED_REDUNDANCY":
		*e = OutputMinioStorageClass(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioStorageClass: %v", v)
	}
}

// ServerSideEncryption - Server-side encryption for uploaded objects
type ServerSideEncryption string

const (
	ServerSideEncryptionAes256 ServerSideEncryption = "AES256"
)

func (e ServerSideEncryption) ToPointer() *ServerSideEncryption {
	return &e
}
func (e *ServerSideEncryption) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "AES256":
		*e = ServerSideEncryption(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ServerSideEncryption: %v", v)
	}
}

// OutputMinioDataFormat - Format of the output data
type OutputMinioDataFormat string

const (
	OutputMinioDataFormatJSON    OutputMinioDataFormat = "json"
	OutputMinioDataFormatRaw     OutputMinioDataFormat = "raw"
	OutputMinioDataFormatParquet OutputMinioDataFormat = "parquet"
)

func (e OutputMinioDataFormat) ToPointer() *OutputMinioDataFormat {
	return &e
}
func (e *OutputMinioDataFormat) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "parquet":
		*e = OutputMinioDataFormat(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioDataFormat: %v", v)
	}
}

// OutputMinioBackpressureBehavior - How to handle events when all receivers are exerting backpressure
type OutputMinioBackpressureBehavior string

const (
	OutputMinioBackpressureBehaviorBlock OutputMinioBackpressureBehavior = "block"
	OutputMinioBackpressureBehaviorDrop  OutputMinioBackpressureBehavior = "drop"
)

func (e OutputMinioBackpressureBehavior) ToPointer() *OutputMinioBackpressureBehavior {
	return &e
}
func (e *OutputMinioBackpressureBehavior) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = OutputMinioBackpressureBehavior(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioBackpressureBehavior: %v", v)
	}
}

// OutputMinioDiskSpaceProtection - How to handle events when disk space is below the global 'Min free disk space' limit
type OutputMinioDiskSpaceProtection string

const (
	OutputMinioDiskSpaceProtectionBlock OutputMinioDiskSpaceProtection = "block"
	OutputMinioDiskSpaceProtectionDrop  OutputMinioDiskSpaceProtection = "drop"
)

func (e OutputMinioDiskSpaceProtection) ToPointer() *OutputMinioDiskSpaceProtection {
	return &e
}
func (e *OutputMinioDiskSpaceProtection) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = OutputMinioDiskSpaceProtection(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioDiskSpaceProtection: %v", v)
	}
}

// OutputMinioCompression - Data compression format to apply to HTTP content before it is delivered
type OutputMinioCompression string

const (
	OutputMinioCompressionNone OutputMinioCompression = "none"
	OutputMinioCompressionGzip OutputMinioCompression = "gzip"
)

func (e OutputMinioCompression) ToPointer() *OutputMinioCompression {
	return &e
}
func (e *OutputMinioCompression) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = OutputMinioCompression(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioCompression: %v", v)
	}
}

// OutputMinioCompressionLevel - Compression level to apply before moving files to final destination
type OutputMinioCompressionLevel string

const (
	OutputMinioCompressionLevelBestSpeed       OutputMinioCompressionLevel = "best_speed"
	OutputMinioCompressionLevelNormal          OutputMinioCompressionLevel = "normal"
	OutputMinioCompressionLevelBestCompression OutputMinioCompressionLevel = "best_compression"
)

func (e OutputMinioCompressionLevel) ToPointer() *OutputMinioCompressionLevel {
	return &e
}
func (e *OutputMinioCompressionLevel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "best_speed":
		fallthrough
	case "normal":
		fallthrough
	case "best_compression":
		*e = OutputMinioCompressionLevel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioCompressionLevel: %v", v)
	}
}

// OutputMinioParquetVersion - Determines which data types are supported and how they are represented
type OutputMinioParquetVersion string

const (
	OutputMinioParquetVersionParquet10 OutputMinioParquetVersion = "PARQUET_1_0"
	OutputMinioParquetVersionParquet24 OutputMinioParquetVersion = "PARQUET_2_4"
	OutputMinioParquetVersionParquet26 OutputMinioParquetVersion = "PARQUET_2_6"
)

func (e OutputMinioParquetVersion) ToPointer() *OutputMinioParquetVersion {
	return &e
}
func (e *OutputMinioParquetVersion) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PARQUET_1_0":
		fallthrough
	case "PARQUET_2_4":
		fallthrough
	case "PARQUET_2_6":
		*e = OutputMinioParquetVersion(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioParquetVersion: %v", v)
	}
}

// OutputMinioDataPageVersion - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type OutputMinioDataPageVersion string

const (
	OutputMinioDataPageVersionDataPageV1 OutputMinioDataPageVersion = "DATA_PAGE_V1"
	OutputMinioDataPageVersionDataPageV2 OutputMinioDataPageVersion = "DATA_PAGE_V2"
)

func (e OutputMinioDataPageVersion) ToPointer() *OutputMinioDataPageVersion {
	return &e
}
func (e *OutputMinioDataPageVersion) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "DATA_PAGE_V1":
		fallthrough
	case "DATA_PAGE_V2":
		*e = OutputMinioDataPageVersion(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioDataPageVersion: %v", v)
	}
}

type OutputMinioKeyValueMetadatum struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (o OutputMinioKeyValueMetadatum) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputMinioKeyValueMetadatum) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputMinioKeyValueMetadatum) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *OutputMinioKeyValueMetadatum) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputMinio struct {
	// Unique ID for this output
	ID   string           `json:"id"`
	Type *OutputMinioType `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// MinIO service url (e.g. http://minioHost:9000)
	Endpoint string `json:"endpoint"`
	// Name of the destination MinIO bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket string `json:"bucket"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *OutputMinioAuthenticationMethod `default:"auto" json:"awsAuthenticationMethod"`
	// Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// Region where the MinIO service/cluster is located
	Region *string `json:"region,omitempty"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.
	DestPath *string `json:"destPath,omitempty"`
	// Signature version to use for signing MinIO requests
	SignatureVersion *OutputMinioSignatureVersion `default:"v4" json:"signatureVersion"`
	// Object ACL to assign to uploaded objects
	ObjectACL *OutputMinioObjectACL `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass *OutputMinioStorageClass `json:"storageClass,omitempty"`
	// Server-side encryption for uploaded objects
	ServerSideEncryption *ServerSideEncryption `json:"serverSideEncryption,omitempty"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value  if present  otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *OutputMinioDataFormat `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *OutputMinioBackpressureBehavior `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *OutputMinioDiskSpaceProtection `default:"block" json:"onDiskFullBackpressure"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	Description            *string  `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *OutputMinioCompression `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *OutputMinioCompressionLevel `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *OutputMinioParquetVersion `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *OutputMinioDataPageVersion `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []OutputMinioKeyValueMetadatum `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputMinio) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputMinio) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputMinio) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputMinio) GetType() *OutputMinioType {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputMinio) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputMinio) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputMinio) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputMinio) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputMinio) GetEndpoint() string {
	if o == nil {
		return ""
	}
	return o.Endpoint
}

func (o *OutputMinio) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputMinio) GetAwsAuthenticationMethod() *OutputMinioAuthenticationMethod {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputMinio) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputMinio) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputMinio) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputMinio) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputMinio) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputMinio) GetSignatureVersion() *OutputMinioSignatureVersion {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputMinio) GetObjectACL() *OutputMinioObjectACL {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputMinio) GetStorageClass() *OutputMinioStorageClass {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputMinio) GetServerSideEncryption() *ServerSideEncryption {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputMinio) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputMinio) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputMinio) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputMinio) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputMinio) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputMinio) GetFormat() *OutputMinioDataFormat {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputMinio) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputMinio) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputMinio) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputMinio) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputMinio) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputMinio) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputMinio) GetOnBackpressure() *OutputMinioBackpressureBehavior {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputMinio) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputMinio) GetOnDiskFullBackpressure() *OutputMinioDiskSpaceProtection {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputMinio) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputMinio) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputMinio) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputMinio) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputMinio) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputMinio) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputMinio) GetCompress() *OutputMinioCompression {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputMinio) GetCompressionLevel() *OutputMinioCompressionLevel {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputMinio) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputMinio) GetParquetVersion() *OutputMinioParquetVersion {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputMinio) GetParquetDataPageVersion() *OutputMinioDataPageVersion {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputMinio) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputMinio) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputMinio) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputMinio) GetKeyValueMetadata() []OutputMinioKeyValueMetadatum {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputMinio) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputMinio) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputMinio) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputMinio) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputMinio) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputMinio) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type TypeCloudwatch string

const (
	TypeCloudwatchCloudwatch TypeCloudwatch = "cloudwatch"
)

func (e TypeCloudwatch) ToPointer() *TypeCloudwatch {
	return &e
}
func (e *TypeCloudwatch) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cloudwatch":
		*e = TypeCloudwatch(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeCloudwatch: %v", v)
	}
}

// AuthenticationMethodCloudwatch - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodCloudwatch string

const (
	AuthenticationMethodCloudwatchAuto   AuthenticationMethodCloudwatch = "auto"
	AuthenticationMethodCloudwatchManual AuthenticationMethodCloudwatch = "manual"
	AuthenticationMethodCloudwatchSecret AuthenticationMethodCloudwatch = "secret"
)

func (e AuthenticationMethodCloudwatch) ToPointer() *AuthenticationMethodCloudwatch {
	return &e
}
func (e *AuthenticationMethodCloudwatch) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodCloudwatch(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodCloudwatch: %v", v)
	}
}

// BackpressureBehaviorCloudwatch - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorCloudwatch string

const (
	BackpressureBehaviorCloudwatchBlock BackpressureBehaviorCloudwatch = "block"
	BackpressureBehaviorCloudwatchDrop  BackpressureBehaviorCloudwatch = "drop"
	BackpressureBehaviorCloudwatchQueue BackpressureBehaviorCloudwatch = "queue"
)

func (e BackpressureBehaviorCloudwatch) ToPointer() *BackpressureBehaviorCloudwatch {
	return &e
}
func (e *BackpressureBehaviorCloudwatch) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorCloudwatch(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorCloudwatch: %v", v)
	}
}

// CompressionCloudwatch - Codec to use to compress the persisted data
type CompressionCloudwatch string

const (
	CompressionCloudwatchNone CompressionCloudwatch = "none"
	CompressionCloudwatchGzip CompressionCloudwatch = "gzip"
)

func (e CompressionCloudwatch) ToPointer() *CompressionCloudwatch {
	return &e
}
func (e *CompressionCloudwatch) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionCloudwatch(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionCloudwatch: %v", v)
	}
}

// QueueFullBehaviorCloudwatch - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorCloudwatch string

const (
	QueueFullBehaviorCloudwatchBlock QueueFullBehaviorCloudwatch = "block"
	QueueFullBehaviorCloudwatchDrop  QueueFullBehaviorCloudwatch = "drop"
)

func (e QueueFullBehaviorCloudwatch) ToPointer() *QueueFullBehaviorCloudwatch {
	return &e
}
func (e *QueueFullBehaviorCloudwatch) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorCloudwatch(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorCloudwatch: %v", v)
	}
}

// ModeCloudwatch - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeCloudwatch string

const (
	ModeCloudwatchError        ModeCloudwatch = "error"
	ModeCloudwatchBackpressure ModeCloudwatch = "backpressure"
	ModeCloudwatchAlways       ModeCloudwatch = "always"
)

func (e ModeCloudwatch) ToPointer() *ModeCloudwatch {
	return &e
}
func (e *ModeCloudwatch) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeCloudwatch(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeCloudwatch: %v", v)
	}
}

type PqControlsCloudwatch struct {
}

type OutputCloudwatch struct {
	// Unique ID for this output
	ID   string          `json:"id"`
	Type *TypeCloudwatch `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// CloudWatch log group to associate events with
	LogGroupName string `json:"logGroupName"`
	// Prefix for CloudWatch log stream name. This prefix will be used to generate a unique log stream name per cribl instance, for example: myStream_myHost_myOutputId
	LogStreamName string `json:"logStreamName"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodCloudwatch `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                         `json:"awsSecretKey,omitempty"`
	// Region where the CloudWatchLogs is located
	Region string `json:"region"`
	// CloudWatchLogs service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to CloudWatchLogs-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access CloudWatchLogs
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Maximum number of queued batches before blocking
	MaxQueueSize *float64 `default:"5" json:"maxQueueSize"`
	// Maximum size (KB) of each individual record before compression. For non compressible data 1MB is the max recommended size
	MaxRecordSizeKB *float64 `default:"1024" json:"maxRecordSizeKB"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCloudwatch `default:"block" json:"onBackpressure"`
	Description    *string                         `json:"description,omitempty"`
	AwsAPIKey      *string                         `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionCloudwatch `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorCloudwatch `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeCloudwatch       `default:"error" json:"pqMode"`
	PqControls *PqControlsCloudwatch `json:"pqControls,omitempty"`
}

func (o OutputCloudwatch) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCloudwatch) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputCloudwatch) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputCloudwatch) GetType() *TypeCloudwatch {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputCloudwatch) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCloudwatch) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCloudwatch) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCloudwatch) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCloudwatch) GetLogGroupName() string {
	if o == nil {
		return ""
	}
	return o.LogGroupName
}

func (o *OutputCloudwatch) GetLogStreamName() string {
	if o == nil {
		return ""
	}
	return o.LogStreamName
}

func (o *OutputCloudwatch) GetAwsAuthenticationMethod() *AuthenticationMethodCloudwatch {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputCloudwatch) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputCloudwatch) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputCloudwatch) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputCloudwatch) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputCloudwatch) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCloudwatch) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputCloudwatch) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputCloudwatch) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputCloudwatch) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputCloudwatch) GetMaxQueueSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxQueueSize
}

func (o *OutputCloudwatch) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputCloudwatch) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputCloudwatch) GetOnBackpressure() *BackpressureBehaviorCloudwatch {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCloudwatch) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCloudwatch) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputCloudwatch) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputCloudwatch) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputCloudwatch) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputCloudwatch) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputCloudwatch) GetPqCompress() *CompressionCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputCloudwatch) GetPqOnBackpressure() *QueueFullBehaviorCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputCloudwatch) GetPqMode() *ModeCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputCloudwatch) GetPqControls() *PqControlsCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeInfluxdb string

const (
	TypeInfluxdbInfluxdb TypeInfluxdb = "influxdb"
)

func (e TypeInfluxdb) ToPointer() *TypeInfluxdb {
	return &e
}
func (e *TypeInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "influxdb":
		*e = TypeInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeInfluxdb: %v", v)
	}
}

// TimestampPrecision - Sets the precision for the supplied Unix time values. Defaults to milliseconds.
type TimestampPrecision string

const (
	TimestampPrecisionNs TimestampPrecision = "ns"
	TimestampPrecisionU  TimestampPrecision = "u"
	TimestampPrecisionMs TimestampPrecision = "ms"
	TimestampPrecisionS  TimestampPrecision = "s"
	TimestampPrecisionM  TimestampPrecision = "m"
	TimestampPrecisionH  TimestampPrecision = "h"
)

func (e TimestampPrecision) ToPointer() *TimestampPrecision {
	return &e
}
func (e *TimestampPrecision) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "ns":
		fallthrough
	case "u":
		fallthrough
	case "ms":
		fallthrough
	case "s":
		fallthrough
	case "m":
		fallthrough
	case "h":
		*e = TimestampPrecision(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TimestampPrecision: %v", v)
	}
}

type ExtraHTTPHeaderInfluxdb struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderInfluxdb) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderInfluxdb) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeInfluxdb - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeInfluxdb string

const (
	FailedRequestLoggingModeInfluxdbPayload           FailedRequestLoggingModeInfluxdb = "payload"
	FailedRequestLoggingModeInfluxdbPayloadAndHeaders FailedRequestLoggingModeInfluxdb = "payloadAndHeaders"
	FailedRequestLoggingModeInfluxdbNone              FailedRequestLoggingModeInfluxdb = "none"
)

func (e FailedRequestLoggingModeInfluxdb) ToPointer() *FailedRequestLoggingModeInfluxdb {
	return &e
}
func (e *FailedRequestLoggingModeInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeInfluxdb: %v", v)
	}
}

type ResponseRetrySettingInfluxdb struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingInfluxdb) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingInfluxdb) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingInfluxdb) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingInfluxdb) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsInfluxdb struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsInfluxdb) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsInfluxdb) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsInfluxdb) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsInfluxdb) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorInfluxdb - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorInfluxdb string

const (
	BackpressureBehaviorInfluxdbBlock BackpressureBehaviorInfluxdb = "block"
	BackpressureBehaviorInfluxdbDrop  BackpressureBehaviorInfluxdb = "drop"
	BackpressureBehaviorInfluxdbQueue BackpressureBehaviorInfluxdb = "queue"
)

func (e BackpressureBehaviorInfluxdb) ToPointer() *BackpressureBehaviorInfluxdb {
	return &e
}
func (e *BackpressureBehaviorInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorInfluxdb: %v", v)
	}
}

// AuthenticationTypeInfluxdb - InfluxDB authentication type
type AuthenticationTypeInfluxdb string

const (
	AuthenticationTypeInfluxdbNone              AuthenticationTypeInfluxdb = "none"
	AuthenticationTypeInfluxdbBasic             AuthenticationTypeInfluxdb = "basic"
	AuthenticationTypeInfluxdbCredentialsSecret AuthenticationTypeInfluxdb = "credentialsSecret"
	AuthenticationTypeInfluxdbToken             AuthenticationTypeInfluxdb = "token"
	AuthenticationTypeInfluxdbTextSecret        AuthenticationTypeInfluxdb = "textSecret"
	AuthenticationTypeInfluxdbOauth             AuthenticationTypeInfluxdb = "oauth"
)

func (e AuthenticationTypeInfluxdb) ToPointer() *AuthenticationTypeInfluxdb {
	return &e
}
func (e *AuthenticationTypeInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "basic":
		fallthrough
	case "credentialsSecret":
		fallthrough
	case "token":
		fallthrough
	case "textSecret":
		fallthrough
	case "oauth":
		*e = AuthenticationTypeInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationTypeInfluxdb: %v", v)
	}
}

// CompressionInfluxdb - Codec to use to compress the persisted data
type CompressionInfluxdb string

const (
	CompressionInfluxdbNone CompressionInfluxdb = "none"
	CompressionInfluxdbGzip CompressionInfluxdb = "gzip"
)

func (e CompressionInfluxdb) ToPointer() *CompressionInfluxdb {
	return &e
}
func (e *CompressionInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionInfluxdb: %v", v)
	}
}

// QueueFullBehaviorInfluxdb - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorInfluxdb string

const (
	QueueFullBehaviorInfluxdbBlock QueueFullBehaviorInfluxdb = "block"
	QueueFullBehaviorInfluxdbDrop  QueueFullBehaviorInfluxdb = "drop"
)

func (e QueueFullBehaviorInfluxdb) ToPointer() *QueueFullBehaviorInfluxdb {
	return &e
}
func (e *QueueFullBehaviorInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorInfluxdb: %v", v)
	}
}

// ModeInfluxdb - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeInfluxdb string

const (
	ModeInfluxdbError        ModeInfluxdb = "error"
	ModeInfluxdbBackpressure ModeInfluxdb = "backpressure"
	ModeInfluxdbAlways       ModeInfluxdb = "always"
)

func (e ModeInfluxdb) ToPointer() *ModeInfluxdb {
	return &e
}
func (e *ModeInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeInfluxdb: %v", v)
	}
}

type PqControlsInfluxdb struct {
}

type OauthParamInfluxdb struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o *OauthParamInfluxdb) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthParamInfluxdb) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OauthHeaderInfluxdb struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o *OauthHeaderInfluxdb) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthHeaderInfluxdb) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputInfluxdb struct {
	// Unique ID for this output
	ID   string       `json:"id"`
	Type TypeInfluxdb `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL of an InfluxDB cluster to send events to, e.g., http://localhost:8086/write
	URL string `json:"url"`
	// The v2 API can be enabled with InfluxDB versions 1.8 and later.
	UseV2API *bool `default:"false" json:"useV2API"`
	// Sets the precision for the supplied Unix time values. Defaults to milliseconds.
	TimestampPrecision *TimestampPrecision `default:"ms" json:"timestampPrecision"`
	// Enabling this will pull the value field from the metric name. E,g, 'db.query.user' will use 'db.query' as the measurement and 'user' as the value field.
	DynamicValueFieldName *bool `default:"true" json:"dynamicValueFieldName"`
	// Name of the field in which to store the metric when sending to InfluxDB. If dynamic generation is enabled and fails, this will be used as a fallback.
	ValueFieldName *string `default:"value" json:"valueFieldName"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderInfluxdb `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeInfluxdb `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingInfluxdb `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsInfluxdb  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorInfluxdb `default:"block" json:"onBackpressure"`
	// InfluxDB authentication type
	AuthType    *AuthenticationTypeInfluxdb `default:"none" json:"authType"`
	Description *string                     `json:"description,omitempty"`
	// Database to write to.
	Database *string `json:"database,omitempty"`
	// Bucket to write to.
	Bucket *string `json:"bucket,omitempty"`
	// Organization ID for this bucket.
	Org *string `json:"org,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionInfluxdb `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorInfluxdb `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeInfluxdb       `default:"error" json:"pqMode"`
	PqControls *PqControlsInfluxdb `json:"pqControls,omitempty"`
	Username   *string             `json:"username,omitempty"`
	Password   *string             `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OauthParamInfluxdb `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []OauthHeaderInfluxdb `json:"oauthHeaders,omitempty"`
}

func (o OutputInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputInfluxdb) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputInfluxdb) GetType() TypeInfluxdb {
	if o == nil {
		return TypeInfluxdb("")
	}
	return o.Type
}

func (o *OutputInfluxdb) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputInfluxdb) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputInfluxdb) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputInfluxdb) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputInfluxdb) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputInfluxdb) GetUseV2API() *bool {
	if o == nil {
		return nil
	}
	return o.UseV2API
}

func (o *OutputInfluxdb) GetTimestampPrecision() *TimestampPrecision {
	if o == nil {
		return nil
	}
	return o.TimestampPrecision
}

func (o *OutputInfluxdb) GetDynamicValueFieldName() *bool {
	if o == nil {
		return nil
	}
	return o.DynamicValueFieldName
}

func (o *OutputInfluxdb) GetValueFieldName() *string {
	if o == nil {
		return nil
	}
	return o.ValueFieldName
}

func (o *OutputInfluxdb) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputInfluxdb) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputInfluxdb) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputInfluxdb) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputInfluxdb) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputInfluxdb) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputInfluxdb) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputInfluxdb) GetExtraHTTPHeaders() []ExtraHTTPHeaderInfluxdb {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputInfluxdb) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputInfluxdb) GetFailedRequestLoggingMode() *FailedRequestLoggingModeInfluxdb {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputInfluxdb) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputInfluxdb) GetResponseRetrySettings() []ResponseRetrySettingInfluxdb {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputInfluxdb) GetTimeoutRetrySettings() *TimeoutRetrySettingsInfluxdb {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputInfluxdb) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputInfluxdb) GetOnBackpressure() *BackpressureBehaviorInfluxdb {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputInfluxdb) GetAuthType() *AuthenticationTypeInfluxdb {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputInfluxdb) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputInfluxdb) GetDatabase() *string {
	if o == nil {
		return nil
	}
	return o.Database
}

func (o *OutputInfluxdb) GetBucket() *string {
	if o == nil {
		return nil
	}
	return o.Bucket
}

func (o *OutputInfluxdb) GetOrg() *string {
	if o == nil {
		return nil
	}
	return o.Org
}

func (o *OutputInfluxdb) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputInfluxdb) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputInfluxdb) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputInfluxdb) GetPqCompress() *CompressionInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputInfluxdb) GetPqOnBackpressure() *QueueFullBehaviorInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputInfluxdb) GetPqMode() *ModeInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputInfluxdb) GetPqControls() *PqControlsInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputInfluxdb) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputInfluxdb) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputInfluxdb) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputInfluxdb) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputInfluxdb) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputInfluxdb) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputInfluxdb) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputInfluxdb) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputInfluxdb) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputInfluxdb) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputInfluxdb) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputInfluxdb) GetOauthParams() []OauthParamInfluxdb {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputInfluxdb) GetOauthHeaders() []OauthHeaderInfluxdb {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

type TypeNewrelicEvents string

const (
	TypeNewrelicEventsNewrelicEvents TypeNewrelicEvents = "newrelic_events"
)

func (e TypeNewrelicEvents) ToPointer() *TypeNewrelicEvents {
	return &e
}
func (e *TypeNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "newrelic_events":
		*e = TypeNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeNewrelicEvents: %v", v)
	}
}

// RegionNewrelicEvents - Which New Relic region endpoint to use.
type RegionNewrelicEvents string

const (
	RegionNewrelicEventsUs     RegionNewrelicEvents = "US"
	RegionNewrelicEventsEu     RegionNewrelicEvents = "EU"
	RegionNewrelicEventsCustom RegionNewrelicEvents = "Custom"
)

func (e RegionNewrelicEvents) ToPointer() *RegionNewrelicEvents {
	return &e
}
func (e *RegionNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "US":
		fallthrough
	case "EU":
		fallthrough
	case "Custom":
		*e = RegionNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RegionNewrelicEvents: %v", v)
	}
}

type ExtraHTTPHeaderNewrelicEvents struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderNewrelicEvents) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderNewrelicEvents) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeNewrelicEvents - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeNewrelicEvents string

const (
	FailedRequestLoggingModeNewrelicEventsPayload           FailedRequestLoggingModeNewrelicEvents = "payload"
	FailedRequestLoggingModeNewrelicEventsPayloadAndHeaders FailedRequestLoggingModeNewrelicEvents = "payloadAndHeaders"
	FailedRequestLoggingModeNewrelicEventsNone              FailedRequestLoggingModeNewrelicEvents = "none"
)

func (e FailedRequestLoggingModeNewrelicEvents) ToPointer() *FailedRequestLoggingModeNewrelicEvents {
	return &e
}
func (e *FailedRequestLoggingModeNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeNewrelicEvents: %v", v)
	}
}

type ResponseRetrySettingNewrelicEvents struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingNewrelicEvents) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingNewrelicEvents) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingNewrelicEvents) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingNewrelicEvents) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsNewrelicEvents struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsNewrelicEvents) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsNewrelicEvents) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsNewrelicEvents) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsNewrelicEvents) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorNewrelicEvents - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorNewrelicEvents string

const (
	BackpressureBehaviorNewrelicEventsBlock BackpressureBehaviorNewrelicEvents = "block"
	BackpressureBehaviorNewrelicEventsDrop  BackpressureBehaviorNewrelicEvents = "drop"
	BackpressureBehaviorNewrelicEventsQueue BackpressureBehaviorNewrelicEvents = "queue"
)

func (e BackpressureBehaviorNewrelicEvents) ToPointer() *BackpressureBehaviorNewrelicEvents {
	return &e
}
func (e *BackpressureBehaviorNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorNewrelicEvents: %v", v)
	}
}

// AuthenticationMethodNewrelicEvents - Enter API key directly, or select a stored secret
type AuthenticationMethodNewrelicEvents string

const (
	AuthenticationMethodNewrelicEventsManual AuthenticationMethodNewrelicEvents = "manual"
	AuthenticationMethodNewrelicEventsSecret AuthenticationMethodNewrelicEvents = "secret"
)

func (e AuthenticationMethodNewrelicEvents) ToPointer() *AuthenticationMethodNewrelicEvents {
	return &e
}
func (e *AuthenticationMethodNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodNewrelicEvents: %v", v)
	}
}

// CompressionNewrelicEvents - Codec to use to compress the persisted data
type CompressionNewrelicEvents string

const (
	CompressionNewrelicEventsNone CompressionNewrelicEvents = "none"
	CompressionNewrelicEventsGzip CompressionNewrelicEvents = "gzip"
)

func (e CompressionNewrelicEvents) ToPointer() *CompressionNewrelicEvents {
	return &e
}
func (e *CompressionNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionNewrelicEvents: %v", v)
	}
}

// QueueFullBehaviorNewrelicEvents - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorNewrelicEvents string

const (
	QueueFullBehaviorNewrelicEventsBlock QueueFullBehaviorNewrelicEvents = "block"
	QueueFullBehaviorNewrelicEventsDrop  QueueFullBehaviorNewrelicEvents = "drop"
)

func (e QueueFullBehaviorNewrelicEvents) ToPointer() *QueueFullBehaviorNewrelicEvents {
	return &e
}
func (e *QueueFullBehaviorNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorNewrelicEvents: %v", v)
	}
}

// ModeNewrelicEvents - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeNewrelicEvents string

const (
	ModeNewrelicEventsError        ModeNewrelicEvents = "error"
	ModeNewrelicEventsBackpressure ModeNewrelicEvents = "backpressure"
	ModeNewrelicEventsAlways       ModeNewrelicEvents = "always"
)

func (e ModeNewrelicEvents) ToPointer() *ModeNewrelicEvents {
	return &e
}
func (e *ModeNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeNewrelicEvents: %v", v)
	}
}

type PqControlsNewrelicEvents struct {
}

type OutputNewrelicEvents struct {
	// Unique ID for this output
	ID   string              `json:"id"`
	Type *TypeNewrelicEvents `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Which New Relic region endpoint to use.
	Region *RegionNewrelicEvents `default:"US" json:"region"`
	// New Relic account ID
	AccountID string `json:"accountId"`
	// Default eventType to use when not present in an event. For more information, see [here](https://docs.newrelic.com/docs/telemetry-data-platform/custom-data/custom-events/data-requirements-limits-custom-event-data/#reserved-words).
	EventType string `json:"eventType"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderNewrelicEvents `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeNewrelicEvents `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingNewrelicEvents `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsNewrelicEvents  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorNewrelicEvents `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType    *AuthenticationMethodNewrelicEvents `default:"manual" json:"authType"`
	Description *string                             `json:"description,omitempty"`
	CustomURL   *string                             `json:"customUrl,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionNewrelicEvents `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorNewrelicEvents `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeNewrelicEvents       `default:"error" json:"pqMode"`
	PqControls *PqControlsNewrelicEvents `json:"pqControls,omitempty"`
	// New Relic API key. Can be overridden using __newRelic_apiKey field.
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (o OutputNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputNewrelicEvents) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputNewrelicEvents) GetType() *TypeNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputNewrelicEvents) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputNewrelicEvents) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputNewrelicEvents) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputNewrelicEvents) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputNewrelicEvents) GetRegion() *RegionNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputNewrelicEvents) GetAccountID() string {
	if o == nil {
		return ""
	}
	return o.AccountID
}

func (o *OutputNewrelicEvents) GetEventType() string {
	if o == nil {
		return ""
	}
	return o.EventType
}

func (o *OutputNewrelicEvents) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputNewrelicEvents) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputNewrelicEvents) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputNewrelicEvents) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputNewrelicEvents) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputNewrelicEvents) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputNewrelicEvents) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputNewrelicEvents) GetExtraHTTPHeaders() []ExtraHTTPHeaderNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputNewrelicEvents) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputNewrelicEvents) GetFailedRequestLoggingMode() *FailedRequestLoggingModeNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputNewrelicEvents) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputNewrelicEvents) GetResponseRetrySettings() []ResponseRetrySettingNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputNewrelicEvents) GetTimeoutRetrySettings() *TimeoutRetrySettingsNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputNewrelicEvents) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputNewrelicEvents) GetOnBackpressure() *BackpressureBehaviorNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputNewrelicEvents) GetAuthType() *AuthenticationMethodNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputNewrelicEvents) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputNewrelicEvents) GetCustomURL() *string {
	if o == nil {
		return nil
	}
	return o.CustomURL
}

func (o *OutputNewrelicEvents) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputNewrelicEvents) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputNewrelicEvents) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputNewrelicEvents) GetPqCompress() *CompressionNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputNewrelicEvents) GetPqOnBackpressure() *QueueFullBehaviorNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputNewrelicEvents) GetPqMode() *ModeNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputNewrelicEvents) GetPqControls() *PqControlsNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputNewrelicEvents) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputNewrelicEvents) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

type TypeElasticCloud string

const (
	TypeElasticCloudElasticCloud TypeElasticCloud = "elastic_cloud"
)

func (e TypeElasticCloud) ToPointer() *TypeElasticCloud {
	return &e
}
func (e *TypeElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "elastic_cloud":
		*e = TypeElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeElasticCloud: %v", v)
	}
}

type ExtraHTTPHeaderElasticCloud struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderElasticCloud) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderElasticCloud) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeElasticCloud - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeElasticCloud string

const (
	FailedRequestLoggingModeElasticCloudPayload           FailedRequestLoggingModeElasticCloud = "payload"
	FailedRequestLoggingModeElasticCloudPayloadAndHeaders FailedRequestLoggingModeElasticCloud = "payloadAndHeaders"
	FailedRequestLoggingModeElasticCloudNone              FailedRequestLoggingModeElasticCloud = "none"
)

func (e FailedRequestLoggingModeElasticCloud) ToPointer() *FailedRequestLoggingModeElasticCloud {
	return &e
}
func (e *FailedRequestLoggingModeElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeElasticCloud: %v", v)
	}
}

type ExtraParamElasticCloud struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}

func (o *ExtraParamElasticCloud) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *ExtraParamElasticCloud) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// AuthenticationMethodElasticCloud - Enter credentials directly, or select a stored secret
type AuthenticationMethodElasticCloud string

const (
	AuthenticationMethodElasticCloudManual       AuthenticationMethodElasticCloud = "manual"
	AuthenticationMethodElasticCloudSecret       AuthenticationMethodElasticCloud = "secret"
	AuthenticationMethodElasticCloudManualAPIKey AuthenticationMethodElasticCloud = "manualAPIKey"
	AuthenticationMethodElasticCloudTextSecret   AuthenticationMethodElasticCloud = "textSecret"
)

func (e AuthenticationMethodElasticCloud) ToPointer() *AuthenticationMethodElasticCloud {
	return &e
}
func (e *AuthenticationMethodElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		fallthrough
	case "manualAPIKey":
		fallthrough
	case "textSecret":
		*e = AuthenticationMethodElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodElasticCloud: %v", v)
	}
}

type AuthElasticCloud struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Enter credentials directly, or select a stored secret
	AuthType *AuthenticationMethodElasticCloud `default:"manual" json:"authType"`
}

func (a AuthElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *AuthElasticCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *AuthElasticCloud) GetAuthType() *AuthenticationMethodElasticCloud {
	if o == nil {
		return nil
	}
	return o.AuthType
}

type ResponseRetrySettingElasticCloud struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingElasticCloud) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingElasticCloud) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingElasticCloud) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingElasticCloud) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsElasticCloud struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsElasticCloud) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsElasticCloud) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsElasticCloud) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsElasticCloud) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorElasticCloud - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorElasticCloud string

const (
	BackpressureBehaviorElasticCloudBlock BackpressureBehaviorElasticCloud = "block"
	BackpressureBehaviorElasticCloudDrop  BackpressureBehaviorElasticCloud = "drop"
	BackpressureBehaviorElasticCloudQueue BackpressureBehaviorElasticCloud = "queue"
)

func (e BackpressureBehaviorElasticCloud) ToPointer() *BackpressureBehaviorElasticCloud {
	return &e
}
func (e *BackpressureBehaviorElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorElasticCloud: %v", v)
	}
}

// CompressionElasticCloud - Codec to use to compress the persisted data
type CompressionElasticCloud string

const (
	CompressionElasticCloudNone CompressionElasticCloud = "none"
	CompressionElasticCloudGzip CompressionElasticCloud = "gzip"
)

func (e CompressionElasticCloud) ToPointer() *CompressionElasticCloud {
	return &e
}
func (e *CompressionElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionElasticCloud: %v", v)
	}
}

// QueueFullBehaviorElasticCloud - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorElasticCloud string

const (
	QueueFullBehaviorElasticCloudBlock QueueFullBehaviorElasticCloud = "block"
	QueueFullBehaviorElasticCloudDrop  QueueFullBehaviorElasticCloud = "drop"
)

func (e QueueFullBehaviorElasticCloud) ToPointer() *QueueFullBehaviorElasticCloud {
	return &e
}
func (e *QueueFullBehaviorElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorElasticCloud: %v", v)
	}
}

// ModeElasticCloud - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeElasticCloud string

const (
	ModeElasticCloudError        ModeElasticCloud = "error"
	ModeElasticCloudBackpressure ModeElasticCloud = "backpressure"
	ModeElasticCloudAlways       ModeElasticCloud = "always"
)

func (e ModeElasticCloud) ToPointer() *ModeElasticCloud {
	return &e
}
func (e *ModeElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeElasticCloud: %v", v)
	}
}

type PqControlsElasticCloud struct {
}

type OutputElasticCloud struct {
	// Unique ID for this output
	ID   string            `json:"id"`
	Type *TypeElasticCloud `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enter Cloud ID of the Elastic Cloud environment to send events to
	URL string `json:"url"`
	// Data stream or index to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.
	Index string `json:"index"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderElasticCloud `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeElasticCloud `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Extra parameters to use in HTTP requests
	ExtraParams []ExtraParamElasticCloud `json:"extraParams,omitempty"`
	Auth        *AuthElasticCloud        `json:"auth,omitempty"`
	// Optional Elastic Cloud Destination pipeline
	ElasticPipeline *string `json:"elasticPipeline,omitempty"`
	// Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)
	IncludeDocID *bool `default:"true" json:"includeDocId"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingElasticCloud `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsElasticCloud  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorElasticCloud `default:"block" json:"onBackpressure"`
	Description    *string                           `json:"description,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionElasticCloud `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorElasticCloud `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeElasticCloud       `default:"error" json:"pqMode"`
	PqControls *PqControlsElasticCloud `json:"pqControls,omitempty"`
}

func (o OutputElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputElasticCloud) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputElasticCloud) GetType() *TypeElasticCloud {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputElasticCloud) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputElasticCloud) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputElasticCloud) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputElasticCloud) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputElasticCloud) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputElasticCloud) GetIndex() string {
	if o == nil {
		return ""
	}
	return o.Index
}

func (o *OutputElasticCloud) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputElasticCloud) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputElasticCloud) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputElasticCloud) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputElasticCloud) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputElasticCloud) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputElasticCloud) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputElasticCloud) GetExtraHTTPHeaders() []ExtraHTTPHeaderElasticCloud {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputElasticCloud) GetFailedRequestLoggingMode() *FailedRequestLoggingModeElasticCloud {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputElasticCloud) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputElasticCloud) GetExtraParams() []ExtraParamElasticCloud {
	if o == nil {
		return nil
	}
	return o.ExtraParams
}

func (o *OutputElasticCloud) GetAuth() *AuthElasticCloud {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *OutputElasticCloud) GetElasticPipeline() *string {
	if o == nil {
		return nil
	}
	return o.ElasticPipeline
}

func (o *OutputElasticCloud) GetIncludeDocID() *bool {
	if o == nil {
		return nil
	}
	return o.IncludeDocID
}

func (o *OutputElasticCloud) GetResponseRetrySettings() []ResponseRetrySettingElasticCloud {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputElasticCloud) GetTimeoutRetrySettings() *TimeoutRetrySettingsElasticCloud {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputElasticCloud) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputElasticCloud) GetOnBackpressure() *BackpressureBehaviorElasticCloud {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputElasticCloud) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputElasticCloud) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputElasticCloud) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputElasticCloud) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputElasticCloud) GetPqCompress() *CompressionElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputElasticCloud) GetPqOnBackpressure() *QueueFullBehaviorElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputElasticCloud) GetPqMode() *ModeElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputElasticCloud) GetPqControls() *PqControlsElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeElastic string

const (
	CreateOutputTypeElasticElastic CreateOutputTypeElastic = "elastic"
)

func (e CreateOutputTypeElastic) ToPointer() *CreateOutputTypeElastic {
	return &e
}
func (e *CreateOutputTypeElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "elastic":
		*e = CreateOutputTypeElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeElastic: %v", v)
	}
}

type CreateOutputExtraHTTPHeaderElastic struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *CreateOutputExtraHTTPHeaderElastic) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *CreateOutputExtraHTTPHeaderElastic) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeElastic - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeElastic string

const (
	FailedRequestLoggingModeElasticPayload           FailedRequestLoggingModeElastic = "payload"
	FailedRequestLoggingModeElasticPayloadAndHeaders FailedRequestLoggingModeElastic = "payloadAndHeaders"
	FailedRequestLoggingModeElasticNone              FailedRequestLoggingModeElastic = "none"
)

func (e FailedRequestLoggingModeElastic) ToPointer() *FailedRequestLoggingModeElastic {
	return &e
}
func (e *FailedRequestLoggingModeElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeElastic: %v", v)
	}
}

type ResponseRetrySettingElastic struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingElastic) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingElastic) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingElastic) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingElastic) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsElastic struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsElastic) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsElastic) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsElastic) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsElastic) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type ExtraParamElastic struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}

func (o *ExtraParamElastic) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *ExtraParamElastic) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// AuthAuthenticationMethodElastic - Enter credentials directly, or select a stored secret
type AuthAuthenticationMethodElastic string

const (
	AuthAuthenticationMethodElasticManual       AuthAuthenticationMethodElastic = "manual"
	AuthAuthenticationMethodElasticSecret       AuthAuthenticationMethodElastic = "secret"
	AuthAuthenticationMethodElasticManualAPIKey AuthAuthenticationMethodElastic = "manualAPIKey"
	AuthAuthenticationMethodElasticTextSecret   AuthAuthenticationMethodElastic = "textSecret"
)

func (e AuthAuthenticationMethodElastic) ToPointer() *AuthAuthenticationMethodElastic {
	return &e
}
func (e *AuthAuthenticationMethodElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		fallthrough
	case "manualAPIKey":
		fallthrough
	case "textSecret":
		*e = AuthAuthenticationMethodElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthAuthenticationMethodElastic: %v", v)
	}
}

type AuthElastic struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Enter credentials directly, or select a stored secret
	AuthType *AuthAuthenticationMethodElastic `default:"manual" json:"authType"`
}

func (a AuthElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *AuthElastic) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *AuthElastic) GetAuthType() *AuthAuthenticationMethodElastic {
	if o == nil {
		return nil
	}
	return o.AuthType
}

// ElasticVersion - Optional Elasticsearch version, used to format events. If not specified, will auto-discover version.
type ElasticVersion string

const (
	ElasticVersionAuto  ElasticVersion = "auto"
	ElasticVersionSix   ElasticVersion = "6"
	ElasticVersionSeven ElasticVersion = "7"
)

func (e ElasticVersion) ToPointer() *ElasticVersion {
	return &e
}
func (e *ElasticVersion) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "6":
		fallthrough
	case "7":
		*e = ElasticVersion(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ElasticVersion: %v", v)
	}
}

// WriteAction - Action to use when writing events. Must be set to `Create` when writing to a data stream.
type WriteAction string

const (
	WriteActionIndex  WriteAction = "index"
	WriteActionCreate WriteAction = "create"
)

func (e WriteAction) ToPointer() *WriteAction {
	return &e
}
func (e *WriteAction) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "index":
		fallthrough
	case "create":
		*e = WriteAction(v)
		return nil
	default:
		return fmt.Errorf("invalid value for WriteAction: %v", v)
	}
}

// BackpressureBehaviorElastic - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorElastic string

const (
	BackpressureBehaviorElasticBlock BackpressureBehaviorElastic = "block"
	BackpressureBehaviorElasticDrop  BackpressureBehaviorElastic = "drop"
	BackpressureBehaviorElasticQueue BackpressureBehaviorElastic = "queue"
)

func (e BackpressureBehaviorElastic) ToPointer() *BackpressureBehaviorElastic {
	return &e
}
func (e *BackpressureBehaviorElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorElastic: %v", v)
	}
}

type URL struct {
	// The URL to an Elastic node to send events to. Example: http://elastic:9200/_bulk
	URL string `json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URL) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URL) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *URL) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *URL) GetWeight() *float64 {
	if o == nil {
		return nil
	}
	return o.Weight
}

// PqCompressCompressionElastic - Codec to use to compress the persisted data
type PqCompressCompressionElastic string

const (
	PqCompressCompressionElasticNone PqCompressCompressionElastic = "none"
	PqCompressCompressionElasticGzip PqCompressCompressionElastic = "gzip"
)

func (e PqCompressCompressionElastic) ToPointer() *PqCompressCompressionElastic {
	return &e
}
func (e *PqCompressCompressionElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionElastic: %v", v)
	}
}

// QueueFullBehaviorElastic - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorElastic string

const (
	QueueFullBehaviorElasticBlock QueueFullBehaviorElastic = "block"
	QueueFullBehaviorElasticDrop  QueueFullBehaviorElastic = "drop"
)

func (e QueueFullBehaviorElastic) ToPointer() *QueueFullBehaviorElastic {
	return &e
}
func (e *QueueFullBehaviorElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorElastic: %v", v)
	}
}

// CreateOutputModeElastic - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeElastic string

const (
	CreateOutputModeElasticError        CreateOutputModeElastic = "error"
	CreateOutputModeElasticBackpressure CreateOutputModeElastic = "backpressure"
	CreateOutputModeElasticAlways       CreateOutputModeElastic = "always"
)

func (e CreateOutputModeElastic) ToPointer() *CreateOutputModeElastic {
	return &e
}
func (e *CreateOutputModeElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = CreateOutputModeElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputModeElastic: %v", v)
	}
}

type PqControlsElastic struct {
}

type OutputElastic struct {
	// Unique ID for this output
	ID   string                  `json:"id"`
	Type CreateOutputTypeElastic `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// Index or data stream to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.
	Index string `json:"index"`
	// Document type to use for events. Can be overwritten by an event's __type field.
	DocType *string `json:"docType,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []CreateOutputExtraHTTPHeaderElastic `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeElastic `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingElastic `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsElastic  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool               `default:"false" json:"responseHonorRetryAfterHeader"`
	ExtraParams                   []ExtraParamElastic `json:"extraParams,omitempty"`
	Auth                          *AuthElastic        `json:"auth,omitempty"`
	// Optional Elasticsearch version, used to format events. If not specified, will auto-discover version.
	ElasticVersion *ElasticVersion `default:"auto" json:"elasticVersion"`
	// Optional Elasticsearch destination pipeline
	ElasticPipeline *string `json:"elasticPipeline,omitempty"`
	// Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)
	IncludeDocID *bool `default:"false" json:"includeDocId"`
	// Action to use when writing events. Must be set to `Create` when writing to a data stream.
	WriteAction *WriteAction `default:"create" json:"writeAction"`
	// Retry failed events when a bulk request to Elastic is successful, but the response body returns an error for one or more events in the batch
	RetryPartialErrors *bool `default:"false" json:"retryPartialErrors"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorElastic `default:"block" json:"onBackpressure"`
	Description    *string                      `json:"description,omitempty"`
	// The Cloud ID or URL to an Elastic cluster to send events to. Example: http://elastic:9200/_bulk
	URL *string `json:"url,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool `default:"false" json:"excludeSelf"`
	Urls        []URL `json:"urls,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionElastic `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorElastic `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *CreateOutputModeElastic `default:"error" json:"pqMode"`
	PqControls *PqControlsElastic       `json:"pqControls,omitempty"`
}

func (o OutputElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputElastic) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputElastic) GetType() CreateOutputTypeElastic {
	if o == nil {
		return CreateOutputTypeElastic("")
	}
	return o.Type
}

func (o *OutputElastic) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputElastic) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputElastic) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputElastic) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputElastic) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputElastic) GetIndex() string {
	if o == nil {
		return ""
	}
	return o.Index
}

func (o *OutputElastic) GetDocType() *string {
	if o == nil {
		return nil
	}
	return o.DocType
}

func (o *OutputElastic) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputElastic) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputElastic) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputElastic) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputElastic) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputElastic) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputElastic) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputElastic) GetExtraHTTPHeaders() []CreateOutputExtraHTTPHeaderElastic {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputElastic) GetFailedRequestLoggingMode() *FailedRequestLoggingModeElastic {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputElastic) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputElastic) GetResponseRetrySettings() []ResponseRetrySettingElastic {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputElastic) GetTimeoutRetrySettings() *TimeoutRetrySettingsElastic {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputElastic) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputElastic) GetExtraParams() []ExtraParamElastic {
	if o == nil {
		return nil
	}
	return o.ExtraParams
}

func (o *OutputElastic) GetAuth() *AuthElastic {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *OutputElastic) GetElasticVersion() *ElasticVersion {
	if o == nil {
		return nil
	}
	return o.ElasticVersion
}

func (o *OutputElastic) GetElasticPipeline() *string {
	if o == nil {
		return nil
	}
	return o.ElasticPipeline
}

func (o *OutputElastic) GetIncludeDocID() *bool {
	if o == nil {
		return nil
	}
	return o.IncludeDocID
}

func (o *OutputElastic) GetWriteAction() *WriteAction {
	if o == nil {
		return nil
	}
	return o.WriteAction
}

func (o *OutputElastic) GetRetryPartialErrors() *bool {
	if o == nil {
		return nil
	}
	return o.RetryPartialErrors
}

func (o *OutputElastic) GetOnBackpressure() *BackpressureBehaviorElastic {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputElastic) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputElastic) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputElastic) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputElastic) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputElastic) GetUrls() []URL {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputElastic) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputElastic) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputElastic) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputElastic) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputElastic) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputElastic) GetPqCompress() *PqCompressCompressionElastic {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputElastic) GetPqOnBackpressure() *QueueFullBehaviorElastic {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputElastic) GetPqMode() *CreateOutputModeElastic {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputElastic) GetPqControls() *PqControlsElastic {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeMsk string

const (
	CreateOutputTypeMskMsk CreateOutputTypeMsk = "msk"
)

func (e CreateOutputTypeMsk) ToPointer() *CreateOutputTypeMsk {
	return &e
}
func (e *CreateOutputTypeMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "msk":
		*e = CreateOutputTypeMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeMsk: %v", v)
	}
}

// AcknowledgmentsMsk - Control the number of required acknowledgments.
type AcknowledgmentsMsk int64

const (
	AcknowledgmentsMskOne    AcknowledgmentsMsk = 1
	AcknowledgmentsMskZero   AcknowledgmentsMsk = 0
	AcknowledgmentsMskMinus1 AcknowledgmentsMsk = -1
)

func (e AcknowledgmentsMsk) ToPointer() *AcknowledgmentsMsk {
	return &e
}
func (e *AcknowledgmentsMsk) UnmarshalJSON(data []byte) error {
	var v int64
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case 1:
		fallthrough
	case 0:
		fallthrough
	case -1:
		*e = AcknowledgmentsMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AcknowledgmentsMsk: %v", v)
	}
}

// RecordDataFormatMsk - Format to use to serialize events before writing to Kafka.
type RecordDataFormatMsk string

const (
	RecordDataFormatMskJSON     RecordDataFormatMsk = "json"
	RecordDataFormatMskRaw      RecordDataFormatMsk = "raw"
	RecordDataFormatMskProtobuf RecordDataFormatMsk = "protobuf"
)

func (e RecordDataFormatMsk) ToPointer() *RecordDataFormatMsk {
	return &e
}
func (e *RecordDataFormatMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "protobuf":
		*e = RecordDataFormatMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RecordDataFormatMsk: %v", v)
	}
}

// CreateOutputCompressionMsk - Codec to use to compress the data before sending to Kafka
type CreateOutputCompressionMsk string

const (
	CreateOutputCompressionMskNone   CreateOutputCompressionMsk = "none"
	CreateOutputCompressionMskGzip   CreateOutputCompressionMsk = "gzip"
	CreateOutputCompressionMskSnappy CreateOutputCompressionMsk = "snappy"
	CreateOutputCompressionMskLz4    CreateOutputCompressionMsk = "lz4"
)

func (e CreateOutputCompressionMsk) ToPointer() *CreateOutputCompressionMsk {
	return &e
}
func (e *CreateOutputCompressionMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		fallthrough
	case "snappy":
		fallthrough
	case "lz4":
		*e = CreateOutputCompressionMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputCompressionMsk: %v", v)
	}
}

// CreateOutputAuthMsk - Credentials to use when authenticating with the schema registry using basic HTTP authentication
type CreateOutputAuthMsk struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (c CreateOutputAuthMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputAuthMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputAuthMsk) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *CreateOutputAuthMsk) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

type CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk string

const (
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv1  CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv11 CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1.1"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv12 CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1.2"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv13 CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1.3"
)

func (e CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk) ToPointer() *CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk {
	return &e
}
func (e *CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk: %v", v)
	}
}

type CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk string

const (
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv1  CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv11 CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1.1"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv12 CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1.2"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv13 CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1.3"
)

func (e CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk) ToPointer() *CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk {
	return &e
}
func (e *CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk: %v", v)
	}
}

type CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                              `json:"passphrase,omitempty"`
	MinVersion *CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk `json:"maxVersion,omitempty"`
}

func (c CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetMinVersion() *CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetMaxVersion() *CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

type CreateOutputKafkaSchemaRegistryAuthenticationMsk struct {
	Disabled *bool `default:"true" json:"disabled"`
	// URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.
	SchemaRegistryURL *string `default:"http://localhost:8081" json:"schemaRegistryURL"`
	// Maximum time to wait for a Schema Registry connection to complete successfully
	ConnectionTimeout *float64 `default:"30000" json:"connectionTimeout"`
	// Maximum time to wait for the Schema Registry to respond to a request
	RequestTimeout *float64 `default:"30000" json:"requestTimeout"`
	// Maximum number of times to try fetching schemas from the Schema Registry
	MaxRetries *float64 `default:"1" json:"maxRetries"`
	// Credentials to use when authenticating with the schema registry using basic HTTP authentication
	Auth *CreateOutputAuthMsk                                     `json:"auth,omitempty"`
	TLS  *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk `json:"tls,omitempty"`
	// Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.
	DefaultKeySchemaID *float64 `json:"defaultKeySchemaId,omitempty"`
	// Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.
	DefaultValueSchemaID *float64 `json:"defaultValueSchemaId,omitempty"`
}

func (c CreateOutputKafkaSchemaRegistryAuthenticationMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetSchemaRegistryURL() *string {
	if o == nil {
		return nil
	}
	return o.SchemaRegistryURL
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetAuth() *CreateOutputAuthMsk {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetTLS() *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetDefaultKeySchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultKeySchemaID
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetDefaultValueSchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultValueSchemaID
}

// CreateOutputAuthenticationMethodMsk - AWS authentication method. Choose Auto to use IAM roles.
type CreateOutputAuthenticationMethodMsk string

const (
	CreateOutputAuthenticationMethodMskAuto   CreateOutputAuthenticationMethodMsk = "auto"
	CreateOutputAuthenticationMethodMskManual CreateOutputAuthenticationMethodMsk = "manual"
	CreateOutputAuthenticationMethodMskSecret CreateOutputAuthenticationMethodMsk = "secret"
)

func (e CreateOutputAuthenticationMethodMsk) ToPointer() *CreateOutputAuthenticationMethodMsk {
	return &e
}
func (e *CreateOutputAuthenticationMethodMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = CreateOutputAuthenticationMethodMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputAuthenticationMethodMsk: %v", v)
	}
}

// CreateOutputSignatureVersionMsk - Signature version to use for signing MSK cluster requests
type CreateOutputSignatureVersionMsk string

const (
	CreateOutputSignatureVersionMskV2 CreateOutputSignatureVersionMsk = "v2"
	CreateOutputSignatureVersionMskV4 CreateOutputSignatureVersionMsk = "v4"
)

func (e CreateOutputSignatureVersionMsk) ToPointer() *CreateOutputSignatureVersionMsk {
	return &e
}
func (e *CreateOutputSignatureVersionMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = CreateOutputSignatureVersionMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputSignatureVersionMsk: %v", v)
	}
}

type CreateOutputMinimumTLSVersionMsk string

const (
	CreateOutputMinimumTLSVersionMskTlSv1  CreateOutputMinimumTLSVersionMsk = "TLSv1"
	CreateOutputMinimumTLSVersionMskTlSv11 CreateOutputMinimumTLSVersionMsk = "TLSv1.1"
	CreateOutputMinimumTLSVersionMskTlSv12 CreateOutputMinimumTLSVersionMsk = "TLSv1.2"
	CreateOutputMinimumTLSVersionMskTlSv13 CreateOutputMinimumTLSVersionMsk = "TLSv1.3"
)

func (e CreateOutputMinimumTLSVersionMsk) ToPointer() *CreateOutputMinimumTLSVersionMsk {
	return &e
}
func (e *CreateOutputMinimumTLSVersionMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputMinimumTLSVersionMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputMinimumTLSVersionMsk: %v", v)
	}
}

type CreateOutputMaximumTLSVersionMsk string

const (
	CreateOutputMaximumTLSVersionMskTlSv1  CreateOutputMaximumTLSVersionMsk = "TLSv1"
	CreateOutputMaximumTLSVersionMskTlSv11 CreateOutputMaximumTLSVersionMsk = "TLSv1.1"
	CreateOutputMaximumTLSVersionMskTlSv12 CreateOutputMaximumTLSVersionMsk = "TLSv1.2"
	CreateOutputMaximumTLSVersionMskTlSv13 CreateOutputMaximumTLSVersionMsk = "TLSv1.3"
)

func (e CreateOutputMaximumTLSVersionMsk) ToPointer() *CreateOutputMaximumTLSVersionMsk {
	return &e
}
func (e *CreateOutputMaximumTLSVersionMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputMaximumTLSVersionMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputMaximumTLSVersionMsk: %v", v)
	}
}

type CreateOutputTLSSettingsClientSideMsk struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                           `json:"passphrase,omitempty"`
	MinVersion *CreateOutputMinimumTLSVersionMsk `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputMaximumTLSVersionMsk `json:"maxVersion,omitempty"`
}

func (c CreateOutputTLSSettingsClientSideMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputTLSSettingsClientSideMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputTLSSettingsClientSideMsk) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *CreateOutputTLSSettingsClientSideMsk) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *CreateOutputTLSSettingsClientSideMsk) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *CreateOutputTLSSettingsClientSideMsk) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *CreateOutputTLSSettingsClientSideMsk) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *CreateOutputTLSSettingsClientSideMsk) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *CreateOutputTLSSettingsClientSideMsk) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *CreateOutputTLSSettingsClientSideMsk) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *CreateOutputTLSSettingsClientSideMsk) GetMinVersion() *CreateOutputMinimumTLSVersionMsk {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *CreateOutputTLSSettingsClientSideMsk) GetMaxVersion() *CreateOutputMaximumTLSVersionMsk {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// BackpressureBehaviorMsk - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorMsk string

const (
	BackpressureBehaviorMskBlock BackpressureBehaviorMsk = "block"
	BackpressureBehaviorMskDrop  BackpressureBehaviorMsk = "drop"
	BackpressureBehaviorMskQueue BackpressureBehaviorMsk = "queue"
)

func (e BackpressureBehaviorMsk) ToPointer() *BackpressureBehaviorMsk {
	return &e
}
func (e *BackpressureBehaviorMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorMsk: %v", v)
	}
}

// PqCompressCompressionMsk - Codec to use to compress the persisted data
type PqCompressCompressionMsk string

const (
	PqCompressCompressionMskNone PqCompressCompressionMsk = "none"
	PqCompressCompressionMskGzip PqCompressCompressionMsk = "gzip"
)

func (e PqCompressCompressionMsk) ToPointer() *PqCompressCompressionMsk {
	return &e
}
func (e *PqCompressCompressionMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionMsk: %v", v)
	}
}

// QueueFullBehaviorMsk - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorMsk string

const (
	QueueFullBehaviorMskBlock QueueFullBehaviorMsk = "block"
	QueueFullBehaviorMskDrop  QueueFullBehaviorMsk = "drop"
)

func (e QueueFullBehaviorMsk) ToPointer() *QueueFullBehaviorMsk {
	return &e
}
func (e *QueueFullBehaviorMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorMsk: %v", v)
	}
}

// CreateOutputModeMsk - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeMsk string

const (
	CreateOutputModeMskError        CreateOutputModeMsk = "error"
	CreateOutputModeMskBackpressure CreateOutputModeMsk = "backpressure"
	CreateOutputModeMskAlways       CreateOutputModeMsk = "always"
)

func (e CreateOutputModeMsk) ToPointer() *CreateOutputModeMsk {
	return &e
}
func (e *CreateOutputModeMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = CreateOutputModeMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputModeMsk: %v", v)
	}
}

type PqControlsMsk struct {
}

type OutputMsk struct {
	// Unique ID for this output
	ID   string               `json:"id"`
	Type *CreateOutputTypeMsk `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.
	Brokers []string `json:"brokers"`
	// The topic to publish events to. Can be overridden using the __topicOut field.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments.
	Ack *AcknowledgmentsMsk `default:"1" json:"ack"`
	// Format to use to serialize events before writing to Kafka.
	Format *RecordDataFormatMsk `default:"json" json:"format"`
	// Codec to use to compress the data before sending to Kafka
	Compression *CreateOutputCompressionMsk `default:"gzip" json:"compression"`
	// Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// The maximum number of events you want the Destination to allow in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.
	FlushPeriodSec      *float64                                          `default:"1" json:"flushPeriodSec"`
	KafkaSchemaRegistry *CreateOutputKafkaSchemaRegistryAuthenticationMsk `json:"kafkaSchemaRegistry,omitempty"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *CreateOutputAuthenticationMethodMsk `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                              `json:"awsSecretKey,omitempty"`
	// Region where the MSK cluster is located
	Region string `json:"region"`
	// MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing MSK cluster requests
	SignatureVersion *CreateOutputSignatureVersionMsk `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access MSK
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64                              `default:"3600" json:"durationSeconds"`
	TLS             *CreateOutputTLSSettingsClientSideMsk `json:"tls,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorMsk `default:"block" json:"onBackpressure"`
	Description    *string                  `json:"description,omitempty"`
	AwsAPIKey      *string                  `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Select a set of Protobuf definitions for the events you want to send
	ProtobufLibraryID *string `json:"protobufLibraryId,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionMsk `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorMsk `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *CreateOutputModeMsk `default:"error" json:"pqMode"`
	PqControls *PqControlsMsk       `json:"pqControls,omitempty"`
}

func (o OutputMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputMsk) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputMsk) GetType() *CreateOutputTypeMsk {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputMsk) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputMsk) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputMsk) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputMsk) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputMsk) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputMsk) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputMsk) GetAck() *AcknowledgmentsMsk {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputMsk) GetFormat() *RecordDataFormatMsk {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputMsk) GetCompression() *CreateOutputCompressionMsk {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputMsk) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputMsk) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputMsk) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputMsk) GetKafkaSchemaRegistry() *CreateOutputKafkaSchemaRegistryAuthenticationMsk {
	if o == nil {
		return nil
	}
	return o.KafkaSchemaRegistry
}

func (o *OutputMsk) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputMsk) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputMsk) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputMsk) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputMsk) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputMsk) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputMsk) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputMsk) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputMsk) GetAwsAuthenticationMethod() *CreateOutputAuthenticationMethodMsk {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputMsk) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputMsk) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputMsk) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputMsk) GetSignatureVersion() *CreateOutputSignatureVersionMsk {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputMsk) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputMsk) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputMsk) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputMsk) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputMsk) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputMsk) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputMsk) GetTLS() *CreateOutputTLSSettingsClientSideMsk {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputMsk) GetOnBackpressure() *BackpressureBehaviorMsk {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputMsk) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputMsk) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputMsk) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputMsk) GetProtobufLibraryID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufLibraryID
}

func (o *OutputMsk) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputMsk) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputMsk) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputMsk) GetPqCompress() *PqCompressCompressionMsk {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputMsk) GetPqOnBackpressure() *QueueFullBehaviorMsk {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputMsk) GetPqMode() *CreateOutputModeMsk {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputMsk) GetPqControls() *PqControlsMsk {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeConfluentCloud string

const (
	CreateOutputTypeConfluentCloudConfluentCloud CreateOutputTypeConfluentCloud = "confluent_cloud"
)

func (e CreateOutputTypeConfluentCloud) ToPointer() *CreateOutputTypeConfluentCloud {
	return &e
}
func (e *CreateOutputTypeConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "confluent_cloud":
		*e = CreateOutputTypeConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeConfluentCloud: %v", v)
	}
}

type CreateOutputMinimumTLSVersionConfluentCloud string

const (
	CreateOutputMinimumTLSVersionConfluentCloudTlSv1  CreateOutputMinimumTLSVersionConfluentCloud = "TLSv1"
	CreateOutputMinimumTLSVersionConfluentCloudTlSv11 CreateOutputMinimumTLSVersionConfluentCloud = "TLSv1.1"
	CreateOutputMinimumTLSVersionConfluentCloudTlSv12 CreateOutputMinimumTLSVersionConfluentCloud = "TLSv1.2"
	CreateOutputMinimumTLSVersionConfluentCloudTlSv13 CreateOutputMinimumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e CreateOutputMinimumTLSVersionConfluentCloud) ToPointer() *CreateOutputMinimumTLSVersionConfluentCloud {
	return &e
}
func (e *CreateOutputMinimumTLSVersionConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputMinimumTLSVersionConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputMinimumTLSVersionConfluentCloud: %v", v)
	}
}

type CreateOutputMaximumTLSVersionConfluentCloud string

const (
	CreateOutputMaximumTLSVersionConfluentCloudTlSv1  CreateOutputMaximumTLSVersionConfluentCloud = "TLSv1"
	CreateOutputMaximumTLSVersionConfluentCloudTlSv11 CreateOutputMaximumTLSVersionConfluentCloud = "TLSv1.1"
	CreateOutputMaximumTLSVersionConfluentCloudTlSv12 CreateOutputMaximumTLSVersionConfluentCloud = "TLSv1.2"
	CreateOutputMaximumTLSVersionConfluentCloudTlSv13 CreateOutputMaximumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e CreateOutputMaximumTLSVersionConfluentCloud) ToPointer() *CreateOutputMaximumTLSVersionConfluentCloud {
	return &e
}
func (e *CreateOutputMaximumTLSVersionConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputMaximumTLSVersionConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputMaximumTLSVersionConfluentCloud: %v", v)
	}
}

type CreateOutputTLSSettingsClientSideConfluentCloud struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                      `json:"passphrase,omitempty"`
	MinVersion *CreateOutputMinimumTLSVersionConfluentCloud `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputMaximumTLSVersionConfluentCloud `json:"maxVersion,omitempty"`
}

func (c CreateOutputTLSSettingsClientSideConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputTLSSettingsClientSideConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputTLSSettingsClientSideConfluentCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *CreateOutputTLSSettingsClientSideConfluentCloud) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *CreateOutputTLSSettingsClientSideConfluentCloud) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *CreateOutputTLSSettingsClientSideConfluentCloud) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *CreateOutputTLSSettingsClientSideConfluentCloud) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *CreateOutputTLSSettingsClientSideConfluentCloud) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *CreateOutputTLSSettingsClientSideConfluentCloud) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *CreateOutputTLSSettingsClientSideConfluentCloud) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *CreateOutputTLSSettingsClientSideConfluentCloud) GetMinVersion() *CreateOutputMinimumTLSVersionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *CreateOutputTLSSettingsClientSideConfluentCloud) GetMaxVersion() *CreateOutputMaximumTLSVersionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// AcknowledgmentsConfluentCloud - Control the number of required acknowledgments.
type AcknowledgmentsConfluentCloud int64

const (
	AcknowledgmentsConfluentCloudOne    AcknowledgmentsConfluentCloud = 1
	AcknowledgmentsConfluentCloudZero   AcknowledgmentsConfluentCloud = 0
	AcknowledgmentsConfluentCloudMinus1 AcknowledgmentsConfluentCloud = -1
)

func (e AcknowledgmentsConfluentCloud) ToPointer() *AcknowledgmentsConfluentCloud {
	return &e
}
func (e *AcknowledgmentsConfluentCloud) UnmarshalJSON(data []byte) error {
	var v int64
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case 1:
		fallthrough
	case 0:
		fallthrough
	case -1:
		*e = AcknowledgmentsConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AcknowledgmentsConfluentCloud: %v", v)
	}
}

// RecordDataFormatConfluentCloud - Format to use to serialize events before writing to Kafka.
type RecordDataFormatConfluentCloud string

const (
	RecordDataFormatConfluentCloudJSON     RecordDataFormatConfluentCloud = "json"
	RecordDataFormatConfluentCloudRaw      RecordDataFormatConfluentCloud = "raw"
	RecordDataFormatConfluentCloudProtobuf RecordDataFormatConfluentCloud = "protobuf"
)

func (e RecordDataFormatConfluentCloud) ToPointer() *RecordDataFormatConfluentCloud {
	return &e
}
func (e *RecordDataFormatConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "protobuf":
		*e = RecordDataFormatConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RecordDataFormatConfluentCloud: %v", v)
	}
}

// CreateOutputCompressionConfluentCloud - Codec to use to compress the data before sending to Kafka
type CreateOutputCompressionConfluentCloud string

const (
	CreateOutputCompressionConfluentCloudNone   CreateOutputCompressionConfluentCloud = "none"
	CreateOutputCompressionConfluentCloudGzip   CreateOutputCompressionConfluentCloud = "gzip"
	CreateOutputCompressionConfluentCloudSnappy CreateOutputCompressionConfluentCloud = "snappy"
	CreateOutputCompressionConfluentCloudLz4    CreateOutputCompressionConfluentCloud = "lz4"
)

func (e CreateOutputCompressionConfluentCloud) ToPointer() *CreateOutputCompressionConfluentCloud {
	return &e
}
func (e *CreateOutputCompressionConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		fallthrough
	case "snappy":
		fallthrough
	case "lz4":
		*e = CreateOutputCompressionConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputCompressionConfluentCloud: %v", v)
	}
}

// CreateOutputAuthConfluentCloud - Credentials to use when authenticating with the schema registry using basic HTTP authentication
type CreateOutputAuthConfluentCloud struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (c CreateOutputAuthConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputAuthConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputAuthConfluentCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *CreateOutputAuthConfluentCloud) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

type CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud string

const (
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv1  CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv11 CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1.1"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv12 CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1.2"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv13 CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud) ToPointer() *CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud {
	return &e
}
func (e *CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud: %v", v)
	}
}

type CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud string

const (
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv1  CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv11 CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1.1"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv12 CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1.2"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv13 CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud) ToPointer() *CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud {
	return &e
}
func (e *CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud: %v", v)
	}
}

type CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                                         `json:"passphrase,omitempty"`
	MinVersion *CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud `json:"maxVersion,omitempty"`
}

func (c CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetMinVersion() *CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetMaxVersion() *CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

type CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud struct {
	Disabled *bool `default:"true" json:"disabled"`
	// URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.
	SchemaRegistryURL *string `default:"http://localhost:8081" json:"schemaRegistryURL"`
	// Maximum time to wait for a Schema Registry connection to complete successfully
	ConnectionTimeout *float64 `default:"30000" json:"connectionTimeout"`
	// Maximum time to wait for the Schema Registry to respond to a request
	RequestTimeout *float64 `default:"30000" json:"requestTimeout"`
	// Maximum number of times to try fetching schemas from the Schema Registry
	MaxRetries *float64 `default:"1" json:"maxRetries"`
	// Credentials to use when authenticating with the schema registry using basic HTTP authentication
	Auth *CreateOutputAuthConfluentCloud                                     `json:"auth,omitempty"`
	TLS  *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud `json:"tls,omitempty"`
	// Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.
	DefaultKeySchemaID *float64 `json:"defaultKeySchemaId,omitempty"`
	// Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.
	DefaultValueSchemaID *float64 `json:"defaultValueSchemaId,omitempty"`
}

func (c CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetSchemaRegistryURL() *string {
	if o == nil {
		return nil
	}
	return o.SchemaRegistryURL
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetAuth() *CreateOutputAuthConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetTLS() *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetDefaultKeySchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultKeySchemaID
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetDefaultValueSchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultValueSchemaID
}

type CreateOutputSASLMechanismConfluentCloud string

const (
	CreateOutputSASLMechanismConfluentCloudPlain       CreateOutputSASLMechanismConfluentCloud = "plain"
	CreateOutputSASLMechanismConfluentCloudScramSha256 CreateOutputSASLMechanismConfluentCloud = "scram-sha-256"
	CreateOutputSASLMechanismConfluentCloudScramSha512 CreateOutputSASLMechanismConfluentCloud = "scram-sha-512"
	CreateOutputSASLMechanismConfluentCloudKerberos    CreateOutputSASLMechanismConfluentCloud = "kerberos"
)

func (e CreateOutputSASLMechanismConfluentCloud) ToPointer() *CreateOutputSASLMechanismConfluentCloud {
	return &e
}
func (e *CreateOutputSASLMechanismConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "plain":
		fallthrough
	case "scram-sha-256":
		fallthrough
	case "scram-sha-512":
		fallthrough
	case "kerberos":
		*e = CreateOutputSASLMechanismConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputSASLMechanismConfluentCloud: %v", v)
	}
}

// CreateOutputAuthenticationConfluentCloud - Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
type CreateOutputAuthenticationConfluentCloud struct {
	Disabled  *bool                                    `default:"true" json:"disabled"`
	Mechanism *CreateOutputSASLMechanismConfluentCloud `default:"plain" json:"mechanism"`
}

func (c CreateOutputAuthenticationConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputAuthenticationConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputAuthenticationConfluentCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *CreateOutputAuthenticationConfluentCloud) GetMechanism() *CreateOutputSASLMechanismConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Mechanism
}

// BackpressureBehaviorConfluentCloud - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorConfluentCloud string

const (
	BackpressureBehaviorConfluentCloudBlock BackpressureBehaviorConfluentCloud = "block"
	BackpressureBehaviorConfluentCloudDrop  BackpressureBehaviorConfluentCloud = "drop"
	BackpressureBehaviorConfluentCloudQueue BackpressureBehaviorConfluentCloud = "queue"
)

func (e BackpressureBehaviorConfluentCloud) ToPointer() *BackpressureBehaviorConfluentCloud {
	return &e
}
func (e *BackpressureBehaviorConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorConfluentCloud: %v", v)
	}
}

// PqCompressCompressionConfluentCloud - Codec to use to compress the persisted data
type PqCompressCompressionConfluentCloud string

const (
	PqCompressCompressionConfluentCloudNone PqCompressCompressionConfluentCloud = "none"
	PqCompressCompressionConfluentCloudGzip PqCompressCompressionConfluentCloud = "gzip"
)

func (e PqCompressCompressionConfluentCloud) ToPointer() *PqCompressCompressionConfluentCloud {
	return &e
}
func (e *PqCompressCompressionConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionConfluentCloud: %v", v)
	}
}

// QueueFullBehaviorConfluentCloud - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorConfluentCloud string

const (
	QueueFullBehaviorConfluentCloudBlock QueueFullBehaviorConfluentCloud = "block"
	QueueFullBehaviorConfluentCloudDrop  QueueFullBehaviorConfluentCloud = "drop"
)

func (e QueueFullBehaviorConfluentCloud) ToPointer() *QueueFullBehaviorConfluentCloud {
	return &e
}
func (e *QueueFullBehaviorConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorConfluentCloud: %v", v)
	}
}

// CreateOutputModeConfluentCloud - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeConfluentCloud string

const (
	CreateOutputModeConfluentCloudError        CreateOutputModeConfluentCloud = "error"
	CreateOutputModeConfluentCloudBackpressure CreateOutputModeConfluentCloud = "backpressure"
	CreateOutputModeConfluentCloudAlways       CreateOutputModeConfluentCloud = "always"
)

func (e CreateOutputModeConfluentCloud) ToPointer() *CreateOutputModeConfluentCloud {
	return &e
}
func (e *CreateOutputModeConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = CreateOutputModeConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputModeConfluentCloud: %v", v)
	}
}

type PqControlsConfluentCloud struct {
}

type OutputConfluentCloud struct {
	// Unique ID for this output
	ID   string                          `json:"id"`
	Type *CreateOutputTypeConfluentCloud `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092.
	Brokers []string                                         `json:"brokers"`
	TLS     *CreateOutputTLSSettingsClientSideConfluentCloud `json:"tls,omitempty"`
	// The topic to publish events to. Can be overridden using the __topicOut field.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments.
	Ack *AcknowledgmentsConfluentCloud `default:"1" json:"ack"`
	// Format to use to serialize events before writing to Kafka.
	Format *RecordDataFormatConfluentCloud `default:"json" json:"format"`
	// Codec to use to compress the data before sending to Kafka
	Compression *CreateOutputCompressionConfluentCloud `default:"gzip" json:"compression"`
	// Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// The maximum number of events you want the Destination to allow in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.
	FlushPeriodSec      *float64                                                     `default:"1" json:"flushPeriodSec"`
	KafkaSchemaRegistry *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud `json:"kafkaSchemaRegistry,omitempty"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
	Sasl *CreateOutputAuthenticationConfluentCloud `json:"sasl,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorConfluentCloud `default:"block" json:"onBackpressure"`
	Description    *string                             `json:"description,omitempty"`
	// Select a set of Protobuf definitions for the events you want to send
	ProtobufLibraryID *string `json:"protobufLibraryId,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionConfluentCloud `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorConfluentCloud `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *CreateOutputModeConfluentCloud `default:"error" json:"pqMode"`
	PqControls *PqControlsConfluentCloud       `json:"pqControls,omitempty"`
}

func (o OutputConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputConfluentCloud) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputConfluentCloud) GetType() *CreateOutputTypeConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputConfluentCloud) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputConfluentCloud) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputConfluentCloud) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputConfluentCloud) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputConfluentCloud) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputConfluentCloud) GetTLS() *CreateOutputTLSSettingsClientSideConfluentCloud {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputConfluentCloud) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputConfluentCloud) GetAck() *AcknowledgmentsConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputConfluentCloud) GetFormat() *RecordDataFormatConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputConfluentCloud) GetCompression() *CreateOutputCompressionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputConfluentCloud) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputConfluentCloud) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputConfluentCloud) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputConfluentCloud) GetKafkaSchemaRegistry() *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud {
	if o == nil {
		return nil
	}
	return o.KafkaSchemaRegistry
}

func (o *OutputConfluentCloud) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputConfluentCloud) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputConfluentCloud) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputConfluentCloud) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputConfluentCloud) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputConfluentCloud) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputConfluentCloud) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputConfluentCloud) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputConfluentCloud) GetSasl() *CreateOutputAuthenticationConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Sasl
}

func (o *OutputConfluentCloud) GetOnBackpressure() *BackpressureBehaviorConfluentCloud {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputConfluentCloud) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputConfluentCloud) GetProtobufLibraryID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufLibraryID
}

func (o *OutputConfluentCloud) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputConfluentCloud) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputConfluentCloud) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputConfluentCloud) GetPqCompress() *PqCompressCompressionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputConfluentCloud) GetPqOnBackpressure() *QueueFullBehaviorConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputConfluentCloud) GetPqMode() *CreateOutputModeConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputConfluentCloud) GetPqControls() *PqControlsConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeKafka string

const (
	CreateOutputTypeKafkaKafka CreateOutputTypeKafka = "kafka"
)

func (e CreateOutputTypeKafka) ToPointer() *CreateOutputTypeKafka {
	return &e
}
func (e *CreateOutputTypeKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "kafka":
		*e = CreateOutputTypeKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeKafka: %v", v)
	}
}

// AcknowledgmentsKafka - Control the number of required acknowledgments.
type AcknowledgmentsKafka int64

const (
	AcknowledgmentsKafkaOne    AcknowledgmentsKafka = 1
	AcknowledgmentsKafkaZero   AcknowledgmentsKafka = 0
	AcknowledgmentsKafkaMinus1 AcknowledgmentsKafka = -1
)

func (e AcknowledgmentsKafka) ToPointer() *AcknowledgmentsKafka {
	return &e
}
func (e *AcknowledgmentsKafka) UnmarshalJSON(data []byte) error {
	var v int64
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case 1:
		fallthrough
	case 0:
		fallthrough
	case -1:
		*e = AcknowledgmentsKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AcknowledgmentsKafka: %v", v)
	}
}

// RecordDataFormatKafka - Format to use to serialize events before writing to Kafka.
type RecordDataFormatKafka string

const (
	RecordDataFormatKafkaJSON     RecordDataFormatKafka = "json"
	RecordDataFormatKafkaRaw      RecordDataFormatKafka = "raw"
	RecordDataFormatKafkaProtobuf RecordDataFormatKafka = "protobuf"
)

func (e RecordDataFormatKafka) ToPointer() *RecordDataFormatKafka {
	return &e
}
func (e *RecordDataFormatKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "protobuf":
		*e = RecordDataFormatKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RecordDataFormatKafka: %v", v)
	}
}

// CreateOutputCompressionKafka - Codec to use to compress the data before sending to Kafka
type CreateOutputCompressionKafka string

const (
	CreateOutputCompressionKafkaNone   CreateOutputCompressionKafka = "none"
	CreateOutputCompressionKafkaGzip   CreateOutputCompressionKafka = "gzip"
	CreateOutputCompressionKafkaSnappy CreateOutputCompressionKafka = "snappy"
	CreateOutputCompressionKafkaLz4    CreateOutputCompressionKafka = "lz4"
)

func (e CreateOutputCompressionKafka) ToPointer() *CreateOutputCompressionKafka {
	return &e
}
func (e *CreateOutputCompressionKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		fallthrough
	case "snappy":
		fallthrough
	case "lz4":
		*e = CreateOutputCompressionKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputCompressionKafka: %v", v)
	}
}

// CreateOutputAuthKafka - Credentials to use when authenticating with the schema registry using basic HTTP authentication
type CreateOutputAuthKafka struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (c CreateOutputAuthKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputAuthKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputAuthKafka) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *CreateOutputAuthKafka) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

type CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka string

const (
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv1  CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv11 CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1.1"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv12 CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1.2"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv13 CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1.3"
)

func (e CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka) ToPointer() *CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka {
	return &e
}
func (e *CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka: %v", v)
	}
}

type CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka string

const (
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv1  CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv11 CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1.1"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv12 CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1.2"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv13 CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1.3"
)

func (e CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka) ToPointer() *CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka {
	return &e
}
func (e *CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka: %v", v)
	}
}

type CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                                `json:"passphrase,omitempty"`
	MinVersion *CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka `json:"maxVersion,omitempty"`
}

func (c CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetMinVersion() *CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetMaxVersion() *CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

type CreateOutputKafkaSchemaRegistryAuthenticationKafka struct {
	Disabled *bool `default:"true" json:"disabled"`
	// URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.
	SchemaRegistryURL *string `default:"http://localhost:8081" json:"schemaRegistryURL"`
	// Maximum time to wait for a Schema Registry connection to complete successfully
	ConnectionTimeout *float64 `default:"30000" json:"connectionTimeout"`
	// Maximum time to wait for the Schema Registry to respond to a request
	RequestTimeout *float64 `default:"30000" json:"requestTimeout"`
	// Maximum number of times to try fetching schemas from the Schema Registry
	MaxRetries *float64 `default:"1" json:"maxRetries"`
	// Credentials to use when authenticating with the schema registry using basic HTTP authentication
	Auth *CreateOutputAuthKafka                                     `json:"auth,omitempty"`
	TLS  *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka `json:"tls,omitempty"`
	// Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.
	DefaultKeySchemaID *float64 `json:"defaultKeySchemaId,omitempty"`
	// Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.
	DefaultValueSchemaID *float64 `json:"defaultValueSchemaId,omitempty"`
}

func (c CreateOutputKafkaSchemaRegistryAuthenticationKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetSchemaRegistryURL() *string {
	if o == nil {
		return nil
	}
	return o.SchemaRegistryURL
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetAuth() *CreateOutputAuthKafka {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetTLS() *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetDefaultKeySchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultKeySchemaID
}

func (o *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetDefaultValueSchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultValueSchemaID
}

type CreateOutputSASLMechanismKafka string

const (
	CreateOutputSASLMechanismKafkaPlain       CreateOutputSASLMechanismKafka = "plain"
	CreateOutputSASLMechanismKafkaScramSha256 CreateOutputSASLMechanismKafka = "scram-sha-256"
	CreateOutputSASLMechanismKafkaScramSha512 CreateOutputSASLMechanismKafka = "scram-sha-512"
	CreateOutputSASLMechanismKafkaKerberos    CreateOutputSASLMechanismKafka = "kerberos"
)

func (e CreateOutputSASLMechanismKafka) ToPointer() *CreateOutputSASLMechanismKafka {
	return &e
}
func (e *CreateOutputSASLMechanismKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "plain":
		fallthrough
	case "scram-sha-256":
		fallthrough
	case "scram-sha-512":
		fallthrough
	case "kerberos":
		*e = CreateOutputSASLMechanismKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputSASLMechanismKafka: %v", v)
	}
}

// CreateOutputAuthenticationKafka - Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
type CreateOutputAuthenticationKafka struct {
	Disabled  *bool                           `default:"true" json:"disabled"`
	Mechanism *CreateOutputSASLMechanismKafka `default:"plain" json:"mechanism"`
}

func (c CreateOutputAuthenticationKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputAuthenticationKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputAuthenticationKafka) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *CreateOutputAuthenticationKafka) GetMechanism() *CreateOutputSASLMechanismKafka {
	if o == nil {
		return nil
	}
	return o.Mechanism
}

type CreateOutputMinimumTLSVersionKafka string

const (
	CreateOutputMinimumTLSVersionKafkaTlSv1  CreateOutputMinimumTLSVersionKafka = "TLSv1"
	CreateOutputMinimumTLSVersionKafkaTlSv11 CreateOutputMinimumTLSVersionKafka = "TLSv1.1"
	CreateOutputMinimumTLSVersionKafkaTlSv12 CreateOutputMinimumTLSVersionKafka = "TLSv1.2"
	CreateOutputMinimumTLSVersionKafkaTlSv13 CreateOutputMinimumTLSVersionKafka = "TLSv1.3"
)

func (e CreateOutputMinimumTLSVersionKafka) ToPointer() *CreateOutputMinimumTLSVersionKafka {
	return &e
}
func (e *CreateOutputMinimumTLSVersionKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputMinimumTLSVersionKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputMinimumTLSVersionKafka: %v", v)
	}
}

type CreateOutputMaximumTLSVersionKafka string

const (
	CreateOutputMaximumTLSVersionKafkaTlSv1  CreateOutputMaximumTLSVersionKafka = "TLSv1"
	CreateOutputMaximumTLSVersionKafkaTlSv11 CreateOutputMaximumTLSVersionKafka = "TLSv1.1"
	CreateOutputMaximumTLSVersionKafkaTlSv12 CreateOutputMaximumTLSVersionKafka = "TLSv1.2"
	CreateOutputMaximumTLSVersionKafkaTlSv13 CreateOutputMaximumTLSVersionKafka = "TLSv1.3"
)

func (e CreateOutputMaximumTLSVersionKafka) ToPointer() *CreateOutputMaximumTLSVersionKafka {
	return &e
}
func (e *CreateOutputMaximumTLSVersionKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputMaximumTLSVersionKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputMaximumTLSVersionKafka: %v", v)
	}
}

type CreateOutputTLSSettingsClientSideKafka struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                             `json:"passphrase,omitempty"`
	MinVersion *CreateOutputMinimumTLSVersionKafka `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputMaximumTLSVersionKafka `json:"maxVersion,omitempty"`
}

func (c CreateOutputTLSSettingsClientSideKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputTLSSettingsClientSideKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputTLSSettingsClientSideKafka) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *CreateOutputTLSSettingsClientSideKafka) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *CreateOutputTLSSettingsClientSideKafka) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *CreateOutputTLSSettingsClientSideKafka) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *CreateOutputTLSSettingsClientSideKafka) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *CreateOutputTLSSettingsClientSideKafka) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *CreateOutputTLSSettingsClientSideKafka) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *CreateOutputTLSSettingsClientSideKafka) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *CreateOutputTLSSettingsClientSideKafka) GetMinVersion() *CreateOutputMinimumTLSVersionKafka {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *CreateOutputTLSSettingsClientSideKafka) GetMaxVersion() *CreateOutputMaximumTLSVersionKafka {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// BackpressureBehaviorKafka - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorKafka string

const (
	BackpressureBehaviorKafkaBlock BackpressureBehaviorKafka = "block"
	BackpressureBehaviorKafkaDrop  BackpressureBehaviorKafka = "drop"
	BackpressureBehaviorKafkaQueue BackpressureBehaviorKafka = "queue"
)

func (e BackpressureBehaviorKafka) ToPointer() *BackpressureBehaviorKafka {
	return &e
}
func (e *BackpressureBehaviorKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorKafka: %v", v)
	}
}

// PqCompressCompressionKafka - Codec to use to compress the persisted data
type PqCompressCompressionKafka string

const (
	PqCompressCompressionKafkaNone PqCompressCompressionKafka = "none"
	PqCompressCompressionKafkaGzip PqCompressCompressionKafka = "gzip"
)

func (e PqCompressCompressionKafka) ToPointer() *PqCompressCompressionKafka {
	return &e
}
func (e *PqCompressCompressionKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionKafka: %v", v)
	}
}

// QueueFullBehaviorKafka - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorKafka string

const (
	QueueFullBehaviorKafkaBlock QueueFullBehaviorKafka = "block"
	QueueFullBehaviorKafkaDrop  QueueFullBehaviorKafka = "drop"
)

func (e QueueFullBehaviorKafka) ToPointer() *QueueFullBehaviorKafka {
	return &e
}
func (e *QueueFullBehaviorKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorKafka: %v", v)
	}
}

// CreateOutputModeKafka - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeKafka string

const (
	CreateOutputModeKafkaError        CreateOutputModeKafka = "error"
	CreateOutputModeKafkaBackpressure CreateOutputModeKafka = "backpressure"
	CreateOutputModeKafkaAlways       CreateOutputModeKafka = "always"
)

func (e CreateOutputModeKafka) ToPointer() *CreateOutputModeKafka {
	return &e
}
func (e *CreateOutputModeKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = CreateOutputModeKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputModeKafka: %v", v)
	}
}

type PqControlsKafka struct {
}

type OutputKafka struct {
	// Unique ID for this output
	ID   string                 `json:"id"`
	Type *CreateOutputTypeKafka `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.
	Brokers []string `json:"brokers"`
	// The topic to publish events to. Can be overridden using the __topicOut field.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments.
	Ack *AcknowledgmentsKafka `default:"1" json:"ack"`
	// Format to use to serialize events before writing to Kafka.
	Format *RecordDataFormatKafka `default:"json" json:"format"`
	// Codec to use to compress the data before sending to Kafka
	Compression *CreateOutputCompressionKafka `default:"gzip" json:"compression"`
	// Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// The maximum number of events you want the Destination to allow in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.
	FlushPeriodSec      *float64                                            `default:"1" json:"flushPeriodSec"`
	KafkaSchemaRegistry *CreateOutputKafkaSchemaRegistryAuthenticationKafka `json:"kafkaSchemaRegistry,omitempty"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
	Sasl *CreateOutputAuthenticationKafka        `json:"sasl,omitempty"`
	TLS  *CreateOutputTLSSettingsClientSideKafka `json:"tls,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorKafka `default:"block" json:"onBackpressure"`
	Description    *string                    `json:"description,omitempty"`
	// Select a set of Protobuf definitions for the events you want to send
	ProtobufLibraryID *string `json:"protobufLibraryId,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionKafka `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorKafka `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *CreateOutputModeKafka `default:"error" json:"pqMode"`
	PqControls *PqControlsKafka       `json:"pqControls,omitempty"`
}

func (o OutputKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafka) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputKafka) GetType() *CreateOutputTypeKafka {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputKafka) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputKafka) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputKafka) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputKafka) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputKafka) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputKafka) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputKafka) GetAck() *AcknowledgmentsKafka {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputKafka) GetFormat() *RecordDataFormatKafka {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputKafka) GetCompression() *CreateOutputCompressionKafka {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputKafka) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputKafka) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputKafka) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputKafka) GetKafkaSchemaRegistry() *CreateOutputKafkaSchemaRegistryAuthenticationKafka {
	if o == nil {
		return nil
	}
	return o.KafkaSchemaRegistry
}

func (o *OutputKafka) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputKafka) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputKafka) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputKafka) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputKafka) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputKafka) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputKafka) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputKafka) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputKafka) GetSasl() *CreateOutputAuthenticationKafka {
	if o == nil {
		return nil
	}
	return o.Sasl
}

func (o *OutputKafka) GetTLS() *CreateOutputTLSSettingsClientSideKafka {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputKafka) GetOnBackpressure() *BackpressureBehaviorKafka {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputKafka) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputKafka) GetProtobufLibraryID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufLibraryID
}

func (o *OutputKafka) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputKafka) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputKafka) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputKafka) GetPqCompress() *PqCompressCompressionKafka {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputKafka) GetPqOnBackpressure() *QueueFullBehaviorKafka {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputKafka) GetPqMode() *CreateOutputModeKafka {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputKafka) GetPqControls() *PqControlsKafka {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeExabeam string

const (
	TypeExabeamExabeam TypeExabeam = "exabeam"
)

func (e TypeExabeam) ToPointer() *TypeExabeam {
	return &e
}
func (e *TypeExabeam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "exabeam":
		*e = TypeExabeam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeExabeam: %v", v)
	}
}

// SignatureVersionExabeam - Signature version to use for signing Google Cloud Storage requests
type SignatureVersionExabeam string

const (
	SignatureVersionExabeamV2 SignatureVersionExabeam = "v2"
	SignatureVersionExabeamV4 SignatureVersionExabeam = "v4"
)

func (e SignatureVersionExabeam) ToPointer() *SignatureVersionExabeam {
	return &e
}
func (e *SignatureVersionExabeam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = SignatureVersionExabeam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SignatureVersionExabeam: %v", v)
	}
}

// ObjectACLExabeam - Object ACL to assign to uploaded objects
type ObjectACLExabeam string

const (
	ObjectACLExabeamPrivate                ObjectACLExabeam = "private"
	ObjectACLExabeamBucketOwnerRead        ObjectACLExabeam = "bucket-owner-read"
	ObjectACLExabeamBucketOwnerFullControl ObjectACLExabeam = "bucket-owner-full-control"
	ObjectACLExabeamProjectPrivate         ObjectACLExabeam = "project-private"
	ObjectACLExabeamAuthenticatedRead      ObjectACLExabeam = "authenticated-read"
	ObjectACLExabeamPublicRead             ObjectACLExabeam = "public-read"
)

func (e ObjectACLExabeam) ToPointer() *ObjectACLExabeam {
	return &e
}
func (e *ObjectACLExabeam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "private":
		fallthrough
	case "bucket-owner-read":
		fallthrough
	case "bucket-owner-full-control":
		fallthrough
	case "project-private":
		fallthrough
	case "authenticated-read":
		fallthrough
	case "public-read":
		*e = ObjectACLExabeam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ObjectACLExabeam: %v", v)
	}
}

// StorageClassExabeam - Storage class to select for uploaded objects
type StorageClassExabeam string

const (
	StorageClassExabeamStandard StorageClassExabeam = "STANDARD"
	StorageClassExabeamNearline StorageClassExabeam = "NEARLINE"
	StorageClassExabeamColdline StorageClassExabeam = "COLDLINE"
	StorageClassExabeamArchive  StorageClassExabeam = "ARCHIVE"
)

func (e StorageClassExabeam) ToPointer() *StorageClassExabeam {
	return &e
}
func (e *StorageClassExabeam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "STANDARD":
		fallthrough
	case "NEARLINE":
		fallthrough
	case "COLDLINE":
		fallthrough
	case "ARCHIVE":
		*e = StorageClassExabeam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for StorageClassExabeam: %v", v)
	}
}

// BackpressureBehaviorExabeam - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorExabeam string

const (
	BackpressureBehaviorExabeamBlock BackpressureBehaviorExabeam = "block"
	BackpressureBehaviorExabeamDrop  BackpressureBehaviorExabeam = "drop"
)

func (e BackpressureBehaviorExabeam) ToPointer() *BackpressureBehaviorExabeam {
	return &e
}
func (e *BackpressureBehaviorExabeam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorExabeam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorExabeam: %v", v)
	}
}

// DiskSpaceProtectionExabeam - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionExabeam string

const (
	DiskSpaceProtectionExabeamBlock DiskSpaceProtectionExabeam = "block"
	DiskSpaceProtectionExabeamDrop  DiskSpaceProtectionExabeam = "drop"
)

func (e DiskSpaceProtectionExabeam) ToPointer() *DiskSpaceProtectionExabeam {
	return &e
}
func (e *DiskSpaceProtectionExabeam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionExabeam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionExabeam: %v", v)
	}
}

type OutputExabeam struct {
	// Unique ID for this output
	ID   string       `json:"id"`
	Type *TypeExabeam `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination bucket. A constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a JavaScript Global Variable: `myBucket-${C.vars.myVar}`.
	Bucket string `json:"bucket"`
	// Region where the bucket is located
	Region string `json:"region"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Google Cloud Storage service endpoint
	Endpoint *string `default:"https://storage.googleapis.com" json:"endpoint"`
	// Signature version to use for signing Google Cloud Storage requests
	SignatureVersion *SignatureVersionExabeam `default:"v4" json:"signatureVersion"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLExabeam `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass *StorageClassExabeam `json:"storageClass,omitempty"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorExabeam `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionExabeam `default:"block" json:"onDiskFullBackpressure"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"10" json:"maxFileSizeMB"`
	// Enter an encoded string containing Exabeam configurations
	EncodedConfiguration *string `json:"encodedConfiguration,omitempty"`
	// ID of the Exabeam Collector where data should be sent. Example: 11112222-3333-4444-5555-666677778888
	//
	CollectorInstanceID string `json:"collectorInstanceId"`
	// Constant or JavaScript expression to create an Exabeam site name. Values that aren't successfully evaluated will be treated as string constants.
	SiteName *string `json:"siteName,omitempty"`
	// Exabeam site ID. If left blank, @{product} will use the value of the Exabeam site name.
	SiteID         *string `json:"siteId,omitempty"`
	TimezoneOffset *string `json:"timezoneOffset,omitempty"`
	// HMAC access key. Can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// HMAC secret. Can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	Description  *string `json:"description,omitempty"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputExabeam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputExabeam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputExabeam) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputExabeam) GetType() *TypeExabeam {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputExabeam) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputExabeam) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputExabeam) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputExabeam) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputExabeam) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputExabeam) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputExabeam) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputExabeam) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputExabeam) GetSignatureVersion() *SignatureVersionExabeam {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputExabeam) GetObjectACL() *ObjectACLExabeam {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputExabeam) GetStorageClass() *StorageClassExabeam {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputExabeam) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputExabeam) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputExabeam) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputExabeam) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputExabeam) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputExabeam) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputExabeam) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputExabeam) GetOnBackpressure() *BackpressureBehaviorExabeam {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputExabeam) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputExabeam) GetOnDiskFullBackpressure() *DiskSpaceProtectionExabeam {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputExabeam) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputExabeam) GetEncodedConfiguration() *string {
	if o == nil {
		return nil
	}
	return o.EncodedConfiguration
}

func (o *OutputExabeam) GetCollectorInstanceID() string {
	if o == nil {
		return ""
	}
	return o.CollectorInstanceID
}

func (o *OutputExabeam) GetSiteName() *string {
	if o == nil {
		return nil
	}
	return o.SiteName
}

func (o *OutputExabeam) GetSiteID() *string {
	if o == nil {
		return nil
	}
	return o.SiteID
}

func (o *OutputExabeam) GetTimezoneOffset() *string {
	if o == nil {
		return nil
	}
	return o.TimezoneOffset
}

func (o *OutputExabeam) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputExabeam) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputExabeam) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputExabeam) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputExabeam) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputExabeam) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type CreateOutputTypeGooglePubsub string

const (
	CreateOutputTypeGooglePubsubGooglePubsub CreateOutputTypeGooglePubsub = "google_pubsub"
)

func (e CreateOutputTypeGooglePubsub) ToPointer() *CreateOutputTypeGooglePubsub {
	return &e
}
func (e *CreateOutputTypeGooglePubsub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_pubsub":
		*e = CreateOutputTypeGooglePubsub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeGooglePubsub: %v", v)
	}
}

// CreateOutputGoogleAuthenticationMethodGooglePubsub - Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
type CreateOutputGoogleAuthenticationMethodGooglePubsub string

const (
	CreateOutputGoogleAuthenticationMethodGooglePubsubAuto   CreateOutputGoogleAuthenticationMethodGooglePubsub = "auto"
	CreateOutputGoogleAuthenticationMethodGooglePubsubManual CreateOutputGoogleAuthenticationMethodGooglePubsub = "manual"
	CreateOutputGoogleAuthenticationMethodGooglePubsubSecret CreateOutputGoogleAuthenticationMethodGooglePubsub = "secret"
)

func (e CreateOutputGoogleAuthenticationMethodGooglePubsub) ToPointer() *CreateOutputGoogleAuthenticationMethodGooglePubsub {
	return &e
}
func (e *CreateOutputGoogleAuthenticationMethodGooglePubsub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = CreateOutputGoogleAuthenticationMethodGooglePubsub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputGoogleAuthenticationMethodGooglePubsub: %v", v)
	}
}

type FlushPeriodSecType string

const (
	FlushPeriodSecTypeNumber FlushPeriodSecType = "number"
)

func (e FlushPeriodSecType) ToPointer() *FlushPeriodSecType {
	return &e
}
func (e *FlushPeriodSecType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "number":
		*e = FlushPeriodSecType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FlushPeriodSecType: %v", v)
	}
}

// FlushPeriodSec - Maximum time to wait before sending a batch (when batch size limit is not reached).
type FlushPeriodSec struct {
	Type    *FlushPeriodSecType `json:"type,omitempty"`
	Default *float64            `json:"default,omitempty"`
}

func (o *FlushPeriodSec) GetType() *FlushPeriodSecType {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *FlushPeriodSec) GetDefault() *float64 {
	if o == nil {
		return nil
	}
	return o.Default
}

// BackpressureBehaviorGooglePubsub - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorGooglePubsub string

const (
	BackpressureBehaviorGooglePubsubBlock BackpressureBehaviorGooglePubsub = "block"
	BackpressureBehaviorGooglePubsubDrop  BackpressureBehaviorGooglePubsub = "drop"
	BackpressureBehaviorGooglePubsubQueue BackpressureBehaviorGooglePubsub = "queue"
)

func (e BackpressureBehaviorGooglePubsub) ToPointer() *BackpressureBehaviorGooglePubsub {
	return &e
}
func (e *BackpressureBehaviorGooglePubsub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorGooglePubsub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorGooglePubsub: %v", v)
	}
}

// PqCompressCompressionGooglePubsub - Codec to use to compress the persisted data
type PqCompressCompressionGooglePubsub string

const (
	PqCompressCompressionGooglePubsubNone PqCompressCompressionGooglePubsub = "none"
	PqCompressCompressionGooglePubsubGzip PqCompressCompressionGooglePubsub = "gzip"
)

func (e PqCompressCompressionGooglePubsub) ToPointer() *PqCompressCompressionGooglePubsub {
	return &e
}
func (e *PqCompressCompressionGooglePubsub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionGooglePubsub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionGooglePubsub: %v", v)
	}
}

// QueueFullBehaviorGooglePubsub - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGooglePubsub string

const (
	QueueFullBehaviorGooglePubsubBlock QueueFullBehaviorGooglePubsub = "block"
	QueueFullBehaviorGooglePubsubDrop  QueueFullBehaviorGooglePubsub = "drop"
)

func (e QueueFullBehaviorGooglePubsub) ToPointer() *QueueFullBehaviorGooglePubsub {
	return &e
}
func (e *QueueFullBehaviorGooglePubsub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorGooglePubsub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorGooglePubsub: %v", v)
	}
}

// CreateOutputModeGooglePubsub - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeGooglePubsub string

const (
	CreateOutputModeGooglePubsubError        CreateOutputModeGooglePubsub = "error"
	CreateOutputModeGooglePubsubBackpressure CreateOutputModeGooglePubsub = "backpressure"
	CreateOutputModeGooglePubsubAlways       CreateOutputModeGooglePubsub = "always"
)

func (e CreateOutputModeGooglePubsub) ToPointer() *CreateOutputModeGooglePubsub {
	return &e
}
func (e *CreateOutputModeGooglePubsub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = CreateOutputModeGooglePubsub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputModeGooglePubsub: %v", v)
	}
}

type PqControlsGooglePubsub struct {
}

type OutputGooglePubsub struct {
	// Unique ID for this output
	ID   string                       `json:"id"`
	Type CreateOutputTypeGooglePubsub `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// ID of the topic to send events to.
	TopicName string `json:"topicName"`
	// If enabled, create topic if it does not exist.
	CreateTopic *bool `default:"false" json:"createTopic"`
	// If enabled, send events in the order they were added to the queue. For this to work correctly, the process receiving events must have ordering enabled.
	OrderedDelivery *bool `default:"false" json:"orderedDelivery"`
	// Region to publish messages to. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.
	Region *string `json:"region,omitempty"`
	// Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
	GoogleAuthMethod *CreateOutputGoogleAuthenticationMethodGooglePubsub `default:"manual" json:"googleAuthMethod"`
	// Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
	ServiceAccountCredentials *string `json:"serviceAccountCredentials,omitempty"`
	// Select or create a stored text secret
	Secret *string `json:"secret,omitempty"`
	// The maximum number of items the Google API should batch before it sends them to the topic.
	BatchSize *float64 `default:"1000" json:"batchSize"`
	// The maximum amount of time, in milliseconds, that the Google API should wait to send a batch (if the Batch size is not reached).
	BatchTimeout *float64 `default:"100" json:"batchTimeout"`
	// Maximum number of queued batches before blocking.
	MaxQueueSize *float64 `default:"100" json:"maxQueueSize"`
	// Maximum size (KB) of batches to send.
	MaxRecordSizeKB *float64 `default:"256" json:"maxRecordSizeKB"`
	// Maximum time to wait before sending a batch (when batch size limit is not reached).
	FlushPeriodSec *FlushPeriodSec `json:"flushPeriodSec,omitempty"`
	// The maximum number of in-progress API requests before backpressure is applied.
	MaxInProgress *float64 `default:"10" json:"maxInProgress"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGooglePubsub `default:"block" json:"onBackpressure"`
	Description    *string                           `json:"description,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionGooglePubsub `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorGooglePubsub `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *CreateOutputModeGooglePubsub `default:"error" json:"pqMode"`
	PqControls *PqControlsGooglePubsub       `json:"pqControls,omitempty"`
}

func (o OutputGooglePubsub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGooglePubsub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputGooglePubsub) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputGooglePubsub) GetType() CreateOutputTypeGooglePubsub {
	if o == nil {
		return CreateOutputTypeGooglePubsub("")
	}
	return o.Type
}

func (o *OutputGooglePubsub) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGooglePubsub) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGooglePubsub) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGooglePubsub) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGooglePubsub) GetTopicName() string {
	if o == nil {
		return ""
	}
	return o.TopicName
}

func (o *OutputGooglePubsub) GetCreateTopic() *bool {
	if o == nil {
		return nil
	}
	return o.CreateTopic
}

func (o *OutputGooglePubsub) GetOrderedDelivery() *bool {
	if o == nil {
		return nil
	}
	return o.OrderedDelivery
}

func (o *OutputGooglePubsub) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputGooglePubsub) GetGoogleAuthMethod() *CreateOutputGoogleAuthenticationMethodGooglePubsub {
	if o == nil {
		return nil
	}
	return o.GoogleAuthMethod
}

func (o *OutputGooglePubsub) GetServiceAccountCredentials() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentials
}

func (o *OutputGooglePubsub) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputGooglePubsub) GetBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.BatchSize
}

func (o *OutputGooglePubsub) GetBatchTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.BatchTimeout
}

func (o *OutputGooglePubsub) GetMaxQueueSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxQueueSize
}

func (o *OutputGooglePubsub) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputGooglePubsub) GetFlushPeriodSec() *FlushPeriodSec {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGooglePubsub) GetMaxInProgress() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxInProgress
}

func (o *OutputGooglePubsub) GetOnBackpressure() *BackpressureBehaviorGooglePubsub {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGooglePubsub) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGooglePubsub) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGooglePubsub) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGooglePubsub) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGooglePubsub) GetPqCompress() *PqCompressCompressionGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGooglePubsub) GetPqOnBackpressure() *QueueFullBehaviorGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGooglePubsub) GetPqMode() *CreateOutputModeGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGooglePubsub) GetPqControls() *PqControlsGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeGoogleCloudLogging string

const (
	TypeGoogleCloudLoggingGoogleCloudLogging TypeGoogleCloudLogging = "google_cloud_logging"
)

func (e TypeGoogleCloudLogging) ToPointer() *TypeGoogleCloudLogging {
	return &e
}
func (e *TypeGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_cloud_logging":
		*e = TypeGoogleCloudLogging(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGoogleCloudLogging: %v", v)
	}
}

type LogLocationType string

const (
	LogLocationTypeProject        LogLocationType = "project"
	LogLocationTypeOrganization   LogLocationType = "organization"
	LogLocationTypeBillingAccount LogLocationType = "billingAccount"
	LogLocationTypeFolder         LogLocationType = "folder"
)

func (e LogLocationType) ToPointer() *LogLocationType {
	return &e
}
func (e *LogLocationType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "project":
		fallthrough
	case "organization":
		fallthrough
	case "billingAccount":
		fallthrough
	case "folder":
		*e = LogLocationType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for LogLocationType: %v", v)
	}
}

// PayloadFormat - Format to use when sending payload. Defaults to Text.
type PayloadFormat string

const (
	PayloadFormatText PayloadFormat = "text"
	PayloadFormatJSON PayloadFormat = "json"
)

func (e PayloadFormat) ToPointer() *PayloadFormat {
	return &e
}
func (e *PayloadFormat) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "text":
		fallthrough
	case "json":
		*e = PayloadFormat(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PayloadFormat: %v", v)
	}
}

type LogLabel struct {
	// Label name
	Label string `json:"label"`
	// JavaScript expression to compute the label's value.
	ValueExpression string `json:"valueExpression"`
}

func (o *LogLabel) GetLabel() string {
	if o == nil {
		return ""
	}
	return o.Label
}

func (o *LogLabel) GetValueExpression() string {
	if o == nil {
		return ""
	}
	return o.ValueExpression
}

type ResourceTypeLabel struct {
	// Label name
	Label string `json:"label"`
	// JavaScript expression to compute the label's value.
	ValueExpression string `json:"valueExpression"`
}

func (o *ResourceTypeLabel) GetLabel() string {
	if o == nil {
		return ""
	}
	return o.Label
}

func (o *ResourceTypeLabel) GetValueExpression() string {
	if o == nil {
		return ""
	}
	return o.ValueExpression
}

// GoogleAuthenticationMethodGoogleCloudLogging - Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
type GoogleAuthenticationMethodGoogleCloudLogging string

const (
	GoogleAuthenticationMethodGoogleCloudLoggingAuto   GoogleAuthenticationMethodGoogleCloudLogging = "auto"
	GoogleAuthenticationMethodGoogleCloudLoggingManual GoogleAuthenticationMethodGoogleCloudLogging = "manual"
	GoogleAuthenticationMethodGoogleCloudLoggingSecret GoogleAuthenticationMethodGoogleCloudLogging = "secret"
)

func (e GoogleAuthenticationMethodGoogleCloudLogging) ToPointer() *GoogleAuthenticationMethodGoogleCloudLogging {
	return &e
}
func (e *GoogleAuthenticationMethodGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = GoogleAuthenticationMethodGoogleCloudLogging(v)
		return nil
	default:
		return fmt.Errorf("invalid value for GoogleAuthenticationMethodGoogleCloudLogging: %v", v)
	}
}

// BackpressureBehaviorGoogleCloudLogging - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorGoogleCloudLogging string

const (
	BackpressureBehaviorGoogleCloudLoggingBlock BackpressureBehaviorGoogleCloudLogging = "block"
	BackpressureBehaviorGoogleCloudLoggingDrop  BackpressureBehaviorGoogleCloudLogging = "drop"
	BackpressureBehaviorGoogleCloudLoggingQueue BackpressureBehaviorGoogleCloudLogging = "queue"
)

func (e BackpressureBehaviorGoogleCloudLogging) ToPointer() *BackpressureBehaviorGoogleCloudLogging {
	return &e
}
func (e *BackpressureBehaviorGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorGoogleCloudLogging(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorGoogleCloudLogging: %v", v)
	}
}

// CompressionGoogleCloudLogging - Codec to use to compress the persisted data
type CompressionGoogleCloudLogging string

const (
	CompressionGoogleCloudLoggingNone CompressionGoogleCloudLogging = "none"
	CompressionGoogleCloudLoggingGzip CompressionGoogleCloudLogging = "gzip"
)

func (e CompressionGoogleCloudLogging) ToPointer() *CompressionGoogleCloudLogging {
	return &e
}
func (e *CompressionGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionGoogleCloudLogging(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionGoogleCloudLogging: %v", v)
	}
}

// QueueFullBehaviorGoogleCloudLogging - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGoogleCloudLogging string

const (
	QueueFullBehaviorGoogleCloudLoggingBlock QueueFullBehaviorGoogleCloudLogging = "block"
	QueueFullBehaviorGoogleCloudLoggingDrop  QueueFullBehaviorGoogleCloudLogging = "drop"
)

func (e QueueFullBehaviorGoogleCloudLogging) ToPointer() *QueueFullBehaviorGoogleCloudLogging {
	return &e
}
func (e *QueueFullBehaviorGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorGoogleCloudLogging(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorGoogleCloudLogging: %v", v)
	}
}

// ModeGoogleCloudLogging - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeGoogleCloudLogging string

const (
	ModeGoogleCloudLoggingError        ModeGoogleCloudLogging = "error"
	ModeGoogleCloudLoggingBackpressure ModeGoogleCloudLogging = "backpressure"
	ModeGoogleCloudLoggingAlways       ModeGoogleCloudLogging = "always"
)

func (e ModeGoogleCloudLogging) ToPointer() *ModeGoogleCloudLogging {
	return &e
}
func (e *ModeGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeGoogleCloudLogging(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeGoogleCloudLogging: %v", v)
	}
}

type PqControlsGoogleCloudLogging struct {
}

type OutputGoogleCloudLogging struct {
	// Unique ID for this output
	ID   string                  `json:"id"`
	Type *TypeGoogleCloudLogging `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags      []string        `json:"streamtags,omitempty"`
	LogLocationType LogLocationType `json:"logLocationType"`
	// JavaScript expression to compute the value of the log name.
	LogNameExpression string `json:"logNameExpression"`
	// Format to use when sending payload. Defaults to Text.
	PayloadFormat *PayloadFormat `default:"text" json:"payloadFormat"`
	// Labels to apply to the log entry
	LogLabels []LogLabel `json:"logLabels,omitempty"`
	// JavaScript expression to compute the value of the managed resource type field. Must evaluate to one of the valid values [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types). Defaults to "global".
	ResourceTypeExpression *string `json:"resourceTypeExpression,omitempty"`
	// Labels to apply to the managed resource. These must correspond to the valid labels for the specified resource type (see [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types)). Otherwise, they will be dropped by Google Cloud Logging.
	ResourceTypeLabels []ResourceTypeLabel `json:"resourceTypeLabels,omitempty"`
	// JavaScript expression to compute the value of the severity field. Must evaluate to one of the severity values supported by Google Cloud Logging [here](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logseverity) (case insensitive). Defaults to "DEFAULT".
	SeverityExpression *string `json:"severityExpression,omitempty"`
	// JavaScript expression to compute the value of the insert ID field.
	InsertIDExpression *string `json:"insertIdExpression,omitempty"`
	// Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
	GoogleAuthMethod *GoogleAuthenticationMethodGoogleCloudLogging `default:"manual" json:"googleAuthMethod"`
	// Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
	ServiceAccountCredentials *string `json:"serviceAccountCredentials,omitempty"`
	// Select or create a stored text secret
	Secret *string `json:"secret,omitempty"`
	// Maximum size, in KB, of the request body.
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Max number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Maximum number of ongoing requests before blocking.
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it.
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum number of requests to limit to per second.
	ThrottleRateReqPerSec *int64 `json:"throttleRateReqPerSec,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request method as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RequestMethodExpression *string `json:"requestMethodExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request URL as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RequestURLExpression *string `json:"requestUrlExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RequestSizeExpression *string `json:"requestSizeExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request method as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	StatusExpression *string `json:"statusExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP response size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	ResponseSizeExpression *string `json:"responseSizeExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request user agent as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	UserAgentExpression *string `json:"userAgentExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request remote IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RemoteIPExpression *string `json:"remoteIpExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request server IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	ServerIPExpression *string `json:"serverIpExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request referer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RefererExpression *string `json:"refererExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request latency, formatted as <seconds>.<nanoseconds>s (for example, 1.23s). See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	LatencyExpression *string `json:"latencyExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache lookup as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheLookupExpression *string `json:"cacheLookupExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache hit as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheHitExpression *string `json:"cacheHitExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache validated with origin server as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheValidatedExpression *string `json:"cacheValidatedExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache fill bytes as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheFillBytesExpression *string `json:"cacheFillBytesExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request protocol as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	ProtocolExpression *string `json:"protocolExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation ID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	IDExpression *string `json:"idExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation producer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	ProducerExpression *string `json:"producerExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation first flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	FirstExpression *string `json:"firstExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation last flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	LastExpression *string `json:"lastExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry source location file as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.
	FileExpression *string `json:"fileExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry source location line as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.
	LineExpression *string `json:"lineExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry source location function as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.
	FunctionExpression *string `json:"functionExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry log split UID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.
	UIDExpression *string `json:"uidExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry log split index as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.
	IndexExpression *string `json:"indexExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry log split total splits as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.
	TotalSplitsExpression *string `json:"totalSplitsExpression,omitempty"`
	// A JavaScript expression that evaluates to the REST resource name of the trace being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.
	TraceExpression *string `json:"traceExpression,omitempty"`
	// A JavaScript expression that evaluates to the ID of the cloud trace span associated with the current operation in which the log is being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.
	SpanIDExpression *string `json:"spanIdExpression,omitempty"`
	// A JavaScript expression that evaluates to the the sampling decision of the span associated with the log entry. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.
	TraceSampledExpression *string `json:"traceSampledExpression,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGoogleCloudLogging `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// JavaScript expression to compute the value of the folder ID with which log entries should be associated.
	LogLocationExpression string `json:"logLocationExpression"`
	// JavaScript expression to compute the value of the payload. Must evaluate to a JavaScript object value. If an invalid value is encountered it will result in the default value instead. Defaults to the entire event.
	PayloadExpression *string `json:"payloadExpression,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionGoogleCloudLogging `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorGoogleCloudLogging `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeGoogleCloudLogging       `default:"error" json:"pqMode"`
	PqControls *PqControlsGoogleCloudLogging `json:"pqControls,omitempty"`
}

func (o OutputGoogleCloudLogging) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputGoogleCloudLogging) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputGoogleCloudLogging) GetType() *TypeGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputGoogleCloudLogging) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGoogleCloudLogging) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGoogleCloudLogging) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGoogleCloudLogging) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGoogleCloudLogging) GetLogLocationType() LogLocationType {
	if o == nil {
		return LogLocationType("")
	}
	return o.LogLocationType
}

func (o *OutputGoogleCloudLogging) GetLogNameExpression() string {
	if o == nil {
		return ""
	}
	return o.LogNameExpression
}

func (o *OutputGoogleCloudLogging) GetPayloadFormat() *PayloadFormat {
	if o == nil {
		return nil
	}
	return o.PayloadFormat
}

func (o *OutputGoogleCloudLogging) GetLogLabels() []LogLabel {
	if o == nil {
		return nil
	}
	return o.LogLabels
}

func (o *OutputGoogleCloudLogging) GetResourceTypeExpression() *string {
	if o == nil {
		return nil
	}
	return o.ResourceTypeExpression
}

func (o *OutputGoogleCloudLogging) GetResourceTypeLabels() []ResourceTypeLabel {
	if o == nil {
		return nil
	}
	return o.ResourceTypeLabels
}

func (o *OutputGoogleCloudLogging) GetSeverityExpression() *string {
	if o == nil {
		return nil
	}
	return o.SeverityExpression
}

func (o *OutputGoogleCloudLogging) GetInsertIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.InsertIDExpression
}

func (o *OutputGoogleCloudLogging) GetGoogleAuthMethod() *GoogleAuthenticationMethodGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.GoogleAuthMethod
}

func (o *OutputGoogleCloudLogging) GetServiceAccountCredentials() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentials
}

func (o *OutputGoogleCloudLogging) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputGoogleCloudLogging) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputGoogleCloudLogging) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputGoogleCloudLogging) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGoogleCloudLogging) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputGoogleCloudLogging) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputGoogleCloudLogging) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputGoogleCloudLogging) GetThrottleRateReqPerSec() *int64 {
	if o == nil {
		return nil
	}
	return o.ThrottleRateReqPerSec
}

func (o *OutputGoogleCloudLogging) GetRequestMethodExpression() *string {
	if o == nil {
		return nil
	}
	return o.RequestMethodExpression
}

func (o *OutputGoogleCloudLogging) GetRequestURLExpression() *string {
	if o == nil {
		return nil
	}
	return o.RequestURLExpression
}

func (o *OutputGoogleCloudLogging) GetRequestSizeExpression() *string {
	if o == nil {
		return nil
	}
	return o.RequestSizeExpression
}

func (o *OutputGoogleCloudLogging) GetStatusExpression() *string {
	if o == nil {
		return nil
	}
	return o.StatusExpression
}

func (o *OutputGoogleCloudLogging) GetResponseSizeExpression() *string {
	if o == nil {
		return nil
	}
	return o.ResponseSizeExpression
}

func (o *OutputGoogleCloudLogging) GetUserAgentExpression() *string {
	if o == nil {
		return nil
	}
	return o.UserAgentExpression
}

func (o *OutputGoogleCloudLogging) GetRemoteIPExpression() *string {
	if o == nil {
		return nil
	}
	return o.RemoteIPExpression
}

func (o *OutputGoogleCloudLogging) GetServerIPExpression() *string {
	if o == nil {
		return nil
	}
	return o.ServerIPExpression
}

func (o *OutputGoogleCloudLogging) GetRefererExpression() *string {
	if o == nil {
		return nil
	}
	return o.RefererExpression
}

func (o *OutputGoogleCloudLogging) GetLatencyExpression() *string {
	if o == nil {
		return nil
	}
	return o.LatencyExpression
}

func (o *OutputGoogleCloudLogging) GetCacheLookupExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheLookupExpression
}

func (o *OutputGoogleCloudLogging) GetCacheHitExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheHitExpression
}

func (o *OutputGoogleCloudLogging) GetCacheValidatedExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheValidatedExpression
}

func (o *OutputGoogleCloudLogging) GetCacheFillBytesExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheFillBytesExpression
}

func (o *OutputGoogleCloudLogging) GetProtocolExpression() *string {
	if o == nil {
		return nil
	}
	return o.ProtocolExpression
}

func (o *OutputGoogleCloudLogging) GetIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.IDExpression
}

func (o *OutputGoogleCloudLogging) GetProducerExpression() *string {
	if o == nil {
		return nil
	}
	return o.ProducerExpression
}

func (o *OutputGoogleCloudLogging) GetFirstExpression() *string {
	if o == nil {
		return nil
	}
	return o.FirstExpression
}

func (o *OutputGoogleCloudLogging) GetLastExpression() *string {
	if o == nil {
		return nil
	}
	return o.LastExpression
}

func (o *OutputGoogleCloudLogging) GetFileExpression() *string {
	if o == nil {
		return nil
	}
	return o.FileExpression
}

func (o *OutputGoogleCloudLogging) GetLineExpression() *string {
	if o == nil {
		return nil
	}
	return o.LineExpression
}

func (o *OutputGoogleCloudLogging) GetFunctionExpression() *string {
	if o == nil {
		return nil
	}
	return o.FunctionExpression
}

func (o *OutputGoogleCloudLogging) GetUIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.UIDExpression
}

func (o *OutputGoogleCloudLogging) GetIndexExpression() *string {
	if o == nil {
		return nil
	}
	return o.IndexExpression
}

func (o *OutputGoogleCloudLogging) GetTotalSplitsExpression() *string {
	if o == nil {
		return nil
	}
	return o.TotalSplitsExpression
}

func (o *OutputGoogleCloudLogging) GetTraceExpression() *string {
	if o == nil {
		return nil
	}
	return o.TraceExpression
}

func (o *OutputGoogleCloudLogging) GetSpanIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.SpanIDExpression
}

func (o *OutputGoogleCloudLogging) GetTraceSampledExpression() *string {
	if o == nil {
		return nil
	}
	return o.TraceSampledExpression
}

func (o *OutputGoogleCloudLogging) GetOnBackpressure() *BackpressureBehaviorGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGoogleCloudLogging) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputGoogleCloudLogging) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGoogleCloudLogging) GetLogLocationExpression() string {
	if o == nil {
		return ""
	}
	return o.LogLocationExpression
}

func (o *OutputGoogleCloudLogging) GetPayloadExpression() *string {
	if o == nil {
		return nil
	}
	return o.PayloadExpression
}

func (o *OutputGoogleCloudLogging) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGoogleCloudLogging) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGoogleCloudLogging) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGoogleCloudLogging) GetPqCompress() *CompressionGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGoogleCloudLogging) GetPqOnBackpressure() *QueueFullBehaviorGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGoogleCloudLogging) GetPqMode() *ModeGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGoogleCloudLogging) GetPqControls() *PqControlsGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeGoogleCloudStorage string

const (
	TypeGoogleCloudStorageGoogleCloudStorage TypeGoogleCloudStorage = "google_cloud_storage"
)

func (e TypeGoogleCloudStorage) ToPointer() *TypeGoogleCloudStorage {
	return &e
}
func (e *TypeGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_cloud_storage":
		*e = TypeGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGoogleCloudStorage: %v", v)
	}
}

// SignatureVersionGoogleCloudStorage - Signature version to use for signing Google Cloud Storage requests
type SignatureVersionGoogleCloudStorage string

const (
	SignatureVersionGoogleCloudStorageV2 SignatureVersionGoogleCloudStorage = "v2"
	SignatureVersionGoogleCloudStorageV4 SignatureVersionGoogleCloudStorage = "v4"
)

func (e SignatureVersionGoogleCloudStorage) ToPointer() *SignatureVersionGoogleCloudStorage {
	return &e
}
func (e *SignatureVersionGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = SignatureVersionGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SignatureVersionGoogleCloudStorage: %v", v)
	}
}

type AuthenticationMethodGoogleCloudStorage string

const (
	AuthenticationMethodGoogleCloudStorageAuto   AuthenticationMethodGoogleCloudStorage = "auto"
	AuthenticationMethodGoogleCloudStorageManual AuthenticationMethodGoogleCloudStorage = "manual"
	AuthenticationMethodGoogleCloudStorageSecret AuthenticationMethodGoogleCloudStorage = "secret"
)

func (e AuthenticationMethodGoogleCloudStorage) ToPointer() *AuthenticationMethodGoogleCloudStorage {
	return &e
}
func (e *AuthenticationMethodGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodGoogleCloudStorage: %v", v)
	}
}

// ObjectACLGoogleCloudStorage - Object ACL to assign to uploaded objects
type ObjectACLGoogleCloudStorage string

const (
	ObjectACLGoogleCloudStoragePrivate                ObjectACLGoogleCloudStorage = "private"
	ObjectACLGoogleCloudStorageBucketOwnerRead        ObjectACLGoogleCloudStorage = "bucket-owner-read"
	ObjectACLGoogleCloudStorageBucketOwnerFullControl ObjectACLGoogleCloudStorage = "bucket-owner-full-control"
	ObjectACLGoogleCloudStorageProjectPrivate         ObjectACLGoogleCloudStorage = "project-private"
	ObjectACLGoogleCloudStorageAuthenticatedRead      ObjectACLGoogleCloudStorage = "authenticated-read"
	ObjectACLGoogleCloudStoragePublicRead             ObjectACLGoogleCloudStorage = "public-read"
)

func (e ObjectACLGoogleCloudStorage) ToPointer() *ObjectACLGoogleCloudStorage {
	return &e
}
func (e *ObjectACLGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "private":
		fallthrough
	case "bucket-owner-read":
		fallthrough
	case "bucket-owner-full-control":
		fallthrough
	case "project-private":
		fallthrough
	case "authenticated-read":
		fallthrough
	case "public-read":
		*e = ObjectACLGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ObjectACLGoogleCloudStorage: %v", v)
	}
}

// StorageClassGoogleCloudStorage - Storage class to select for uploaded objects
type StorageClassGoogleCloudStorage string

const (
	StorageClassGoogleCloudStorageStandard StorageClassGoogleCloudStorage = "STANDARD"
	StorageClassGoogleCloudStorageNearline StorageClassGoogleCloudStorage = "NEARLINE"
	StorageClassGoogleCloudStorageColdline StorageClassGoogleCloudStorage = "COLDLINE"
	StorageClassGoogleCloudStorageArchive  StorageClassGoogleCloudStorage = "ARCHIVE"
)

func (e StorageClassGoogleCloudStorage) ToPointer() *StorageClassGoogleCloudStorage {
	return &e
}
func (e *StorageClassGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "STANDARD":
		fallthrough
	case "NEARLINE":
		fallthrough
	case "COLDLINE":
		fallthrough
	case "ARCHIVE":
		*e = StorageClassGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for StorageClassGoogleCloudStorage: %v", v)
	}
}

// DataFormatGoogleCloudStorage - Format of the output data
type DataFormatGoogleCloudStorage string

const (
	DataFormatGoogleCloudStorageJSON    DataFormatGoogleCloudStorage = "json"
	DataFormatGoogleCloudStorageRaw     DataFormatGoogleCloudStorage = "raw"
	DataFormatGoogleCloudStorageParquet DataFormatGoogleCloudStorage = "parquet"
)

func (e DataFormatGoogleCloudStorage) ToPointer() *DataFormatGoogleCloudStorage {
	return &e
}
func (e *DataFormatGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "parquet":
		*e = DataFormatGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataFormatGoogleCloudStorage: %v", v)
	}
}

// BackpressureBehaviorGoogleCloudStorage - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorGoogleCloudStorage string

const (
	BackpressureBehaviorGoogleCloudStorageBlock BackpressureBehaviorGoogleCloudStorage = "block"
	BackpressureBehaviorGoogleCloudStorageDrop  BackpressureBehaviorGoogleCloudStorage = "drop"
)

func (e BackpressureBehaviorGoogleCloudStorage) ToPointer() *BackpressureBehaviorGoogleCloudStorage {
	return &e
}
func (e *BackpressureBehaviorGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorGoogleCloudStorage: %v", v)
	}
}

// DiskSpaceProtectionGoogleCloudStorage - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionGoogleCloudStorage string

const (
	DiskSpaceProtectionGoogleCloudStorageBlock DiskSpaceProtectionGoogleCloudStorage = "block"
	DiskSpaceProtectionGoogleCloudStorageDrop  DiskSpaceProtectionGoogleCloudStorage = "drop"
)

func (e DiskSpaceProtectionGoogleCloudStorage) ToPointer() *DiskSpaceProtectionGoogleCloudStorage {
	return &e
}
func (e *DiskSpaceProtectionGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionGoogleCloudStorage: %v", v)
	}
}

// CompressionGoogleCloudStorage - Data compression format to apply to HTTP content before it is delivered
type CompressionGoogleCloudStorage string

const (
	CompressionGoogleCloudStorageNone CompressionGoogleCloudStorage = "none"
	CompressionGoogleCloudStorageGzip CompressionGoogleCloudStorage = "gzip"
)

func (e CompressionGoogleCloudStorage) ToPointer() *CompressionGoogleCloudStorage {
	return &e
}
func (e *CompressionGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionGoogleCloudStorage: %v", v)
	}
}

// CompressionLevelGoogleCloudStorage - Compression level to apply before moving files to final destination
type CompressionLevelGoogleCloudStorage string

const (
	CompressionLevelGoogleCloudStorageBestSpeed       CompressionLevelGoogleCloudStorage = "best_speed"
	CompressionLevelGoogleCloudStorageNormal          CompressionLevelGoogleCloudStorage = "normal"
	CompressionLevelGoogleCloudStorageBestCompression CompressionLevelGoogleCloudStorage = "best_compression"
)

func (e CompressionLevelGoogleCloudStorage) ToPointer() *CompressionLevelGoogleCloudStorage {
	return &e
}
func (e *CompressionLevelGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "best_speed":
		fallthrough
	case "normal":
		fallthrough
	case "best_compression":
		*e = CompressionLevelGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionLevelGoogleCloudStorage: %v", v)
	}
}

// ParquetVersionGoogleCloudStorage - Determines which data types are supported and how they are represented
type ParquetVersionGoogleCloudStorage string

const (
	ParquetVersionGoogleCloudStorageParquet10 ParquetVersionGoogleCloudStorage = "PARQUET_1_0"
	ParquetVersionGoogleCloudStorageParquet24 ParquetVersionGoogleCloudStorage = "PARQUET_2_4"
	ParquetVersionGoogleCloudStorageParquet26 ParquetVersionGoogleCloudStorage = "PARQUET_2_6"
)

func (e ParquetVersionGoogleCloudStorage) ToPointer() *ParquetVersionGoogleCloudStorage {
	return &e
}
func (e *ParquetVersionGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PARQUET_1_0":
		fallthrough
	case "PARQUET_2_4":
		fallthrough
	case "PARQUET_2_6":
		*e = ParquetVersionGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ParquetVersionGoogleCloudStorage: %v", v)
	}
}

// DataPageVersionGoogleCloudStorage - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionGoogleCloudStorage string

const (
	DataPageVersionGoogleCloudStorageDataPageV1 DataPageVersionGoogleCloudStorage = "DATA_PAGE_V1"
	DataPageVersionGoogleCloudStorageDataPageV2 DataPageVersionGoogleCloudStorage = "DATA_PAGE_V2"
)

func (e DataPageVersionGoogleCloudStorage) ToPointer() *DataPageVersionGoogleCloudStorage {
	return &e
}
func (e *DataPageVersionGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "DATA_PAGE_V1":
		fallthrough
	case "DATA_PAGE_V2":
		*e = DataPageVersionGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataPageVersionGoogleCloudStorage: %v", v)
	}
}

type KeyValueMetadatumGoogleCloudStorage struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumGoogleCloudStorage) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *KeyValueMetadatumGoogleCloudStorage) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *KeyValueMetadatumGoogleCloudStorage) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputGoogleCloudStorage struct {
	// Unique ID for this output
	ID   string                  `json:"id"`
	Type *TypeGoogleCloudStorage `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a Global Variable: `myBucket-${C.vars.myVar}`.
	Bucket string `json:"bucket"`
	// Region where the bucket is located
	Region string `json:"region"`
	// Google Cloud Storage service endpoint
	Endpoint *string `default:"https://storage.googleapis.com" json:"endpoint"`
	// Signature version to use for signing Google Cloud Storage requests
	SignatureVersion        *SignatureVersionGoogleCloudStorage     `default:"v4" json:"signatureVersion"`
	AwsAuthenticationMethod *AuthenticationMethodGoogleCloudStorage `default:"manual" json:"awsAuthenticationMethod"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Prefix to append to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`
	DestPath *string `default:"" json:"destPath"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLGoogleCloudStorage `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass *StorageClassGoogleCloudStorage `json:"storageClass,omitempty"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value  if present  otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatGoogleCloudStorage `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGoogleCloudStorage `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionGoogleCloudStorage `default:"block" json:"onDiskFullBackpressure"`
	Description            *string                                `json:"description,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressionGoogleCloudStorage `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelGoogleCloudStorage `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionGoogleCloudStorage `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionGoogleCloudStorage `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumGoogleCloudStorage `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
	// HMAC access key. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// HMAC secret. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
}

func (o OutputGoogleCloudStorage) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputGoogleCloudStorage) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputGoogleCloudStorage) GetType() *TypeGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputGoogleCloudStorage) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGoogleCloudStorage) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGoogleCloudStorage) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGoogleCloudStorage) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGoogleCloudStorage) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputGoogleCloudStorage) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputGoogleCloudStorage) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputGoogleCloudStorage) GetSignatureVersion() *SignatureVersionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputGoogleCloudStorage) GetAwsAuthenticationMethod() *AuthenticationMethodGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputGoogleCloudStorage) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputGoogleCloudStorage) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputGoogleCloudStorage) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputGoogleCloudStorage) GetObjectACL() *ObjectACLGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputGoogleCloudStorage) GetStorageClass() *StorageClassGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputGoogleCloudStorage) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputGoogleCloudStorage) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputGoogleCloudStorage) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputGoogleCloudStorage) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputGoogleCloudStorage) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputGoogleCloudStorage) GetFormat() *DataFormatGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputGoogleCloudStorage) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputGoogleCloudStorage) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputGoogleCloudStorage) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputGoogleCloudStorage) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputGoogleCloudStorage) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputGoogleCloudStorage) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputGoogleCloudStorage) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputGoogleCloudStorage) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputGoogleCloudStorage) GetOnBackpressure() *BackpressureBehaviorGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGoogleCloudStorage) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputGoogleCloudStorage) GetOnDiskFullBackpressure() *DiskSpaceProtectionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputGoogleCloudStorage) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGoogleCloudStorage) GetCompress() *CompressionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputGoogleCloudStorage) GetCompressionLevel() *CompressionLevelGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputGoogleCloudStorage) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputGoogleCloudStorage) GetParquetVersion() *ParquetVersionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputGoogleCloudStorage) GetParquetDataPageVersion() *DataPageVersionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputGoogleCloudStorage) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputGoogleCloudStorage) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputGoogleCloudStorage) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputGoogleCloudStorage) GetKeyValueMetadata() []KeyValueMetadatumGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputGoogleCloudStorage) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputGoogleCloudStorage) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputGoogleCloudStorage) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputGoogleCloudStorage) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputGoogleCloudStorage) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputGoogleCloudStorage) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputGoogleCloudStorage) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputGoogleCloudStorage) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputGoogleCloudStorage) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

type TypeGoogleChronicle string

const (
	TypeGoogleChronicleGoogleChronicle TypeGoogleChronicle = "google_chronicle"
)

func (e TypeGoogleChronicle) ToPointer() *TypeGoogleChronicle {
	return &e
}
func (e *TypeGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_chronicle":
		*e = TypeGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGoogleChronicle: %v", v)
	}
}

type CreateOutputAPIVersion string

const (
	CreateOutputAPIVersionV1 CreateOutputAPIVersion = "v1"
	CreateOutputAPIVersionV2 CreateOutputAPIVersion = "v2"
)

func (e CreateOutputAPIVersion) ToPointer() *CreateOutputAPIVersion {
	return &e
}
func (e *CreateOutputAPIVersion) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v1":
		fallthrough
	case "v2":
		*e = CreateOutputAPIVersion(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputAPIVersion: %v", v)
	}
}

type AuthenticationMethodGoogleChronicle string

const (
	AuthenticationMethodGoogleChronicleManual               AuthenticationMethodGoogleChronicle = "manual"
	AuthenticationMethodGoogleChronicleSecret               AuthenticationMethodGoogleChronicle = "secret"
	AuthenticationMethodGoogleChronicleServiceAccount       AuthenticationMethodGoogleChronicle = "serviceAccount"
	AuthenticationMethodGoogleChronicleServiceAccountSecret AuthenticationMethodGoogleChronicle = "serviceAccountSecret"
)

func (e AuthenticationMethodGoogleChronicle) ToPointer() *AuthenticationMethodGoogleChronicle {
	return &e
}
func (e *AuthenticationMethodGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		fallthrough
	case "serviceAccount":
		fallthrough
	case "serviceAccountSecret":
		*e = AuthenticationMethodGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodGoogleChronicle: %v", v)
	}
}

type ResponseRetrySettingGoogleChronicle struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingGoogleChronicle) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingGoogleChronicle) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingGoogleChronicle) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingGoogleChronicle) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsGoogleChronicle struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsGoogleChronicle) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsGoogleChronicle) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsGoogleChronicle) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsGoogleChronicle) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type SendEventsAs string

const (
	SendEventsAsUnstructured SendEventsAs = "unstructured"
	SendEventsAsUdm          SendEventsAs = "udm"
)

func (e SendEventsAs) ToPointer() *SendEventsAs {
	return &e
}
func (e *SendEventsAs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "unstructured":
		fallthrough
	case "udm":
		*e = SendEventsAs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SendEventsAs: %v", v)
	}
}

type ExtraHTTPHeaderGoogleChronicle struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderGoogleChronicle) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderGoogleChronicle) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeGoogleChronicle - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeGoogleChronicle string

const (
	FailedRequestLoggingModeGoogleChroniclePayload           FailedRequestLoggingModeGoogleChronicle = "payload"
	FailedRequestLoggingModeGoogleChroniclePayloadAndHeaders FailedRequestLoggingModeGoogleChronicle = "payloadAndHeaders"
	FailedRequestLoggingModeGoogleChronicleNone              FailedRequestLoggingModeGoogleChronicle = "none"
)

func (e FailedRequestLoggingModeGoogleChronicle) ToPointer() *FailedRequestLoggingModeGoogleChronicle {
	return &e
}
func (e *FailedRequestLoggingModeGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeGoogleChronicle: %v", v)
	}
}

// BackpressureBehaviorGoogleChronicle - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorGoogleChronicle string

const (
	BackpressureBehaviorGoogleChronicleBlock BackpressureBehaviorGoogleChronicle = "block"
	BackpressureBehaviorGoogleChronicleDrop  BackpressureBehaviorGoogleChronicle = "drop"
	BackpressureBehaviorGoogleChronicleQueue BackpressureBehaviorGoogleChronicle = "queue"
)

func (e BackpressureBehaviorGoogleChronicle) ToPointer() *BackpressureBehaviorGoogleChronicle {
	return &e
}
func (e *BackpressureBehaviorGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorGoogleChronicle: %v", v)
	}
}

type ExtraLogType struct {
	LogType     string  `json:"logType"`
	Description *string `json:"description,omitempty"`
}

func (o *ExtraLogType) GetLogType() string {
	if o == nil {
		return ""
	}
	return o.LogType
}

func (o *ExtraLogType) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

type CustomLabel struct {
	Key   string `json:"key"`
	Value string `json:"value"`
}

func (o *CustomLabel) GetKey() string {
	if o == nil {
		return ""
	}
	return o.Key
}

func (o *CustomLabel) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// CompressionGoogleChronicle - Codec to use to compress the persisted data
type CompressionGoogleChronicle string

const (
	CompressionGoogleChronicleNone CompressionGoogleChronicle = "none"
	CompressionGoogleChronicleGzip CompressionGoogleChronicle = "gzip"
)

func (e CompressionGoogleChronicle) ToPointer() *CompressionGoogleChronicle {
	return &e
}
func (e *CompressionGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionGoogleChronicle: %v", v)
	}
}

// QueueFullBehaviorGoogleChronicle - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGoogleChronicle string

const (
	QueueFullBehaviorGoogleChronicleBlock QueueFullBehaviorGoogleChronicle = "block"
	QueueFullBehaviorGoogleChronicleDrop  QueueFullBehaviorGoogleChronicle = "drop"
)

func (e QueueFullBehaviorGoogleChronicle) ToPointer() *QueueFullBehaviorGoogleChronicle {
	return &e
}
func (e *QueueFullBehaviorGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorGoogleChronicle: %v", v)
	}
}

// ModeGoogleChronicle - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeGoogleChronicle string

const (
	ModeGoogleChronicleError        ModeGoogleChronicle = "error"
	ModeGoogleChronicleBackpressure ModeGoogleChronicle = "backpressure"
	ModeGoogleChronicleAlways       ModeGoogleChronicle = "always"
)

func (e ModeGoogleChronicle) ToPointer() *ModeGoogleChronicle {
	return &e
}
func (e *ModeGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeGoogleChronicle: %v", v)
	}
}

type PqControlsGoogleChronicle struct {
}

type OutputGoogleChronicle struct {
	// Unique ID for this output
	ID   string              `json:"id"`
	Type TypeGoogleChronicle `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags           []string                             `json:"streamtags,omitempty"`
	APIVersion           *CreateOutputAPIVersion              `default:"v1" json:"apiVersion"`
	AuthenticationMethod *AuthenticationMethodGoogleChronicle `default:"serviceAccount" json:"authenticationMethod"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingGoogleChronicle `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsGoogleChronicle  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool         `default:"false" json:"responseHonorRetryAfterHeader"`
	LogFormatType                 *SendEventsAs `default:"unstructured" json:"logFormatType"`
	// Regional endpoint to send events to
	Region *string `json:"region,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"90" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderGoogleChronicle `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeGoogleChronicle `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGoogleChronicle `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// Custom log types. If the value "Custom" is selected in the setting "Default log type" above, the first custom log type in this table will be automatically selected as default log type.
	ExtraLogTypes []ExtraLogType `json:"extraLogTypes,omitempty"`
	// Default log type value to send to SecOps. Can be overwritten by event field __logType.
	LogType *string `json:"logType,omitempty"`
	// Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.
	LogTextField *string `json:"logTextField,omitempty"`
	// Unique identifier (UUID) corresponding to a particular SecOps instance. Provided by your SecOps representative.
	CustomerID *string `json:"customerId,omitempty"`
	// User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.
	Namespace *string `json:"namespace,omitempty"`
	// Custom labels to be added to every batch
	CustomLabels []CustomLabel `json:"customLabels,omitempty"`
	// Organization's API key in Google SecOps
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	APIKeySecret *string `json:"apiKeySecret,omitempty"`
	// Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
	ServiceAccountCredentials *string `json:"serviceAccountCredentials,omitempty"`
	// Select or create a stored text secret
	ServiceAccountCredentialsSecret *string `json:"serviceAccountCredentialsSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionGoogleChronicle `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorGoogleChronicle `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeGoogleChronicle       `default:"error" json:"pqMode"`
	PqControls *PqControlsGoogleChronicle `json:"pqControls,omitempty"`
}

func (o OutputGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputGoogleChronicle) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputGoogleChronicle) GetType() TypeGoogleChronicle {
	if o == nil {
		return TypeGoogleChronicle("")
	}
	return o.Type
}

func (o *OutputGoogleChronicle) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGoogleChronicle) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGoogleChronicle) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGoogleChronicle) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGoogleChronicle) GetAPIVersion() *CreateOutputAPIVersion {
	if o == nil {
		return nil
	}
	return o.APIVersion
}

func (o *OutputGoogleChronicle) GetAuthenticationMethod() *AuthenticationMethodGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.AuthenticationMethod
}

func (o *OutputGoogleChronicle) GetResponseRetrySettings() []ResponseRetrySettingGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputGoogleChronicle) GetTimeoutRetrySettings() *TimeoutRetrySettingsGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputGoogleChronicle) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputGoogleChronicle) GetLogFormatType() *SendEventsAs {
	if o == nil {
		return nil
	}
	return o.LogFormatType
}

func (o *OutputGoogleChronicle) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputGoogleChronicle) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputGoogleChronicle) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputGoogleChronicle) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputGoogleChronicle) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputGoogleChronicle) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputGoogleChronicle) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputGoogleChronicle) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGoogleChronicle) GetExtraHTTPHeaders() []ExtraHTTPHeaderGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputGoogleChronicle) GetFailedRequestLoggingMode() *FailedRequestLoggingModeGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputGoogleChronicle) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputGoogleChronicle) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputGoogleChronicle) GetOnBackpressure() *BackpressureBehaviorGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGoogleChronicle) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputGoogleChronicle) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGoogleChronicle) GetExtraLogTypes() []ExtraLogType {
	if o == nil {
		return nil
	}
	return o.ExtraLogTypes
}

func (o *OutputGoogleChronicle) GetLogType() *string {
	if o == nil {
		return nil
	}
	return o.LogType
}

func (o *OutputGoogleChronicle) GetLogTextField() *string {
	if o == nil {
		return nil
	}
	return o.LogTextField
}

func (o *OutputGoogleChronicle) GetCustomerID() *string {
	if o == nil {
		return nil
	}
	return o.CustomerID
}

func (o *OutputGoogleChronicle) GetNamespace() *string {
	if o == nil {
		return nil
	}
	return o.Namespace
}

func (o *OutputGoogleChronicle) GetCustomLabels() []CustomLabel {
	if o == nil {
		return nil
	}
	return o.CustomLabels
}

func (o *OutputGoogleChronicle) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputGoogleChronicle) GetAPIKeySecret() *string {
	if o == nil {
		return nil
	}
	return o.APIKeySecret
}

func (o *OutputGoogleChronicle) GetServiceAccountCredentials() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentials
}

func (o *OutputGoogleChronicle) GetServiceAccountCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentialsSecret
}

func (o *OutputGoogleChronicle) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGoogleChronicle) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGoogleChronicle) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGoogleChronicle) GetPqCompress() *CompressionGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGoogleChronicle) GetPqOnBackpressure() *QueueFullBehaviorGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGoogleChronicle) GetPqMode() *ModeGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGoogleChronicle) GetPqControls() *PqControlsGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeAzureEventhub string

const (
	TypeAzureEventhubAzureEventhub TypeAzureEventhub = "azure_eventhub"
)

func (e TypeAzureEventhub) ToPointer() *TypeAzureEventhub {
	return &e
}
func (e *TypeAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_eventhub":
		*e = TypeAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeAzureEventhub: %v", v)
	}
}

// AcknowledgmentsAzureEventhub - Control the number of required acknowledgments
type AcknowledgmentsAzureEventhub int64

const (
	AcknowledgmentsAzureEventhubOne    AcknowledgmentsAzureEventhub = 1
	AcknowledgmentsAzureEventhubZero   AcknowledgmentsAzureEventhub = 0
	AcknowledgmentsAzureEventhubMinus1 AcknowledgmentsAzureEventhub = -1
)

func (e AcknowledgmentsAzureEventhub) ToPointer() *AcknowledgmentsAzureEventhub {
	return &e
}
func (e *AcknowledgmentsAzureEventhub) UnmarshalJSON(data []byte) error {
	var v int64
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case 1:
		fallthrough
	case 0:
		fallthrough
	case -1:
		*e = AcknowledgmentsAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AcknowledgmentsAzureEventhub: %v", v)
	}
}

// RecordDataFormatAzureEventhub - Format to use to serialize events before writing to the Event Hubs Kafka brokers
type RecordDataFormatAzureEventhub string

const (
	RecordDataFormatAzureEventhubJSON RecordDataFormatAzureEventhub = "json"
	RecordDataFormatAzureEventhubRaw  RecordDataFormatAzureEventhub = "raw"
)

func (e RecordDataFormatAzureEventhub) ToPointer() *RecordDataFormatAzureEventhub {
	return &e
}
func (e *RecordDataFormatAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		*e = RecordDataFormatAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RecordDataFormatAzureEventhub: %v", v)
	}
}

type SASLMechanismAzureEventhub string

const (
	SASLMechanismAzureEventhubPlain       SASLMechanismAzureEventhub = "plain"
	SASLMechanismAzureEventhubOauthbearer SASLMechanismAzureEventhub = "oauthbearer"
)

func (e SASLMechanismAzureEventhub) ToPointer() *SASLMechanismAzureEventhub {
	return &e
}
func (e *SASLMechanismAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "plain":
		fallthrough
	case "oauthbearer":
		*e = SASLMechanismAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SASLMechanismAzureEventhub: %v", v)
	}
}

// AuthenticationAzureEventhub - Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
type AuthenticationAzureEventhub struct {
	Disabled  *bool                       `default:"false" json:"disabled"`
	Mechanism *SASLMechanismAzureEventhub `default:"plain" json:"mechanism"`
}

func (a AuthenticationAzureEventhub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthenticationAzureEventhub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *AuthenticationAzureEventhub) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *AuthenticationAzureEventhub) GetMechanism() *SASLMechanismAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Mechanism
}

type TLSSettingsClientSideAzureEventhub struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
}

func (t TLSSettingsClientSideAzureEventhub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideAzureEventhub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSideAzureEventhub) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSideAzureEventhub) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

// BackpressureBehaviorAzureEventhub - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorAzureEventhub string

const (
	BackpressureBehaviorAzureEventhubBlock BackpressureBehaviorAzureEventhub = "block"
	BackpressureBehaviorAzureEventhubDrop  BackpressureBehaviorAzureEventhub = "drop"
	BackpressureBehaviorAzureEventhubQueue BackpressureBehaviorAzureEventhub = "queue"
)

func (e BackpressureBehaviorAzureEventhub) ToPointer() *BackpressureBehaviorAzureEventhub {
	return &e
}
func (e *BackpressureBehaviorAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorAzureEventhub: %v", v)
	}
}

// CompressionAzureEventhub - Codec to use to compress the persisted data
type CompressionAzureEventhub string

const (
	CompressionAzureEventhubNone CompressionAzureEventhub = "none"
	CompressionAzureEventhubGzip CompressionAzureEventhub = "gzip"
)

func (e CompressionAzureEventhub) ToPointer() *CompressionAzureEventhub {
	return &e
}
func (e *CompressionAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionAzureEventhub: %v", v)
	}
}

// QueueFullBehaviorAzureEventhub - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorAzureEventhub string

const (
	QueueFullBehaviorAzureEventhubBlock QueueFullBehaviorAzureEventhub = "block"
	QueueFullBehaviorAzureEventhubDrop  QueueFullBehaviorAzureEventhub = "drop"
)

func (e QueueFullBehaviorAzureEventhub) ToPointer() *QueueFullBehaviorAzureEventhub {
	return &e
}
func (e *QueueFullBehaviorAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorAzureEventhub: %v", v)
	}
}

// ModeAzureEventhub - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeAzureEventhub string

const (
	ModeAzureEventhubError        ModeAzureEventhub = "error"
	ModeAzureEventhubBackpressure ModeAzureEventhub = "backpressure"
	ModeAzureEventhubAlways       ModeAzureEventhub = "always"
)

func (e ModeAzureEventhub) ToPointer() *ModeAzureEventhub {
	return &e
}
func (e *ModeAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeAzureEventhub: %v", v)
	}
}

type PqControlsAzureEventhub struct {
}

type OutputAzureEventhub struct {
	// Unique ID for this output
	ID   string             `json:"id"`
	Type *TypeAzureEventhub `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// List of Event Hubs Kafka brokers to connect to, eg. yourdomain.servicebus.windows.net:9093. The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.
	Brokers []string `json:"brokers"`
	// The name of the Event Hub (Kafka Topic) to publish events. Can be overwritten using field __topicOut.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments
	Ack *AcknowledgmentsAzureEventhub `default:"1" json:"ack"`
	// Format to use to serialize events before writing to the Event Hubs Kafka brokers
	Format *RecordDataFormatAzureEventhub `default:"json" json:"format"`
	// Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// Maximum number of events in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
	Sasl *AuthenticationAzureEventhub        `json:"sasl,omitempty"`
	TLS  *TLSSettingsClientSideAzureEventhub `json:"tls,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorAzureEventhub `default:"block" json:"onBackpressure"`
	Description    *string                            `json:"description,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionAzureEventhub `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorAzureEventhub `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeAzureEventhub       `default:"error" json:"pqMode"`
	PqControls *PqControlsAzureEventhub `json:"pqControls,omitempty"`
}

func (o OutputAzureEventhub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureEventhub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureEventhub) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputAzureEventhub) GetType() *TypeAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputAzureEventhub) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureEventhub) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureEventhub) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureEventhub) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureEventhub) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputAzureEventhub) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputAzureEventhub) GetAck() *AcknowledgmentsAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputAzureEventhub) GetFormat() *RecordDataFormatAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputAzureEventhub) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputAzureEventhub) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputAzureEventhub) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputAzureEventhub) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputAzureEventhub) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputAzureEventhub) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputAzureEventhub) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputAzureEventhub) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputAzureEventhub) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputAzureEventhub) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputAzureEventhub) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputAzureEventhub) GetSasl() *AuthenticationAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Sasl
}

func (o *OutputAzureEventhub) GetTLS() *TLSSettingsClientSideAzureEventhub {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputAzureEventhub) GetOnBackpressure() *BackpressureBehaviorAzureEventhub {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureEventhub) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureEventhub) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputAzureEventhub) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputAzureEventhub) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputAzureEventhub) GetPqCompress() *CompressionAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputAzureEventhub) GetPqOnBackpressure() *QueueFullBehaviorAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputAzureEventhub) GetPqMode() *ModeAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputAzureEventhub) GetPqControls() *PqControlsAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeHoneycomb string

const (
	TypeHoneycombHoneycomb TypeHoneycomb = "honeycomb"
)

func (e TypeHoneycomb) ToPointer() *TypeHoneycomb {
	return &e
}
func (e *TypeHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "honeycomb":
		*e = TypeHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeHoneycomb: %v", v)
	}
}

type ExtraHTTPHeaderHoneycomb struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderHoneycomb) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderHoneycomb) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeHoneycomb - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeHoneycomb string

const (
	FailedRequestLoggingModeHoneycombPayload           FailedRequestLoggingModeHoneycomb = "payload"
	FailedRequestLoggingModeHoneycombPayloadAndHeaders FailedRequestLoggingModeHoneycomb = "payloadAndHeaders"
	FailedRequestLoggingModeHoneycombNone              FailedRequestLoggingModeHoneycomb = "none"
)

func (e FailedRequestLoggingModeHoneycomb) ToPointer() *FailedRequestLoggingModeHoneycomb {
	return &e
}
func (e *FailedRequestLoggingModeHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeHoneycomb: %v", v)
	}
}

type ResponseRetrySettingHoneycomb struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingHoneycomb) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingHoneycomb) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingHoneycomb) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingHoneycomb) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsHoneycomb struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsHoneycomb) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsHoneycomb) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsHoneycomb) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsHoneycomb) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorHoneycomb - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorHoneycomb string

const (
	BackpressureBehaviorHoneycombBlock BackpressureBehaviorHoneycomb = "block"
	BackpressureBehaviorHoneycombDrop  BackpressureBehaviorHoneycomb = "drop"
	BackpressureBehaviorHoneycombQueue BackpressureBehaviorHoneycomb = "queue"
)

func (e BackpressureBehaviorHoneycomb) ToPointer() *BackpressureBehaviorHoneycomb {
	return &e
}
func (e *BackpressureBehaviorHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorHoneycomb: %v", v)
	}
}

// AuthenticationMethodHoneycomb - Enter API key directly, or select a stored secret
type AuthenticationMethodHoneycomb string

const (
	AuthenticationMethodHoneycombManual AuthenticationMethodHoneycomb = "manual"
	AuthenticationMethodHoneycombSecret AuthenticationMethodHoneycomb = "secret"
)

func (e AuthenticationMethodHoneycomb) ToPointer() *AuthenticationMethodHoneycomb {
	return &e
}
func (e *AuthenticationMethodHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodHoneycomb: %v", v)
	}
}

// CompressionHoneycomb - Codec to use to compress the persisted data
type CompressionHoneycomb string

const (
	CompressionHoneycombNone CompressionHoneycomb = "none"
	CompressionHoneycombGzip CompressionHoneycomb = "gzip"
)

func (e CompressionHoneycomb) ToPointer() *CompressionHoneycomb {
	return &e
}
func (e *CompressionHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionHoneycomb: %v", v)
	}
}

// QueueFullBehaviorHoneycomb - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorHoneycomb string

const (
	QueueFullBehaviorHoneycombBlock QueueFullBehaviorHoneycomb = "block"
	QueueFullBehaviorHoneycombDrop  QueueFullBehaviorHoneycomb = "drop"
)

func (e QueueFullBehaviorHoneycomb) ToPointer() *QueueFullBehaviorHoneycomb {
	return &e
}
func (e *QueueFullBehaviorHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorHoneycomb: %v", v)
	}
}

// ModeHoneycomb - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeHoneycomb string

const (
	ModeHoneycombError        ModeHoneycomb = "error"
	ModeHoneycombBackpressure ModeHoneycomb = "backpressure"
	ModeHoneycombAlways       ModeHoneycomb = "always"
)

func (e ModeHoneycomb) ToPointer() *ModeHoneycomb {
	return &e
}
func (e *ModeHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeHoneycomb: %v", v)
	}
}

type PqControlsHoneycomb struct {
}

type OutputHoneycomb struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type TypeHoneycomb `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the dataset to send events to  e.g., observability
	Dataset string `json:"dataset"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderHoneycomb `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeHoneycomb `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingHoneycomb `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsHoneycomb  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorHoneycomb `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType    *AuthenticationMethodHoneycomb `default:"manual" json:"authType"`
	Description *string                        `json:"description,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionHoneycomb `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorHoneycomb `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeHoneycomb       `default:"error" json:"pqMode"`
	PqControls *PqControlsHoneycomb `json:"pqControls,omitempty"`
	// Team API key where the dataset belongs
	Team *string `json:"team,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (o OutputHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputHoneycomb) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputHoneycomb) GetType() TypeHoneycomb {
	if o == nil {
		return TypeHoneycomb("")
	}
	return o.Type
}

func (o *OutputHoneycomb) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputHoneycomb) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputHoneycomb) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputHoneycomb) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputHoneycomb) GetDataset() string {
	if o == nil {
		return ""
	}
	return o.Dataset
}

func (o *OutputHoneycomb) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputHoneycomb) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputHoneycomb) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputHoneycomb) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputHoneycomb) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputHoneycomb) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputHoneycomb) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputHoneycomb) GetExtraHTTPHeaders() []ExtraHTTPHeaderHoneycomb {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputHoneycomb) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputHoneycomb) GetFailedRequestLoggingMode() *FailedRequestLoggingModeHoneycomb {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputHoneycomb) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputHoneycomb) GetResponseRetrySettings() []ResponseRetrySettingHoneycomb {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputHoneycomb) GetTimeoutRetrySettings() *TimeoutRetrySettingsHoneycomb {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputHoneycomb) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputHoneycomb) GetOnBackpressure() *BackpressureBehaviorHoneycomb {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputHoneycomb) GetAuthType() *AuthenticationMethodHoneycomb {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputHoneycomb) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputHoneycomb) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputHoneycomb) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputHoneycomb) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputHoneycomb) GetPqCompress() *CompressionHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputHoneycomb) GetPqOnBackpressure() *QueueFullBehaviorHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputHoneycomb) GetPqMode() *ModeHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputHoneycomb) GetPqControls() *PqControlsHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputHoneycomb) GetTeam() *string {
	if o == nil {
		return nil
	}
	return o.Team
}

func (o *OutputHoneycomb) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

type CreateOutputTypeKinesis string

const (
	CreateOutputTypeKinesisKinesis CreateOutputTypeKinesis = "kinesis"
)

func (e CreateOutputTypeKinesis) ToPointer() *CreateOutputTypeKinesis {
	return &e
}
func (e *CreateOutputTypeKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "kinesis":
		*e = CreateOutputTypeKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeKinesis: %v", v)
	}
}

// CreateOutputAuthenticationMethodKinesis - AWS authentication method. Choose Auto to use IAM roles.
type CreateOutputAuthenticationMethodKinesis string

const (
	CreateOutputAuthenticationMethodKinesisAuto   CreateOutputAuthenticationMethodKinesis = "auto"
	CreateOutputAuthenticationMethodKinesisManual CreateOutputAuthenticationMethodKinesis = "manual"
	CreateOutputAuthenticationMethodKinesisSecret CreateOutputAuthenticationMethodKinesis = "secret"
)

func (e CreateOutputAuthenticationMethodKinesis) ToPointer() *CreateOutputAuthenticationMethodKinesis {
	return &e
}
func (e *CreateOutputAuthenticationMethodKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = CreateOutputAuthenticationMethodKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputAuthenticationMethodKinesis: %v", v)
	}
}

// CreateOutputSignatureVersionKinesis - Signature version to use for signing Kinesis stream requests
type CreateOutputSignatureVersionKinesis string

const (
	CreateOutputSignatureVersionKinesisV2 CreateOutputSignatureVersionKinesis = "v2"
	CreateOutputSignatureVersionKinesisV4 CreateOutputSignatureVersionKinesis = "v4"
)

func (e CreateOutputSignatureVersionKinesis) ToPointer() *CreateOutputSignatureVersionKinesis {
	return &e
}
func (e *CreateOutputSignatureVersionKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = CreateOutputSignatureVersionKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputSignatureVersionKinesis: %v", v)
	}
}

// CreateOutputCompressionKinesis - Compression type to use for records
type CreateOutputCompressionKinesis string

const (
	CreateOutputCompressionKinesisNone CreateOutputCompressionKinesis = "none"
	CreateOutputCompressionKinesisGzip CreateOutputCompressionKinesis = "gzip"
)

func (e CreateOutputCompressionKinesis) ToPointer() *CreateOutputCompressionKinesis {
	return &e
}
func (e *CreateOutputCompressionKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CreateOutputCompressionKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputCompressionKinesis: %v", v)
	}
}

// BackpressureBehaviorKinesis - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorKinesis string

const (
	BackpressureBehaviorKinesisBlock BackpressureBehaviorKinesis = "block"
	BackpressureBehaviorKinesisDrop  BackpressureBehaviorKinesis = "drop"
	BackpressureBehaviorKinesisQueue BackpressureBehaviorKinesis = "queue"
)

func (e BackpressureBehaviorKinesis) ToPointer() *BackpressureBehaviorKinesis {
	return &e
}
func (e *BackpressureBehaviorKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorKinesis: %v", v)
	}
}

// PqCompressCompressionKinesis - Codec to use to compress the persisted data
type PqCompressCompressionKinesis string

const (
	PqCompressCompressionKinesisNone PqCompressCompressionKinesis = "none"
	PqCompressCompressionKinesisGzip PqCompressCompressionKinesis = "gzip"
)

func (e PqCompressCompressionKinesis) ToPointer() *PqCompressCompressionKinesis {
	return &e
}
func (e *PqCompressCompressionKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionKinesis: %v", v)
	}
}

// QueueFullBehaviorKinesis - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorKinesis string

const (
	QueueFullBehaviorKinesisBlock QueueFullBehaviorKinesis = "block"
	QueueFullBehaviorKinesisDrop  QueueFullBehaviorKinesis = "drop"
)

func (e QueueFullBehaviorKinesis) ToPointer() *QueueFullBehaviorKinesis {
	return &e
}
func (e *QueueFullBehaviorKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorKinesis: %v", v)
	}
}

// CreateOutputModeKinesis - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeKinesis string

const (
	CreateOutputModeKinesisError        CreateOutputModeKinesis = "error"
	CreateOutputModeKinesisBackpressure CreateOutputModeKinesis = "backpressure"
	CreateOutputModeKinesisAlways       CreateOutputModeKinesis = "always"
)

func (e CreateOutputModeKinesis) ToPointer() *CreateOutputModeKinesis {
	return &e
}
func (e *CreateOutputModeKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = CreateOutputModeKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputModeKinesis: %v", v)
	}
}

type PqControlsKinesis struct {
}

type OutputKinesis struct {
	// Unique ID for this output
	ID   string                   `json:"id"`
	Type *CreateOutputTypeKinesis `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Kinesis stream name to send events to.
	StreamName string `json:"streamName"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *CreateOutputAuthenticationMethodKinesis `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                                  `json:"awsSecretKey,omitempty"`
	// Region where the Kinesis stream is located
	Region string `json:"region"`
	// Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing Kinesis stream requests
	SignatureVersion *CreateOutputSignatureVersionKinesis `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access Kinesis stream
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Maximum number of ongoing put requests before blocking.
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size (KB) of each individual record before compression. For uncompressed or non-compressible data 1MB is the max recommended size
	MaxRecordSizeKB *float64 `default:"1024" json:"maxRecordSizeKB"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Compression type to use for records
	Compression *CreateOutputCompressionKinesis `default:"gzip" json:"compression"`
	// Provides higher stream rate limits, improving delivery speed and reliability by minimizing throttling. See the [ListShards API](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html) documentation for details.
	UseListShards *bool `default:"false" json:"useListShards"`
	// Batch events into a single record as NDJSON
	AsNdjson *bool `default:"true" json:"asNdjson"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorKinesis `default:"block" json:"onBackpressure"`
	Description    *string                      `json:"description,omitempty"`
	AwsAPIKey      *string                      `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionKinesis `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorKinesis `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *CreateOutputModeKinesis `default:"error" json:"pqMode"`
	PqControls *PqControlsKinesis       `json:"pqControls,omitempty"`
}

func (o OutputKinesis) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKinesis) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputKinesis) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputKinesis) GetType() *CreateOutputTypeKinesis {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputKinesis) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputKinesis) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputKinesis) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputKinesis) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputKinesis) GetStreamName() string {
	if o == nil {
		return ""
	}
	return o.StreamName
}

func (o *OutputKinesis) GetAwsAuthenticationMethod() *CreateOutputAuthenticationMethodKinesis {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputKinesis) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputKinesis) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputKinesis) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputKinesis) GetSignatureVersion() *CreateOutputSignatureVersionKinesis {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputKinesis) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputKinesis) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputKinesis) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputKinesis) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputKinesis) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputKinesis) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputKinesis) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputKinesis) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputKinesis) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputKinesis) GetCompression() *CreateOutputCompressionKinesis {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputKinesis) GetUseListShards() *bool {
	if o == nil {
		return nil
	}
	return o.UseListShards
}

func (o *OutputKinesis) GetAsNdjson() *bool {
	if o == nil {
		return nil
	}
	return o.AsNdjson
}

func (o *OutputKinesis) GetOnBackpressure() *BackpressureBehaviorKinesis {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputKinesis) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputKinesis) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputKinesis) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputKinesis) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputKinesis) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputKinesis) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputKinesis) GetPqCompress() *PqCompressCompressionKinesis {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputKinesis) GetPqOnBackpressure() *QueueFullBehaviorKinesis {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputKinesis) GetPqMode() *CreateOutputModeKinesis {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputKinesis) GetPqControls() *PqControlsKinesis {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeAzureLogs string

const (
	TypeAzureLogsAzureLogs TypeAzureLogs = "azure_logs"
)

func (e TypeAzureLogs) ToPointer() *TypeAzureLogs {
	return &e
}
func (e *TypeAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_logs":
		*e = TypeAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeAzureLogs: %v", v)
	}
}

type ExtraHTTPHeaderAzureLogs struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderAzureLogs) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderAzureLogs) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeAzureLogs - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeAzureLogs string

const (
	FailedRequestLoggingModeAzureLogsPayload           FailedRequestLoggingModeAzureLogs = "payload"
	FailedRequestLoggingModeAzureLogsPayloadAndHeaders FailedRequestLoggingModeAzureLogs = "payloadAndHeaders"
	FailedRequestLoggingModeAzureLogsNone              FailedRequestLoggingModeAzureLogs = "none"
)

func (e FailedRequestLoggingModeAzureLogs) ToPointer() *FailedRequestLoggingModeAzureLogs {
	return &e
}
func (e *FailedRequestLoggingModeAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeAzureLogs: %v", v)
	}
}

type ResponseRetrySettingAzureLogs struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingAzureLogs) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingAzureLogs) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingAzureLogs) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingAzureLogs) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsAzureLogs struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsAzureLogs) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsAzureLogs) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsAzureLogs) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsAzureLogs) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorAzureLogs - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorAzureLogs string

const (
	BackpressureBehaviorAzureLogsBlock BackpressureBehaviorAzureLogs = "block"
	BackpressureBehaviorAzureLogsDrop  BackpressureBehaviorAzureLogs = "drop"
	BackpressureBehaviorAzureLogsQueue BackpressureBehaviorAzureLogs = "queue"
)

func (e BackpressureBehaviorAzureLogs) ToPointer() *BackpressureBehaviorAzureLogs {
	return &e
}
func (e *BackpressureBehaviorAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorAzureLogs: %v", v)
	}
}

// AuthenticationMethodAzureLogs - Enter workspace ID and workspace key directly, or select a stored secret
type AuthenticationMethodAzureLogs string

const (
	AuthenticationMethodAzureLogsManual AuthenticationMethodAzureLogs = "manual"
	AuthenticationMethodAzureLogsSecret AuthenticationMethodAzureLogs = "secret"
)

func (e AuthenticationMethodAzureLogs) ToPointer() *AuthenticationMethodAzureLogs {
	return &e
}
func (e *AuthenticationMethodAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodAzureLogs: %v", v)
	}
}

// CompressionAzureLogs - Codec to use to compress the persisted data
type CompressionAzureLogs string

const (
	CompressionAzureLogsNone CompressionAzureLogs = "none"
	CompressionAzureLogsGzip CompressionAzureLogs = "gzip"
)

func (e CompressionAzureLogs) ToPointer() *CompressionAzureLogs {
	return &e
}
func (e *CompressionAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionAzureLogs: %v", v)
	}
}

// QueueFullBehaviorAzureLogs - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorAzureLogs string

const (
	QueueFullBehaviorAzureLogsBlock QueueFullBehaviorAzureLogs = "block"
	QueueFullBehaviorAzureLogsDrop  QueueFullBehaviorAzureLogs = "drop"
)

func (e QueueFullBehaviorAzureLogs) ToPointer() *QueueFullBehaviorAzureLogs {
	return &e
}
func (e *QueueFullBehaviorAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorAzureLogs: %v", v)
	}
}

// ModeAzureLogs - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeAzureLogs string

const (
	ModeAzureLogsError        ModeAzureLogs = "error"
	ModeAzureLogsBackpressure ModeAzureLogs = "backpressure"
	ModeAzureLogsAlways       ModeAzureLogs = "always"
)

func (e ModeAzureLogs) ToPointer() *ModeAzureLogs {
	return &e
}
func (e *ModeAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeAzureLogs: %v", v)
	}
}

type PqControlsAzureLogs struct {
}

type OutputAzureLogs struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type TypeAzureLogs `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The Log Type of events sent to this LogAnalytics workspace. Defaults to `Cribl`. Use only letters, numbers, and `_` characters, and can't exceed 100 characters. Can be overwritten by event field __logType.
	LogType *string `default:"Cribl" json:"logType"`
	// Optional Resource ID of the Azure resource to associate the data with. Can be overridden by the __resourceId event field. This ID populates the _ResourceId property, allowing the data to be included in resource-centric queries. If the ID is neither specified nor overridden, resource-centric queries will omit the data.
	ResourceID *string `json:"resourceId,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	Compress         *bool    `json:"compress,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderAzureLogs `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeAzureLogs `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// The DNS name of the Log API endpoint that sends log data to a Log Analytics workspace in Azure Monitor. Defaults to .ods.opinsights.azure.com. @{product} will add a prefix and suffix to construct a URI in this format: <https://<Workspace_ID><your_DNS_name>/api/logs?api-version=<API version>.
	APIURL *string `default:".ods.opinsights.azure.com" json:"apiUrl"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingAzureLogs `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsAzureLogs  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorAzureLogs `default:"block" json:"onBackpressure"`
	// Enter workspace ID and workspace key directly, or select a stored secret
	AuthType    *AuthenticationMethodAzureLogs `default:"manual" json:"authType"`
	Description *string                        `json:"description,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionAzureLogs `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorAzureLogs `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeAzureLogs       `default:"error" json:"pqMode"`
	PqControls *PqControlsAzureLogs `json:"pqControls,omitempty"`
	// Azure Log Analytics Workspace ID. See Azure Dashboard Workspace > Advanced settings.
	WorkspaceID *string `json:"workspaceId,omitempty"`
	// Azure Log Analytics Workspace Primary or Secondary Shared Key. See Azure Dashboard Workspace > Advanced settings.
	WorkspaceKey *string `json:"workspaceKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	KeypairSecret *string `json:"keypairSecret,omitempty"`
}

func (o OutputAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureLogs) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputAzureLogs) GetType() TypeAzureLogs {
	if o == nil {
		return TypeAzureLogs("")
	}
	return o.Type
}

func (o *OutputAzureLogs) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureLogs) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureLogs) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureLogs) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureLogs) GetLogType() *string {
	if o == nil {
		return nil
	}
	return o.LogType
}

func (o *OutputAzureLogs) GetResourceID() *string {
	if o == nil {
		return nil
	}
	return o.ResourceID
}

func (o *OutputAzureLogs) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputAzureLogs) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputAzureLogs) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputAzureLogs) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputAzureLogs) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputAzureLogs) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputAzureLogs) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputAzureLogs) GetExtraHTTPHeaders() []ExtraHTTPHeaderAzureLogs {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputAzureLogs) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputAzureLogs) GetFailedRequestLoggingMode() *FailedRequestLoggingModeAzureLogs {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputAzureLogs) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputAzureLogs) GetAPIURL() *string {
	if o == nil {
		return nil
	}
	return o.APIURL
}

func (o *OutputAzureLogs) GetResponseRetrySettings() []ResponseRetrySettingAzureLogs {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputAzureLogs) GetTimeoutRetrySettings() *TimeoutRetrySettingsAzureLogs {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputAzureLogs) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputAzureLogs) GetOnBackpressure() *BackpressureBehaviorAzureLogs {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureLogs) GetAuthType() *AuthenticationMethodAzureLogs {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputAzureLogs) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureLogs) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputAzureLogs) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputAzureLogs) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputAzureLogs) GetPqCompress() *CompressionAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputAzureLogs) GetPqOnBackpressure() *QueueFullBehaviorAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputAzureLogs) GetPqMode() *ModeAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputAzureLogs) GetPqControls() *PqControlsAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputAzureLogs) GetWorkspaceID() *string {
	if o == nil {
		return nil
	}
	return o.WorkspaceID
}

func (o *OutputAzureLogs) GetWorkspaceKey() *string {
	if o == nil {
		return nil
	}
	return o.WorkspaceKey
}

func (o *OutputAzureLogs) GetKeypairSecret() *string {
	if o == nil {
		return nil
	}
	return o.KeypairSecret
}

type TypeAzureDataExplorer string

const (
	TypeAzureDataExplorerAzureDataExplorer TypeAzureDataExplorer = "azure_data_explorer"
)

func (e TypeAzureDataExplorer) ToPointer() *TypeAzureDataExplorer {
	return &e
}
func (e *TypeAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_data_explorer":
		*e = TypeAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeAzureDataExplorer: %v", v)
	}
}

type IngestionMode string

const (
	IngestionModeBatching  IngestionMode = "batching"
	IngestionModeStreaming IngestionMode = "streaming"
)

func (e IngestionMode) ToPointer() *IngestionMode {
	return &e
}
func (e *IngestionMode) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "batching":
		fallthrough
	case "streaming":
		*e = IngestionMode(v)
		return nil
	default:
		return fmt.Errorf("invalid value for IngestionMode: %v", v)
	}
}

// MicrosoftEntraIDAuthenticationEndpoint - Endpoint used to acquire authentication tokens from Azure
type MicrosoftEntraIDAuthenticationEndpoint string

const (
	MicrosoftEntraIDAuthenticationEndpointHTTPSLoginMicrosoftonlineCom       MicrosoftEntraIDAuthenticationEndpoint = "https://login.microsoftonline.com"
	MicrosoftEntraIDAuthenticationEndpointHTTPSLoginMicrosoftonlineUs        MicrosoftEntraIDAuthenticationEndpoint = "https://login.microsoftonline.us"
	MicrosoftEntraIDAuthenticationEndpointHTTPSLoginPartnerMicrosoftonlineCn MicrosoftEntraIDAuthenticationEndpoint = "https://login.partner.microsoftonline.cn"
)

func (e MicrosoftEntraIDAuthenticationEndpoint) ToPointer() *MicrosoftEntraIDAuthenticationEndpoint {
	return &e
}
func (e *MicrosoftEntraIDAuthenticationEndpoint) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "https://login.microsoftonline.com":
		fallthrough
	case "https://login.microsoftonline.us":
		fallthrough
	case "https://login.partner.microsoftonline.cn":
		*e = MicrosoftEntraIDAuthenticationEndpoint(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MicrosoftEntraIDAuthenticationEndpoint: %v", v)
	}
}

// OauthTypeAuthenticationMethod - The type of OAuth 2.0 client credentials grant flow to use
type OauthTypeAuthenticationMethod string

const (
	OauthTypeAuthenticationMethodClientSecret     OauthTypeAuthenticationMethod = "clientSecret"
	OauthTypeAuthenticationMethodClientTextSecret OauthTypeAuthenticationMethod = "clientTextSecret"
	OauthTypeAuthenticationMethodCertificate      OauthTypeAuthenticationMethod = "certificate"
)

func (e OauthTypeAuthenticationMethod) ToPointer() *OauthTypeAuthenticationMethod {
	return &e
}
func (e *OauthTypeAuthenticationMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "clientSecret":
		fallthrough
	case "clientTextSecret":
		fallthrough
	case "certificate":
		*e = OauthTypeAuthenticationMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OauthTypeAuthenticationMethod: %v", v)
	}
}

type CertificateAzureDataExplorer struct {
	// The certificate you registered as credentials for your app in the Azure portal
	CertificateName *string `json:"certificateName,omitempty"`
}

func (o *CertificateAzureDataExplorer) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

// BackpressureBehaviorAzureDataExplorer - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorAzureDataExplorer string

const (
	BackpressureBehaviorAzureDataExplorerBlock BackpressureBehaviorAzureDataExplorer = "block"
	BackpressureBehaviorAzureDataExplorerDrop  BackpressureBehaviorAzureDataExplorer = "drop"
	BackpressureBehaviorAzureDataExplorerQueue BackpressureBehaviorAzureDataExplorer = "queue"
)

func (e BackpressureBehaviorAzureDataExplorer) ToPointer() *BackpressureBehaviorAzureDataExplorer {
	return &e
}
func (e *BackpressureBehaviorAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorAzureDataExplorer: %v", v)
	}
}

// DataFormatAzureDataExplorer - Format of the output data
type DataFormatAzureDataExplorer string

const (
	DataFormatAzureDataExplorerJSON    DataFormatAzureDataExplorer = "json"
	DataFormatAzureDataExplorerRaw     DataFormatAzureDataExplorer = "raw"
	DataFormatAzureDataExplorerParquet DataFormatAzureDataExplorer = "parquet"
)

func (e DataFormatAzureDataExplorer) ToPointer() *DataFormatAzureDataExplorer {
	return &e
}
func (e *DataFormatAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "parquet":
		*e = DataFormatAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataFormatAzureDataExplorer: %v", v)
	}
}

// DiskSpaceProtectionAzureDataExplorer - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionAzureDataExplorer string

const (
	DiskSpaceProtectionAzureDataExplorerBlock DiskSpaceProtectionAzureDataExplorer = "block"
	DiskSpaceProtectionAzureDataExplorerDrop  DiskSpaceProtectionAzureDataExplorer = "drop"
)

func (e DiskSpaceProtectionAzureDataExplorer) ToPointer() *DiskSpaceProtectionAzureDataExplorer {
	return &e
}
func (e *DiskSpaceProtectionAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionAzureDataExplorer: %v", v)
	}
}

type PrefixOptional string

const (
	PrefixOptionalDropBy   PrefixOptional = "dropBy"
	PrefixOptionalIngestBy PrefixOptional = "ingestBy"
)

func (e PrefixOptional) ToPointer() *PrefixOptional {
	return &e
}
func (e *PrefixOptional) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dropBy":
		fallthrough
	case "ingestBy":
		*e = PrefixOptional(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PrefixOptional: %v", v)
	}
}

type ExtentTag struct {
	Prefix *PrefixOptional `json:"prefix,omitempty"`
	Value  string          `json:"value"`
}

func (o *ExtentTag) GetPrefix() *PrefixOptional {
	if o == nil {
		return nil
	}
	return o.Prefix
}

func (o *ExtentTag) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type IngestIfNotExist struct {
	Value string `json:"value"`
}

func (o *IngestIfNotExist) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// ReportLevel - Level of ingestion status reporting. Defaults to FailuresOnly.
type ReportLevel string

const (
	ReportLevelFailuresOnly         ReportLevel = "failuresOnly"
	ReportLevelDoNotReport          ReportLevel = "doNotReport"
	ReportLevelFailuresAndSuccesses ReportLevel = "failuresAndSuccesses"
)

func (e ReportLevel) ToPointer() *ReportLevel {
	return &e
}
func (e *ReportLevel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "failuresOnly":
		fallthrough
	case "doNotReport":
		fallthrough
	case "failuresAndSuccesses":
		*e = ReportLevel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ReportLevel: %v", v)
	}
}

// ReportMethod - Target of the ingestion status reporting. Defaults to Queue.
type ReportMethod string

const (
	ReportMethodQueue         ReportMethod = "queue"
	ReportMethodTable         ReportMethod = "table"
	ReportMethodQueueAndTable ReportMethod = "queueAndTable"
)

func (e ReportMethod) ToPointer() *ReportMethod {
	return &e
}
func (e *ReportMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "queue":
		fallthrough
	case "table":
		fallthrough
	case "queueAndTable":
		*e = ReportMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ReportMethod: %v", v)
	}
}

type AdditionalProperty struct {
	Key   string `json:"key"`
	Value string `json:"value"`
}

func (o *AdditionalProperty) GetKey() string {
	if o == nil {
		return ""
	}
	return o.Key
}

func (o *AdditionalProperty) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type ResponseRetrySettingAzureDataExplorer struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingAzureDataExplorer) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingAzureDataExplorer) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingAzureDataExplorer) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingAzureDataExplorer) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsAzureDataExplorer struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsAzureDataExplorer) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsAzureDataExplorer) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsAzureDataExplorer) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsAzureDataExplorer) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// CompressCompressionAzureDataExplorer - Data compression format to apply to HTTP content before it is delivered
type CompressCompressionAzureDataExplorer string

const (
	CompressCompressionAzureDataExplorerNone CompressCompressionAzureDataExplorer = "none"
	CompressCompressionAzureDataExplorerGzip CompressCompressionAzureDataExplorer = "gzip"
)

func (e CompressCompressionAzureDataExplorer) ToPointer() *CompressCompressionAzureDataExplorer {
	return &e
}
func (e *CompressCompressionAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressCompressionAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressCompressionAzureDataExplorer: %v", v)
	}
}

// PqCompressCompressionAzureDataExplorer - Codec to use to compress the persisted data
type PqCompressCompressionAzureDataExplorer string

const (
	PqCompressCompressionAzureDataExplorerNone PqCompressCompressionAzureDataExplorer = "none"
	PqCompressCompressionAzureDataExplorerGzip PqCompressCompressionAzureDataExplorer = "gzip"
)

func (e PqCompressCompressionAzureDataExplorer) ToPointer() *PqCompressCompressionAzureDataExplorer {
	return &e
}
func (e *PqCompressCompressionAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionAzureDataExplorer: %v", v)
	}
}

// QueueFullBehaviorAzureDataExplorer - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorAzureDataExplorer string

const (
	QueueFullBehaviorAzureDataExplorerBlock QueueFullBehaviorAzureDataExplorer = "block"
	QueueFullBehaviorAzureDataExplorerDrop  QueueFullBehaviorAzureDataExplorer = "drop"
)

func (e QueueFullBehaviorAzureDataExplorer) ToPointer() *QueueFullBehaviorAzureDataExplorer {
	return &e
}
func (e *QueueFullBehaviorAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorAzureDataExplorer: %v", v)
	}
}

// ModeAzureDataExplorer - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeAzureDataExplorer string

const (
	ModeAzureDataExplorerError        ModeAzureDataExplorer = "error"
	ModeAzureDataExplorerBackpressure ModeAzureDataExplorer = "backpressure"
	ModeAzureDataExplorerAlways       ModeAzureDataExplorer = "always"
)

func (e ModeAzureDataExplorer) ToPointer() *ModeAzureDataExplorer {
	return &e
}
func (e *ModeAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeAzureDataExplorer: %v", v)
	}
}

type PqControlsAzureDataExplorer struct {
}

type OutputAzureDataExplorer struct {
	// Unique ID for this output
	ID   string                 `json:"id"`
	Type *TypeAzureDataExplorer `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The base URI for your cluster. Typically, `https://<cluster>.<region>.kusto.windows.net`.
	ClusterURL string `json:"clusterUrl"`
	// Name of the database containing the table where data will be ingested
	Database string `json:"database"`
	// Name of the table to ingest data into
	Table string `json:"table"`
	// When saving or starting the Destination, validate the database name and credentials; also validate table name, except when creating a new table. Disable if your Azure app does not have both the Database Viewer and the Table Viewer role.
	ValidateDatabaseSettings *bool          `default:"true" json:"validateDatabaseSettings"`
	IngestMode               *IngestionMode `default:"batching" json:"ingestMode"`
	// Endpoint used to acquire authentication tokens from Azure
	OauthEndpoint *MicrosoftEntraIDAuthenticationEndpoint `default:"https://login.microsoftonline.com" json:"oauthEndpoint"`
	// Directory ID (tenant identifier) in Azure Active Directory
	TenantID string `json:"tenantId"`
	// client_id to pass in the OAuth request parameter
	ClientID string `json:"clientId"`
	// Scope to pass in the OAuth request parameter
	Scope string `json:"scope"`
	// The type of OAuth 2.0 client credentials grant flow to use
	OauthType   *OauthTypeAuthenticationMethod `default:"clientSecret" json:"oauthType"`
	Description *string                        `json:"description,omitempty"`
	// The client secret that you generated for your app in the Azure portal
	ClientSecret *string `json:"clientSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret  *string                       `json:"textSecret,omitempty"`
	Certificate *CertificateAzureDataExplorer `json:"certificate,omitempty"`
	// The ingestion service URI for your cluster. Typically, `https://ingest-<cluster>.<region>.kusto.windows.net`.
	IngestURL *string `json:"ingestUrl,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorAzureDataExplorer `default:"block" json:"onBackpressure"`
	// Send a JSON mapping object instead of specifying an existing named data mapping
	IsMappingObj *bool `default:"false" json:"isMappingObj"`
	// Format of the output data
	Format *DataFormatAzureDataExplorer `default:"json" json:"format"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// Maximum number of parts to upload in parallel per file
	MaxConcurrentFileParts *float64 `default:"1" json:"maxConcurrentFileParts"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionAzureDataExplorer `default:"block" json:"onDiskFullBackpressure"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Bypass the data management service's aggregation mechanism
	FlushImmediately *bool `default:"false" json:"flushImmediately"`
	// Prevent blob deletion after ingestion is complete
	RetainBlobOnSuccess *bool `default:"false" json:"retainBlobOnSuccess"`
	// Strings or tags associated with the extent (ingested data shard)
	ExtentTags []ExtentTag `json:"extentTags,omitempty"`
	// Prevents duplicate ingestion by verifying whether an extent with the specified ingest-by tag already exists
	IngestIfNotExists []IngestIfNotExist `json:"ingestIfNotExists,omitempty"`
	// Level of ingestion status reporting. Defaults to FailuresOnly.
	ReportLevel *ReportLevel `default:"failuresOnly" json:"reportLevel"`
	// Target of the ingestion status reporting. Defaults to Queue.
	ReportMethod *ReportMethod `default:"queue" json:"reportMethod"`
	// Optionally, enter additional configuration properties to send to the ingestion service
	AdditionalProperties []AdditionalProperty `json:"additionalProperties,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingAzureDataExplorer `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsAzureDataExplorer  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressCompressionAzureDataExplorer `default:"gzip" json:"compress"`
	// Enter the name of a data mapping associated with your target table. Or, if incoming event and target table fields match exactly, you can leave the field empty.
	MappingRef *string `json:"mappingRef,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionAzureDataExplorer `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorAzureDataExplorer `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeAzureDataExplorer       `default:"error" json:"pqMode"`
	PqControls *PqControlsAzureDataExplorer `json:"pqControls,omitempty"`
}

func (o OutputAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureDataExplorer) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputAzureDataExplorer) GetType() *TypeAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputAzureDataExplorer) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureDataExplorer) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureDataExplorer) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureDataExplorer) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureDataExplorer) GetClusterURL() string {
	if o == nil {
		return ""
	}
	return o.ClusterURL
}

func (o *OutputAzureDataExplorer) GetDatabase() string {
	if o == nil {
		return ""
	}
	return o.Database
}

func (o *OutputAzureDataExplorer) GetTable() string {
	if o == nil {
		return ""
	}
	return o.Table
}

func (o *OutputAzureDataExplorer) GetValidateDatabaseSettings() *bool {
	if o == nil {
		return nil
	}
	return o.ValidateDatabaseSettings
}

func (o *OutputAzureDataExplorer) GetIngestMode() *IngestionMode {
	if o == nil {
		return nil
	}
	return o.IngestMode
}

func (o *OutputAzureDataExplorer) GetOauthEndpoint() *MicrosoftEntraIDAuthenticationEndpoint {
	if o == nil {
		return nil
	}
	return o.OauthEndpoint
}

func (o *OutputAzureDataExplorer) GetTenantID() string {
	if o == nil {
		return ""
	}
	return o.TenantID
}

func (o *OutputAzureDataExplorer) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputAzureDataExplorer) GetScope() string {
	if o == nil {
		return ""
	}
	return o.Scope
}

func (o *OutputAzureDataExplorer) GetOauthType() *OauthTypeAuthenticationMethod {
	if o == nil {
		return nil
	}
	return o.OauthType
}

func (o *OutputAzureDataExplorer) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureDataExplorer) GetClientSecret() *string {
	if o == nil {
		return nil
	}
	return o.ClientSecret
}

func (o *OutputAzureDataExplorer) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputAzureDataExplorer) GetCertificate() *CertificateAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.Certificate
}

func (o *OutputAzureDataExplorer) GetIngestURL() *string {
	if o == nil {
		return nil
	}
	return o.IngestURL
}

func (o *OutputAzureDataExplorer) GetOnBackpressure() *BackpressureBehaviorAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureDataExplorer) GetIsMappingObj() *bool {
	if o == nil {
		return nil
	}
	return o.IsMappingObj
}

func (o *OutputAzureDataExplorer) GetFormat() *DataFormatAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputAzureDataExplorer) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputAzureDataExplorer) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputAzureDataExplorer) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputAzureDataExplorer) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputAzureDataExplorer) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputAzureDataExplorer) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputAzureDataExplorer) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputAzureDataExplorer) GetOnDiskFullBackpressure() *DiskSpaceProtectionAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputAzureDataExplorer) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputAzureDataExplorer) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputAzureDataExplorer) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputAzureDataExplorer) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputAzureDataExplorer) GetFlushImmediately() *bool {
	if o == nil {
		return nil
	}
	return o.FlushImmediately
}

func (o *OutputAzureDataExplorer) GetRetainBlobOnSuccess() *bool {
	if o == nil {
		return nil
	}
	return o.RetainBlobOnSuccess
}

func (o *OutputAzureDataExplorer) GetExtentTags() []ExtentTag {
	if o == nil {
		return nil
	}
	return o.ExtentTags
}

func (o *OutputAzureDataExplorer) GetIngestIfNotExists() []IngestIfNotExist {
	if o == nil {
		return nil
	}
	return o.IngestIfNotExists
}

func (o *OutputAzureDataExplorer) GetReportLevel() *ReportLevel {
	if o == nil {
		return nil
	}
	return o.ReportLevel
}

func (o *OutputAzureDataExplorer) GetReportMethod() *ReportMethod {
	if o == nil {
		return nil
	}
	return o.ReportMethod
}

func (o *OutputAzureDataExplorer) GetAdditionalProperties() []AdditionalProperty {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

func (o *OutputAzureDataExplorer) GetResponseRetrySettings() []ResponseRetrySettingAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputAzureDataExplorer) GetTimeoutRetrySettings() *TimeoutRetrySettingsAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputAzureDataExplorer) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputAzureDataExplorer) GetCompress() *CompressCompressionAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputAzureDataExplorer) GetMappingRef() *string {
	if o == nil {
		return nil
	}
	return o.MappingRef
}

func (o *OutputAzureDataExplorer) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputAzureDataExplorer) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputAzureDataExplorer) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputAzureDataExplorer) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputAzureDataExplorer) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputAzureDataExplorer) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputAzureDataExplorer) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputAzureDataExplorer) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputAzureDataExplorer) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputAzureDataExplorer) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputAzureDataExplorer) GetPqCompress() *PqCompressCompressionAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputAzureDataExplorer) GetPqOnBackpressure() *QueueFullBehaviorAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputAzureDataExplorer) GetPqMode() *ModeAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputAzureDataExplorer) GetPqControls() *PqControlsAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeAzureBlob string

const (
	CreateOutputTypeAzureBlobAzureBlob CreateOutputTypeAzureBlob = "azure_blob"
)

func (e CreateOutputTypeAzureBlob) ToPointer() *CreateOutputTypeAzureBlob {
	return &e
}
func (e *CreateOutputTypeAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_blob":
		*e = CreateOutputTypeAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeAzureBlob: %v", v)
	}
}

// DataFormatAzureBlob - Format of the output data
type DataFormatAzureBlob string

const (
	DataFormatAzureBlobJSON    DataFormatAzureBlob = "json"
	DataFormatAzureBlobRaw     DataFormatAzureBlob = "raw"
	DataFormatAzureBlobParquet DataFormatAzureBlob = "parquet"
)

func (e DataFormatAzureBlob) ToPointer() *DataFormatAzureBlob {
	return &e
}
func (e *DataFormatAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "parquet":
		*e = DataFormatAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataFormatAzureBlob: %v", v)
	}
}

// BackpressureBehaviorAzureBlob - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorAzureBlob string

const (
	BackpressureBehaviorAzureBlobBlock BackpressureBehaviorAzureBlob = "block"
	BackpressureBehaviorAzureBlobDrop  BackpressureBehaviorAzureBlob = "drop"
)

func (e BackpressureBehaviorAzureBlob) ToPointer() *BackpressureBehaviorAzureBlob {
	return &e
}
func (e *BackpressureBehaviorAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorAzureBlob: %v", v)
	}
}

// DiskSpaceProtectionAzureBlob - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionAzureBlob string

const (
	DiskSpaceProtectionAzureBlobBlock DiskSpaceProtectionAzureBlob = "block"
	DiskSpaceProtectionAzureBlobDrop  DiskSpaceProtectionAzureBlob = "drop"
)

func (e DiskSpaceProtectionAzureBlob) ToPointer() *DiskSpaceProtectionAzureBlob {
	return &e
}
func (e *DiskSpaceProtectionAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionAzureBlob: %v", v)
	}
}

type CreateOutputAuthenticationMethodAzureBlob string

const (
	CreateOutputAuthenticationMethodAzureBlobManual       CreateOutputAuthenticationMethodAzureBlob = "manual"
	CreateOutputAuthenticationMethodAzureBlobSecret       CreateOutputAuthenticationMethodAzureBlob = "secret"
	CreateOutputAuthenticationMethodAzureBlobClientSecret CreateOutputAuthenticationMethodAzureBlob = "clientSecret"
	CreateOutputAuthenticationMethodAzureBlobClientCert   CreateOutputAuthenticationMethodAzureBlob = "clientCert"
)

func (e CreateOutputAuthenticationMethodAzureBlob) ToPointer() *CreateOutputAuthenticationMethodAzureBlob {
	return &e
}
func (e *CreateOutputAuthenticationMethodAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		fallthrough
	case "clientSecret":
		fallthrough
	case "clientCert":
		*e = CreateOutputAuthenticationMethodAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputAuthenticationMethodAzureBlob: %v", v)
	}
}

type BlobAccessTier string

const (
	BlobAccessTierInferred BlobAccessTier = "Inferred"
	BlobAccessTierHot      BlobAccessTier = "Hot"
	BlobAccessTierCool     BlobAccessTier = "Cool"
	BlobAccessTierCold     BlobAccessTier = "Cold"
	BlobAccessTierArchive  BlobAccessTier = "Archive"
)

func (e BlobAccessTier) ToPointer() *BlobAccessTier {
	return &e
}
func (e *BlobAccessTier) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "Inferred":
		fallthrough
	case "Hot":
		fallthrough
	case "Cool":
		fallthrough
	case "Cold":
		fallthrough
	case "Archive":
		*e = BlobAccessTier(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BlobAccessTier: %v", v)
	}
}

// CreateOutputCompressionAzureBlob - Data compression format to apply to HTTP content before it is delivered
type CreateOutputCompressionAzureBlob string

const (
	CreateOutputCompressionAzureBlobNone CreateOutputCompressionAzureBlob = "none"
	CreateOutputCompressionAzureBlobGzip CreateOutputCompressionAzureBlob = "gzip"
)

func (e CreateOutputCompressionAzureBlob) ToPointer() *CreateOutputCompressionAzureBlob {
	return &e
}
func (e *CreateOutputCompressionAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CreateOutputCompressionAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputCompressionAzureBlob: %v", v)
	}
}

// CompressionLevelAzureBlob - Compression level to apply before moving files to final destination
type CompressionLevelAzureBlob string

const (
	CompressionLevelAzureBlobBestSpeed       CompressionLevelAzureBlob = "best_speed"
	CompressionLevelAzureBlobNormal          CompressionLevelAzureBlob = "normal"
	CompressionLevelAzureBlobBestCompression CompressionLevelAzureBlob = "best_compression"
)

func (e CompressionLevelAzureBlob) ToPointer() *CompressionLevelAzureBlob {
	return &e
}
func (e *CompressionLevelAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "best_speed":
		fallthrough
	case "normal":
		fallthrough
	case "best_compression":
		*e = CompressionLevelAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionLevelAzureBlob: %v", v)
	}
}

// ParquetVersionAzureBlob - Determines which data types are supported and how they are represented
type ParquetVersionAzureBlob string

const (
	ParquetVersionAzureBlobParquet10 ParquetVersionAzureBlob = "PARQUET_1_0"
	ParquetVersionAzureBlobParquet24 ParquetVersionAzureBlob = "PARQUET_2_4"
	ParquetVersionAzureBlobParquet26 ParquetVersionAzureBlob = "PARQUET_2_6"
)

func (e ParquetVersionAzureBlob) ToPointer() *ParquetVersionAzureBlob {
	return &e
}
func (e *ParquetVersionAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PARQUET_1_0":
		fallthrough
	case "PARQUET_2_4":
		fallthrough
	case "PARQUET_2_6":
		*e = ParquetVersionAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ParquetVersionAzureBlob: %v", v)
	}
}

// DataPageVersionAzureBlob - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionAzureBlob string

const (
	DataPageVersionAzureBlobDataPageV1 DataPageVersionAzureBlob = "DATA_PAGE_V1"
	DataPageVersionAzureBlobDataPageV2 DataPageVersionAzureBlob = "DATA_PAGE_V2"
)

func (e DataPageVersionAzureBlob) ToPointer() *DataPageVersionAzureBlob {
	return &e
}
func (e *DataPageVersionAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "DATA_PAGE_V1":
		fallthrough
	case "DATA_PAGE_V2":
		*e = DataPageVersionAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataPageVersionAzureBlob: %v", v)
	}
}

type KeyValueMetadatumAzureBlob struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumAzureBlob) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumAzureBlob) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *KeyValueMetadatumAzureBlob) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *KeyValueMetadatumAzureBlob) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type CreateOutputCertificateAzureBlob struct {
	// The certificate you registered as credentials for your app in the Azure portal
	CertificateName string `json:"certificateName"`
}

func (o *CreateOutputCertificateAzureBlob) GetCertificateName() string {
	if o == nil {
		return ""
	}
	return o.CertificateName
}

type OutputAzureBlob struct {
	// Unique ID for this output
	ID   string                     `json:"id"`
	Type *CreateOutputTypeAzureBlob `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The Azure Blob Storage container name. Name can include only lowercase letters, numbers, and hyphens. For dynamic container names, enter a JavaScript expression within quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myContainer-${C.env["CRIBL_WORKER_ID"]}`.
	ContainerName string `json:"containerName"`
	// Create the configured container in Azure Blob Storage if it does not already exist
	CreateContainer *bool `default:"false" json:"createContainer"`
	// Root directory prepended to path before uploading. Value can be a JavaScript expression enclosed in quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myBlobPrefix-${C.env["CRIBL_WORKER_ID"]}`.
	DestPath *string `json:"destPath,omitempty"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Maximum number of parts to upload in parallel per file
	MaxConcurrentFileParts *float64 `default:"1" json:"maxConcurrentFileParts"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value  if present  otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatAzureBlob `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorAzureBlob `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionAzureBlob              `default:"block" json:"onDiskFullBackpressure"`
	AuthType               *CreateOutputAuthenticationMethodAzureBlob `default:"manual" json:"authType"`
	StorageClass           *BlobAccessTier                            `default:"Inferred" json:"storageClass"`
	Description            *string                                    `json:"description,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CreateOutputCompressionAzureBlob `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelAzureBlob `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionAzureBlob `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionAzureBlob `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumAzureBlob `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
	// Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.
	ConnectionString *string `json:"connectionString,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// The name of your Azure storage account
	StorageAccountName *string `json:"storageAccountName,omitempty"`
	// The service principal's tenant ID
	TenantID *string `json:"tenantId,omitempty"`
	// The service principal's client ID
	ClientID *string `json:"clientId,omitempty"`
	// The Azure cloud to use. Defaults to Azure Public Cloud.
	AzureCloud *string `json:"azureCloud,omitempty"`
	// Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.
	EndpointSuffix *string `json:"endpointSuffix,omitempty"`
	// Select or create a stored text secret
	ClientTextSecret *string                           `json:"clientTextSecret,omitempty"`
	Certificate      *CreateOutputCertificateAzureBlob `json:"certificate,omitempty"`
}

func (o OutputAzureBlob) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureBlob) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureBlob) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputAzureBlob) GetType() *CreateOutputTypeAzureBlob {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputAzureBlob) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureBlob) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureBlob) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureBlob) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureBlob) GetContainerName() string {
	if o == nil {
		return ""
	}
	return o.ContainerName
}

func (o *OutputAzureBlob) GetCreateContainer() *bool {
	if o == nil {
		return nil
	}
	return o.CreateContainer
}

func (o *OutputAzureBlob) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputAzureBlob) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputAzureBlob) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputAzureBlob) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputAzureBlob) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputAzureBlob) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputAzureBlob) GetFormat() *DataFormatAzureBlob {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputAzureBlob) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputAzureBlob) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputAzureBlob) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputAzureBlob) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputAzureBlob) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputAzureBlob) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputAzureBlob) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputAzureBlob) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputAzureBlob) GetOnBackpressure() *BackpressureBehaviorAzureBlob {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureBlob) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputAzureBlob) GetOnDiskFullBackpressure() *DiskSpaceProtectionAzureBlob {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputAzureBlob) GetAuthType() *CreateOutputAuthenticationMethodAzureBlob {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputAzureBlob) GetStorageClass() *BlobAccessTier {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputAzureBlob) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureBlob) GetCompress() *CreateOutputCompressionAzureBlob {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputAzureBlob) GetCompressionLevel() *CompressionLevelAzureBlob {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputAzureBlob) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputAzureBlob) GetParquetVersion() *ParquetVersionAzureBlob {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputAzureBlob) GetParquetDataPageVersion() *DataPageVersionAzureBlob {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputAzureBlob) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputAzureBlob) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputAzureBlob) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputAzureBlob) GetKeyValueMetadata() []KeyValueMetadatumAzureBlob {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputAzureBlob) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputAzureBlob) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputAzureBlob) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputAzureBlob) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputAzureBlob) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputAzureBlob) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputAzureBlob) GetConnectionString() *string {
	if o == nil {
		return nil
	}
	return o.ConnectionString
}

func (o *OutputAzureBlob) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputAzureBlob) GetStorageAccountName() *string {
	if o == nil {
		return nil
	}
	return o.StorageAccountName
}

func (o *OutputAzureBlob) GetTenantID() *string {
	if o == nil {
		return nil
	}
	return o.TenantID
}

func (o *OutputAzureBlob) GetClientID() *string {
	if o == nil {
		return nil
	}
	return o.ClientID
}

func (o *OutputAzureBlob) GetAzureCloud() *string {
	if o == nil {
		return nil
	}
	return o.AzureCloud
}

func (o *OutputAzureBlob) GetEndpointSuffix() *string {
	if o == nil {
		return nil
	}
	return o.EndpointSuffix
}

func (o *OutputAzureBlob) GetClientTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.ClientTextSecret
}

func (o *OutputAzureBlob) GetCertificate() *CreateOutputCertificateAzureBlob {
	if o == nil {
		return nil
	}
	return o.Certificate
}

type CreateOutputTypeS3 string

const (
	CreateOutputTypeS3S3 CreateOutputTypeS3 = "s3"
)

func (e CreateOutputTypeS3) ToPointer() *CreateOutputTypeS3 {
	return &e
}
func (e *CreateOutputTypeS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "s3":
		*e = CreateOutputTypeS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeS3: %v", v)
	}
}

// CreateOutputAuthenticationMethodS3 - AWS authentication method. Choose Auto to use IAM roles.
type CreateOutputAuthenticationMethodS3 string

const (
	CreateOutputAuthenticationMethodS3Auto   CreateOutputAuthenticationMethodS3 = "auto"
	CreateOutputAuthenticationMethodS3Manual CreateOutputAuthenticationMethodS3 = "manual"
	CreateOutputAuthenticationMethodS3Secret CreateOutputAuthenticationMethodS3 = "secret"
)

func (e CreateOutputAuthenticationMethodS3) ToPointer() *CreateOutputAuthenticationMethodS3 {
	return &e
}
func (e *CreateOutputAuthenticationMethodS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = CreateOutputAuthenticationMethodS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputAuthenticationMethodS3: %v", v)
	}
}

// CreateOutputSignatureVersionS3 - Signature version to use for signing S3 requests
type CreateOutputSignatureVersionS3 string

const (
	CreateOutputSignatureVersionS3V2 CreateOutputSignatureVersionS3 = "v2"
	CreateOutputSignatureVersionS3V4 CreateOutputSignatureVersionS3 = "v4"
)

func (e CreateOutputSignatureVersionS3) ToPointer() *CreateOutputSignatureVersionS3 {
	return &e
}
func (e *CreateOutputSignatureVersionS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = CreateOutputSignatureVersionS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputSignatureVersionS3: %v", v)
	}
}

// ObjectACLS3 - Object ACL to assign to uploaded objects
type ObjectACLS3 string

const (
	ObjectACLS3Private                ObjectACLS3 = "private"
	ObjectACLS3PublicRead             ObjectACLS3 = "public-read"
	ObjectACLS3PublicReadWrite        ObjectACLS3 = "public-read-write"
	ObjectACLS3AuthenticatedRead      ObjectACLS3 = "authenticated-read"
	ObjectACLS3AwsExecRead            ObjectACLS3 = "aws-exec-read"
	ObjectACLS3BucketOwnerRead        ObjectACLS3 = "bucket-owner-read"
	ObjectACLS3BucketOwnerFullControl ObjectACLS3 = "bucket-owner-full-control"
)

func (e ObjectACLS3) ToPointer() *ObjectACLS3 {
	return &e
}
func (e *ObjectACLS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "private":
		fallthrough
	case "public-read":
		fallthrough
	case "public-read-write":
		fallthrough
	case "authenticated-read":
		fallthrough
	case "aws-exec-read":
		fallthrough
	case "bucket-owner-read":
		fallthrough
	case "bucket-owner-full-control":
		*e = ObjectACLS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ObjectACLS3: %v", v)
	}
}

// StorageClassS3 - Storage class to select for uploaded objects
type StorageClassS3 string

const (
	StorageClassS3Standard           StorageClassS3 = "STANDARD"
	StorageClassS3ReducedRedundancy  StorageClassS3 = "REDUCED_REDUNDANCY"
	StorageClassS3StandardIa         StorageClassS3 = "STANDARD_IA"
	StorageClassS3OnezoneIa          StorageClassS3 = "ONEZONE_IA"
	StorageClassS3IntelligentTiering StorageClassS3 = "INTELLIGENT_TIERING"
	StorageClassS3Glacier            StorageClassS3 = "GLACIER"
	StorageClassS3GlacierIr          StorageClassS3 = "GLACIER_IR"
	StorageClassS3DeepArchive        StorageClassS3 = "DEEP_ARCHIVE"
)

func (e StorageClassS3) ToPointer() *StorageClassS3 {
	return &e
}
func (e *StorageClassS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "STANDARD":
		fallthrough
	case "REDUCED_REDUNDANCY":
		fallthrough
	case "STANDARD_IA":
		fallthrough
	case "ONEZONE_IA":
		fallthrough
	case "INTELLIGENT_TIERING":
		fallthrough
	case "GLACIER":
		fallthrough
	case "GLACIER_IR":
		fallthrough
	case "DEEP_ARCHIVE":
		*e = StorageClassS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for StorageClassS3: %v", v)
	}
}

type ServerSideEncryptionForUploadedObjectsS3 string

const (
	ServerSideEncryptionForUploadedObjectsS3Aes256 ServerSideEncryptionForUploadedObjectsS3 = "AES256"
	ServerSideEncryptionForUploadedObjectsS3AwsKms ServerSideEncryptionForUploadedObjectsS3 = "aws:kms"
)

func (e ServerSideEncryptionForUploadedObjectsS3) ToPointer() *ServerSideEncryptionForUploadedObjectsS3 {
	return &e
}
func (e *ServerSideEncryptionForUploadedObjectsS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "AES256":
		fallthrough
	case "aws:kms":
		*e = ServerSideEncryptionForUploadedObjectsS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ServerSideEncryptionForUploadedObjectsS3: %v", v)
	}
}

// DataFormatS3 - Format of the output data
type DataFormatS3 string

const (
	DataFormatS3JSON    DataFormatS3 = "json"
	DataFormatS3Raw     DataFormatS3 = "raw"
	DataFormatS3Parquet DataFormatS3 = "parquet"
)

func (e DataFormatS3) ToPointer() *DataFormatS3 {
	return &e
}
func (e *DataFormatS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "parquet":
		*e = DataFormatS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataFormatS3: %v", v)
	}
}

// BackpressureBehaviorS3 - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorS3 string

const (
	BackpressureBehaviorS3Block BackpressureBehaviorS3 = "block"
	BackpressureBehaviorS3Drop  BackpressureBehaviorS3 = "drop"
)

func (e BackpressureBehaviorS3) ToPointer() *BackpressureBehaviorS3 {
	return &e
}
func (e *BackpressureBehaviorS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorS3: %v", v)
	}
}

// DiskSpaceProtectionS3 - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionS3 string

const (
	DiskSpaceProtectionS3Block DiskSpaceProtectionS3 = "block"
	DiskSpaceProtectionS3Drop  DiskSpaceProtectionS3 = "drop"
)

func (e DiskSpaceProtectionS3) ToPointer() *DiskSpaceProtectionS3 {
	return &e
}
func (e *DiskSpaceProtectionS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionS3: %v", v)
	}
}

// CreateOutputCompressionS3 - Data compression format to apply to HTTP content before it is delivered
type CreateOutputCompressionS3 string

const (
	CreateOutputCompressionS3None CreateOutputCompressionS3 = "none"
	CreateOutputCompressionS3Gzip CreateOutputCompressionS3 = "gzip"
)

func (e CreateOutputCompressionS3) ToPointer() *CreateOutputCompressionS3 {
	return &e
}
func (e *CreateOutputCompressionS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CreateOutputCompressionS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputCompressionS3: %v", v)
	}
}

// CompressionLevelS3 - Compression level to apply before moving files to final destination
type CompressionLevelS3 string

const (
	CompressionLevelS3BestSpeed       CompressionLevelS3 = "best_speed"
	CompressionLevelS3Normal          CompressionLevelS3 = "normal"
	CompressionLevelS3BestCompression CompressionLevelS3 = "best_compression"
)

func (e CompressionLevelS3) ToPointer() *CompressionLevelS3 {
	return &e
}
func (e *CompressionLevelS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "best_speed":
		fallthrough
	case "normal":
		fallthrough
	case "best_compression":
		*e = CompressionLevelS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionLevelS3: %v", v)
	}
}

// ParquetVersionS3 - Determines which data types are supported and how they are represented
type ParquetVersionS3 string

const (
	ParquetVersionS3Parquet10 ParquetVersionS3 = "PARQUET_1_0"
	ParquetVersionS3Parquet24 ParquetVersionS3 = "PARQUET_2_4"
	ParquetVersionS3Parquet26 ParquetVersionS3 = "PARQUET_2_6"
)

func (e ParquetVersionS3) ToPointer() *ParquetVersionS3 {
	return &e
}
func (e *ParquetVersionS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PARQUET_1_0":
		fallthrough
	case "PARQUET_2_4":
		fallthrough
	case "PARQUET_2_6":
		*e = ParquetVersionS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ParquetVersionS3: %v", v)
	}
}

// DataPageVersionS3 - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionS3 string

const (
	DataPageVersionS3DataPageV1 DataPageVersionS3 = "DATA_PAGE_V1"
	DataPageVersionS3DataPageV2 DataPageVersionS3 = "DATA_PAGE_V2"
)

func (e DataPageVersionS3) ToPointer() *DataPageVersionS3 {
	return &e
}
func (e *DataPageVersionS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "DATA_PAGE_V1":
		fallthrough
	case "DATA_PAGE_V2":
		*e = DataPageVersionS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataPageVersionS3: %v", v)
	}
}

type KeyValueMetadatumS3 struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *KeyValueMetadatumS3) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *KeyValueMetadatumS3) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputS3 struct {
	// Unique ID for this output
	ID   string              `json:"id"`
	Type *CreateOutputTypeS3 `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket string `json:"bucket"`
	// Region where the S3 bucket is located
	Region *string `json:"region,omitempty"`
	// Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *CreateOutputAuthenticationMethodS3 `default:"auto" json:"awsAuthenticationMethod"`
	// S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing S3 requests
	SignatureVersion *CreateOutputSignatureVersionS3 `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access S3
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Prefix to append to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`
	DestPath *string `default:"" json:"destPath"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLS3 `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass         *StorageClassS3                           `json:"storageClass,omitempty"`
	ServerSideEncryption *ServerSideEncryptionForUploadedObjectsS3 `json:"serverSideEncryption,omitempty"`
	// ID or ARN of the KMS customer-managed key to use for encryption
	KmsKeyID *string `json:"kmsKeyId,omitempty"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value  if present  otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatS3 `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorS3 `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionS3 `default:"block" json:"onDiskFullBackpressure"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Maximum number of files that can be waiting for upload before backpressure is applied
	MaxClosingFilesToBackpressure *float64 `default:"100" json:"maxClosingFilesToBackpressure"`
	Description                   *string  `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CreateOutputCompressionS3 `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelS3 `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionS3 `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionS3 `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumS3 `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputS3) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputS3) GetType() *CreateOutputTypeS3 {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputS3) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputS3) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputS3) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputS3) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputS3) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputS3) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputS3) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputS3) GetAwsAuthenticationMethod() *CreateOutputAuthenticationMethodS3 {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputS3) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputS3) GetSignatureVersion() *CreateOutputSignatureVersionS3 {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputS3) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputS3) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputS3) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputS3) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputS3) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputS3) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputS3) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputS3) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputS3) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputS3) GetObjectACL() *ObjectACLS3 {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputS3) GetStorageClass() *StorageClassS3 {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputS3) GetServerSideEncryption() *ServerSideEncryptionForUploadedObjectsS3 {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputS3) GetKmsKeyID() *string {
	if o == nil {
		return nil
	}
	return o.KmsKeyID
}

func (o *OutputS3) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputS3) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputS3) GetFormat() *DataFormatS3 {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputS3) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputS3) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputS3) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputS3) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputS3) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputS3) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputS3) GetOnBackpressure() *BackpressureBehaviorS3 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputS3) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputS3) GetOnDiskFullBackpressure() *DiskSpaceProtectionS3 {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputS3) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputS3) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputS3) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputS3) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputS3) GetMaxClosingFilesToBackpressure() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxClosingFilesToBackpressure
}

func (o *OutputS3) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputS3) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputS3) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputS3) GetCompress() *CreateOutputCompressionS3 {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputS3) GetCompressionLevel() *CompressionLevelS3 {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputS3) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputS3) GetParquetVersion() *ParquetVersionS3 {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputS3) GetParquetDataPageVersion() *DataPageVersionS3 {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputS3) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputS3) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputS3) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputS3) GetKeyValueMetadata() []KeyValueMetadatumS3 {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputS3) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputS3) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputS3) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputS3) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputS3) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputS3) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type TypeFilesystem string

const (
	TypeFilesystemFilesystem TypeFilesystem = "filesystem"
)

func (e TypeFilesystem) ToPointer() *TypeFilesystem {
	return &e
}
func (e *TypeFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "filesystem":
		*e = TypeFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeFilesystem: %v", v)
	}
}

// DataFormatFilesystem - Format of the output data
type DataFormatFilesystem string

const (
	DataFormatFilesystemJSON    DataFormatFilesystem = "json"
	DataFormatFilesystemRaw     DataFormatFilesystem = "raw"
	DataFormatFilesystemParquet DataFormatFilesystem = "parquet"
)

func (e DataFormatFilesystem) ToPointer() *DataFormatFilesystem {
	return &e
}
func (e *DataFormatFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "parquet":
		*e = DataFormatFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataFormatFilesystem: %v", v)
	}
}

// BackpressureBehaviorFilesystem - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorFilesystem string

const (
	BackpressureBehaviorFilesystemBlock BackpressureBehaviorFilesystem = "block"
	BackpressureBehaviorFilesystemDrop  BackpressureBehaviorFilesystem = "drop"
)

func (e BackpressureBehaviorFilesystem) ToPointer() *BackpressureBehaviorFilesystem {
	return &e
}
func (e *BackpressureBehaviorFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorFilesystem: %v", v)
	}
}

// DiskSpaceProtectionFilesystem - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionFilesystem string

const (
	DiskSpaceProtectionFilesystemBlock DiskSpaceProtectionFilesystem = "block"
	DiskSpaceProtectionFilesystemDrop  DiskSpaceProtectionFilesystem = "drop"
)

func (e DiskSpaceProtectionFilesystem) ToPointer() *DiskSpaceProtectionFilesystem {
	return &e
}
func (e *DiskSpaceProtectionFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionFilesystem: %v", v)
	}
}

// CompressionFilesystem - Data compression format to apply to HTTP content before it is delivered
type CompressionFilesystem string

const (
	CompressionFilesystemNone CompressionFilesystem = "none"
	CompressionFilesystemGzip CompressionFilesystem = "gzip"
)

func (e CompressionFilesystem) ToPointer() *CompressionFilesystem {
	return &e
}
func (e *CompressionFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionFilesystem: %v", v)
	}
}

// CompressionLevelFilesystem - Compression level to apply before moving files to final destination
type CompressionLevelFilesystem string

const (
	CompressionLevelFilesystemBestSpeed       CompressionLevelFilesystem = "best_speed"
	CompressionLevelFilesystemNormal          CompressionLevelFilesystem = "normal"
	CompressionLevelFilesystemBestCompression CompressionLevelFilesystem = "best_compression"
)

func (e CompressionLevelFilesystem) ToPointer() *CompressionLevelFilesystem {
	return &e
}
func (e *CompressionLevelFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "best_speed":
		fallthrough
	case "normal":
		fallthrough
	case "best_compression":
		*e = CompressionLevelFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionLevelFilesystem: %v", v)
	}
}

// ParquetVersionFilesystem - Determines which data types are supported and how they are represented
type ParquetVersionFilesystem string

const (
	ParquetVersionFilesystemParquet10 ParquetVersionFilesystem = "PARQUET_1_0"
	ParquetVersionFilesystemParquet24 ParquetVersionFilesystem = "PARQUET_2_4"
	ParquetVersionFilesystemParquet26 ParquetVersionFilesystem = "PARQUET_2_6"
)

func (e ParquetVersionFilesystem) ToPointer() *ParquetVersionFilesystem {
	return &e
}
func (e *ParquetVersionFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PARQUET_1_0":
		fallthrough
	case "PARQUET_2_4":
		fallthrough
	case "PARQUET_2_6":
		*e = ParquetVersionFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ParquetVersionFilesystem: %v", v)
	}
}

// DataPageVersionFilesystem - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionFilesystem string

const (
	DataPageVersionFilesystemDataPageV1 DataPageVersionFilesystem = "DATA_PAGE_V1"
	DataPageVersionFilesystemDataPageV2 DataPageVersionFilesystem = "DATA_PAGE_V2"
)

func (e DataPageVersionFilesystem) ToPointer() *DataPageVersionFilesystem {
	return &e
}
func (e *DataPageVersionFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "DATA_PAGE_V1":
		fallthrough
	case "DATA_PAGE_V2":
		*e = DataPageVersionFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataPageVersionFilesystem: %v", v)
	}
}

type KeyValueMetadatumFilesystem struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumFilesystem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumFilesystem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *KeyValueMetadatumFilesystem) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *KeyValueMetadatumFilesystem) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputFilesystem struct {
	// Unique ID for this output
	ID   string         `json:"id"`
	Type TypeFilesystem `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Final destination for the output files
	DestPath string `json:"destPath"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
	StagePath *string `json:"stagePath,omitempty"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value  if present  otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatFilesystem `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorFilesystem `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionFilesystem `default:"block" json:"onDiskFullBackpressure"`
	Description            *string                        `json:"description,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressionFilesystem `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelFilesystem `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionFilesystem `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionFilesystem `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumFilesystem `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputFilesystem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputFilesystem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputFilesystem) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputFilesystem) GetType() TypeFilesystem {
	if o == nil {
		return TypeFilesystem("")
	}
	return o.Type
}

func (o *OutputFilesystem) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputFilesystem) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputFilesystem) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputFilesystem) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputFilesystem) GetDestPath() string {
	if o == nil {
		return ""
	}
	return o.DestPath
}

func (o *OutputFilesystem) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputFilesystem) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputFilesystem) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputFilesystem) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputFilesystem) GetFormat() *DataFormatFilesystem {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputFilesystem) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputFilesystem) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputFilesystem) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputFilesystem) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputFilesystem) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputFilesystem) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputFilesystem) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputFilesystem) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputFilesystem) GetOnBackpressure() *BackpressureBehaviorFilesystem {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputFilesystem) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputFilesystem) GetOnDiskFullBackpressure() *DiskSpaceProtectionFilesystem {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputFilesystem) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputFilesystem) GetCompress() *CompressionFilesystem {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputFilesystem) GetCompressionLevel() *CompressionLevelFilesystem {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputFilesystem) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputFilesystem) GetParquetVersion() *ParquetVersionFilesystem {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputFilesystem) GetParquetDataPageVersion() *DataPageVersionFilesystem {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputFilesystem) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputFilesystem) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputFilesystem) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputFilesystem) GetKeyValueMetadata() []KeyValueMetadatumFilesystem {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputFilesystem) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputFilesystem) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputFilesystem) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputFilesystem) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputFilesystem) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputFilesystem) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type TypeSignalfx string

const (
	TypeSignalfxSignalfx TypeSignalfx = "signalfx"
)

func (e TypeSignalfx) ToPointer() *TypeSignalfx {
	return &e
}
func (e *TypeSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "signalfx":
		*e = TypeSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSignalfx: %v", v)
	}
}

// AuthenticationMethodSignalfx - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodSignalfx string

const (
	AuthenticationMethodSignalfxManual AuthenticationMethodSignalfx = "manual"
	AuthenticationMethodSignalfxSecret AuthenticationMethodSignalfx = "secret"
)

func (e AuthenticationMethodSignalfx) ToPointer() *AuthenticationMethodSignalfx {
	return &e
}
func (e *AuthenticationMethodSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodSignalfx: %v", v)
	}
}

type ExtraHTTPHeaderSignalfx struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderSignalfx) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderSignalfx) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeSignalfx - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSignalfx string

const (
	FailedRequestLoggingModeSignalfxPayload           FailedRequestLoggingModeSignalfx = "payload"
	FailedRequestLoggingModeSignalfxPayloadAndHeaders FailedRequestLoggingModeSignalfx = "payloadAndHeaders"
	FailedRequestLoggingModeSignalfxNone              FailedRequestLoggingModeSignalfx = "none"
)

func (e FailedRequestLoggingModeSignalfx) ToPointer() *FailedRequestLoggingModeSignalfx {
	return &e
}
func (e *FailedRequestLoggingModeSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeSignalfx: %v", v)
	}
}

type ResponseRetrySettingSignalfx struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingSignalfx) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingSignalfx) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingSignalfx) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingSignalfx) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsSignalfx struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsSignalfx) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsSignalfx) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsSignalfx) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsSignalfx) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorSignalfx - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSignalfx string

const (
	BackpressureBehaviorSignalfxBlock BackpressureBehaviorSignalfx = "block"
	BackpressureBehaviorSignalfxDrop  BackpressureBehaviorSignalfx = "drop"
	BackpressureBehaviorSignalfxQueue BackpressureBehaviorSignalfx = "queue"
)

func (e BackpressureBehaviorSignalfx) ToPointer() *BackpressureBehaviorSignalfx {
	return &e
}
func (e *BackpressureBehaviorSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSignalfx: %v", v)
	}
}

// CompressionSignalfx - Codec to use to compress the persisted data
type CompressionSignalfx string

const (
	CompressionSignalfxNone CompressionSignalfx = "none"
	CompressionSignalfxGzip CompressionSignalfx = "gzip"
)

func (e CompressionSignalfx) ToPointer() *CompressionSignalfx {
	return &e
}
func (e *CompressionSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionSignalfx: %v", v)
	}
}

// QueueFullBehaviorSignalfx - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSignalfx string

const (
	QueueFullBehaviorSignalfxBlock QueueFullBehaviorSignalfx = "block"
	QueueFullBehaviorSignalfxDrop  QueueFullBehaviorSignalfx = "drop"
)

func (e QueueFullBehaviorSignalfx) ToPointer() *QueueFullBehaviorSignalfx {
	return &e
}
func (e *QueueFullBehaviorSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSignalfx: %v", v)
	}
}

// ModeSignalfx - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSignalfx string

const (
	ModeSignalfxError        ModeSignalfx = "error"
	ModeSignalfxBackpressure ModeSignalfx = "backpressure"
	ModeSignalfxAlways       ModeSignalfx = "always"
)

func (e ModeSignalfx) ToPointer() *ModeSignalfx {
	return &e
}
func (e *ModeSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeSignalfx: %v", v)
	}
}

type PqControlsSignalfx struct {
}

type OutputSignalfx struct {
	// Unique ID for this output
	ID   string       `json:"id"`
	Type TypeSignalfx `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodSignalfx `default:"manual" json:"authType"`
	// SignalFx realm name, e.g. "us0". For a complete list of available SignalFx realm names, please check [here](https://docs.splunk.com/observability/en/get-started/service-description.html#sd-regions).
	Realm *string `default:"us0" json:"realm"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderSignalfx `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSignalfx `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingSignalfx `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSignalfx  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSignalfx `default:"block" json:"onBackpressure"`
	Description    *string                       `json:"description,omitempty"`
	// SignalFx API access token (see [here](https://docs.signalfx.com/en/latest/admin-guide/tokens.html#working-with-access-tokens))
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSignalfx `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSignalfx `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeSignalfx       `default:"error" json:"pqMode"`
	PqControls *PqControlsSignalfx `json:"pqControls,omitempty"`
}

func (o OutputSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSignalfx) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSignalfx) GetType() TypeSignalfx {
	if o == nil {
		return TypeSignalfx("")
	}
	return o.Type
}

func (o *OutputSignalfx) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSignalfx) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSignalfx) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSignalfx) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSignalfx) GetAuthType() *AuthenticationMethodSignalfx {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSignalfx) GetRealm() *string {
	if o == nil {
		return nil
	}
	return o.Realm
}

func (o *OutputSignalfx) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSignalfx) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSignalfx) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSignalfx) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSignalfx) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSignalfx) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSignalfx) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSignalfx) GetExtraHTTPHeaders() []ExtraHTTPHeaderSignalfx {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSignalfx) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputSignalfx) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSignalfx {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSignalfx) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSignalfx) GetResponseRetrySettings() []ResponseRetrySettingSignalfx {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSignalfx) GetTimeoutRetrySettings() *TimeoutRetrySettingsSignalfx {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSignalfx) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSignalfx) GetOnBackpressure() *BackpressureBehaviorSignalfx {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSignalfx) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSignalfx) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputSignalfx) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputSignalfx) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSignalfx) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSignalfx) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSignalfx) GetPqCompress() *CompressionSignalfx {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSignalfx) GetPqOnBackpressure() *QueueFullBehaviorSignalfx {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSignalfx) GetPqMode() *ModeSignalfx {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSignalfx) GetPqControls() *PqControlsSignalfx {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeWavefront string

const (
	TypeWavefrontWavefront TypeWavefront = "wavefront"
)

func (e TypeWavefront) ToPointer() *TypeWavefront {
	return &e
}
func (e *TypeWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "wavefront":
		*e = TypeWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeWavefront: %v", v)
	}
}

// AuthenticationMethodWavefront - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodWavefront string

const (
	AuthenticationMethodWavefrontManual AuthenticationMethodWavefront = "manual"
	AuthenticationMethodWavefrontSecret AuthenticationMethodWavefront = "secret"
)

func (e AuthenticationMethodWavefront) ToPointer() *AuthenticationMethodWavefront {
	return &e
}
func (e *AuthenticationMethodWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodWavefront: %v", v)
	}
}

type ExtraHTTPHeaderWavefront struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderWavefront) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderWavefront) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeWavefront - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeWavefront string

const (
	FailedRequestLoggingModeWavefrontPayload           FailedRequestLoggingModeWavefront = "payload"
	FailedRequestLoggingModeWavefrontPayloadAndHeaders FailedRequestLoggingModeWavefront = "payloadAndHeaders"
	FailedRequestLoggingModeWavefrontNone              FailedRequestLoggingModeWavefront = "none"
)

func (e FailedRequestLoggingModeWavefront) ToPointer() *FailedRequestLoggingModeWavefront {
	return &e
}
func (e *FailedRequestLoggingModeWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeWavefront: %v", v)
	}
}

type ResponseRetrySettingWavefront struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingWavefront) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingWavefront) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingWavefront) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingWavefront) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsWavefront struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsWavefront) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsWavefront) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsWavefront) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsWavefront) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorWavefront - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorWavefront string

const (
	BackpressureBehaviorWavefrontBlock BackpressureBehaviorWavefront = "block"
	BackpressureBehaviorWavefrontDrop  BackpressureBehaviorWavefront = "drop"
	BackpressureBehaviorWavefrontQueue BackpressureBehaviorWavefront = "queue"
)

func (e BackpressureBehaviorWavefront) ToPointer() *BackpressureBehaviorWavefront {
	return &e
}
func (e *BackpressureBehaviorWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorWavefront: %v", v)
	}
}

// CompressionWavefront - Codec to use to compress the persisted data
type CompressionWavefront string

const (
	CompressionWavefrontNone CompressionWavefront = "none"
	CompressionWavefrontGzip CompressionWavefront = "gzip"
)

func (e CompressionWavefront) ToPointer() *CompressionWavefront {
	return &e
}
func (e *CompressionWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionWavefront: %v", v)
	}
}

// QueueFullBehaviorWavefront - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorWavefront string

const (
	QueueFullBehaviorWavefrontBlock QueueFullBehaviorWavefront = "block"
	QueueFullBehaviorWavefrontDrop  QueueFullBehaviorWavefront = "drop"
)

func (e QueueFullBehaviorWavefront) ToPointer() *QueueFullBehaviorWavefront {
	return &e
}
func (e *QueueFullBehaviorWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorWavefront: %v", v)
	}
}

// ModeWavefront - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeWavefront string

const (
	ModeWavefrontError        ModeWavefront = "error"
	ModeWavefrontBackpressure ModeWavefront = "backpressure"
	ModeWavefrontAlways       ModeWavefront = "always"
)

func (e ModeWavefront) ToPointer() *ModeWavefront {
	return &e
}
func (e *ModeWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeWavefront: %v", v)
	}
}

type PqControlsWavefront struct {
}

type OutputWavefront struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type TypeWavefront `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodWavefront `default:"manual" json:"authType"`
	// WaveFront domain name, e.g. "longboard"
	Domain *string `default:"longboard" json:"domain"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderWavefront `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeWavefront `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingWavefront `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsWavefront  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorWavefront `default:"block" json:"onBackpressure"`
	Description    *string                        `json:"description,omitempty"`
	// WaveFront API authentication token (see [here](https://docs.wavefront.com/wavefront_api.html#generating-an-api-token))
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionWavefront `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorWavefront `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeWavefront       `default:"error" json:"pqMode"`
	PqControls *PqControlsWavefront `json:"pqControls,omitempty"`
}

func (o OutputWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputWavefront) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputWavefront) GetType() TypeWavefront {
	if o == nil {
		return TypeWavefront("")
	}
	return o.Type
}

func (o *OutputWavefront) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputWavefront) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputWavefront) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputWavefront) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputWavefront) GetAuthType() *AuthenticationMethodWavefront {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputWavefront) GetDomain() *string {
	if o == nil {
		return nil
	}
	return o.Domain
}

func (o *OutputWavefront) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputWavefront) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputWavefront) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputWavefront) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputWavefront) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputWavefront) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputWavefront) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputWavefront) GetExtraHTTPHeaders() []ExtraHTTPHeaderWavefront {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputWavefront) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputWavefront) GetFailedRequestLoggingMode() *FailedRequestLoggingModeWavefront {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputWavefront) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputWavefront) GetResponseRetrySettings() []ResponseRetrySettingWavefront {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputWavefront) GetTimeoutRetrySettings() *TimeoutRetrySettingsWavefront {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputWavefront) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputWavefront) GetOnBackpressure() *BackpressureBehaviorWavefront {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputWavefront) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputWavefront) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputWavefront) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputWavefront) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputWavefront) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputWavefront) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputWavefront) GetPqCompress() *CompressionWavefront {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputWavefront) GetPqOnBackpressure() *QueueFullBehaviorWavefront {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputWavefront) GetPqMode() *ModeWavefront {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputWavefront) GetPqControls() *PqControlsWavefront {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeSplunkLb string

const (
	TypeSplunkLbSplunkLb TypeSplunkLb = "splunk_lb"
)

func (e TypeSplunkLb) ToPointer() *TypeSplunkLb {
	return &e
}
func (e *TypeSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "splunk_lb":
		*e = TypeSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSplunkLb: %v", v)
	}
}

// NestedFieldSerializationSplunkLb - How to serialize nested fields into index-time fields
type NestedFieldSerializationSplunkLb string

const (
	NestedFieldSerializationSplunkLbJSON NestedFieldSerializationSplunkLb = "json"
	NestedFieldSerializationSplunkLbNone NestedFieldSerializationSplunkLb = "none"
)

func (e NestedFieldSerializationSplunkLb) ToPointer() *NestedFieldSerializationSplunkLb {
	return &e
}
func (e *NestedFieldSerializationSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "none":
		*e = NestedFieldSerializationSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for NestedFieldSerializationSplunkLb: %v", v)
	}
}

type MinimumTLSVersionSplunkLb string

const (
	MinimumTLSVersionSplunkLbTlSv1  MinimumTLSVersionSplunkLb = "TLSv1"
	MinimumTLSVersionSplunkLbTlSv11 MinimumTLSVersionSplunkLb = "TLSv1.1"
	MinimumTLSVersionSplunkLbTlSv12 MinimumTLSVersionSplunkLb = "TLSv1.2"
	MinimumTLSVersionSplunkLbTlSv13 MinimumTLSVersionSplunkLb = "TLSv1.3"
)

func (e MinimumTLSVersionSplunkLb) ToPointer() *MinimumTLSVersionSplunkLb {
	return &e
}
func (e *MinimumTLSVersionSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MinimumTLSVersionSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MinimumTLSVersionSplunkLb: %v", v)
	}
}

type MaximumTLSVersionSplunkLb string

const (
	MaximumTLSVersionSplunkLbTlSv1  MaximumTLSVersionSplunkLb = "TLSv1"
	MaximumTLSVersionSplunkLbTlSv11 MaximumTLSVersionSplunkLb = "TLSv1.1"
	MaximumTLSVersionSplunkLbTlSv12 MaximumTLSVersionSplunkLb = "TLSv1.2"
	MaximumTLSVersionSplunkLbTlSv13 MaximumTLSVersionSplunkLb = "TLSv1.3"
)

func (e MaximumTLSVersionSplunkLb) ToPointer() *MaximumTLSVersionSplunkLb {
	return &e
}
func (e *MaximumTLSVersionSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MaximumTLSVersionSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MaximumTLSVersionSplunkLb: %v", v)
	}
}

type TLSSettingsClientSideSplunkLb struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                    `json:"passphrase,omitempty"`
	MinVersion *MinimumTLSVersionSplunkLb `json:"minVersion,omitempty"`
	MaxVersion *MaximumTLSVersionSplunkLb `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideSplunkLb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideSplunkLb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSideSplunkLb) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSideSplunkLb) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *TLSSettingsClientSideSplunkLb) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *TLSSettingsClientSideSplunkLb) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *TLSSettingsClientSideSplunkLb) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *TLSSettingsClientSideSplunkLb) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *TLSSettingsClientSideSplunkLb) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *TLSSettingsClientSideSplunkLb) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *TLSSettingsClientSideSplunkLb) GetMinVersion() *MinimumTLSVersionSplunkLb {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *TLSSettingsClientSideSplunkLb) GetMaxVersion() *MaximumTLSVersionSplunkLb {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// MaxS2SVersionSplunkLb - The highest S2S protocol version to advertise during handshake
type MaxS2SVersionSplunkLb string

const (
	MaxS2SVersionSplunkLbV3 MaxS2SVersionSplunkLb = "v3"
	MaxS2SVersionSplunkLbV4 MaxS2SVersionSplunkLb = "v4"
)

func (e MaxS2SVersionSplunkLb) ToPointer() *MaxS2SVersionSplunkLb {
	return &e
}
func (e *MaxS2SVersionSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v3":
		fallthrough
	case "v4":
		*e = MaxS2SVersionSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MaxS2SVersionSplunkLb: %v", v)
	}
}

// BackpressureBehaviorSplunkLb - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSplunkLb string

const (
	BackpressureBehaviorSplunkLbBlock BackpressureBehaviorSplunkLb = "block"
	BackpressureBehaviorSplunkLbDrop  BackpressureBehaviorSplunkLb = "drop"
	BackpressureBehaviorSplunkLbQueue BackpressureBehaviorSplunkLb = "queue"
)

func (e BackpressureBehaviorSplunkLb) ToPointer() *BackpressureBehaviorSplunkLb {
	return &e
}
func (e *BackpressureBehaviorSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSplunkLb: %v", v)
	}
}

// AuthenticationMethodSplunkLb - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodSplunkLb string

const (
	AuthenticationMethodSplunkLbManual AuthenticationMethodSplunkLb = "manual"
	AuthenticationMethodSplunkLbSecret AuthenticationMethodSplunkLb = "secret"
)

func (e AuthenticationMethodSplunkLb) ToPointer() *AuthenticationMethodSplunkLb {
	return &e
}
func (e *AuthenticationMethodSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodSplunkLb: %v", v)
	}
}

// CompressCompressionSplunkLb - Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
type CompressCompressionSplunkLb string

const (
	CompressCompressionSplunkLbDisabled CompressCompressionSplunkLb = "disabled"
	CompressCompressionSplunkLbAuto     CompressCompressionSplunkLb = "auto"
	CompressCompressionSplunkLbAlways   CompressCompressionSplunkLb = "always"
)

func (e CompressCompressionSplunkLb) ToPointer() *CompressCompressionSplunkLb {
	return &e
}
func (e *CompressCompressionSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "disabled":
		fallthrough
	case "auto":
		fallthrough
	case "always":
		*e = CompressCompressionSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressCompressionSplunkLb: %v", v)
	}
}

// IndexerDiscoveryConfigsAuthTokenAuthenticationMethod - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type IndexerDiscoveryConfigsAuthTokenAuthenticationMethod string

const (
	IndexerDiscoveryConfigsAuthTokenAuthenticationMethodManual IndexerDiscoveryConfigsAuthTokenAuthenticationMethod = "manual"
	IndexerDiscoveryConfigsAuthTokenAuthenticationMethodSecret IndexerDiscoveryConfigsAuthTokenAuthenticationMethod = "secret"
)

func (e IndexerDiscoveryConfigsAuthTokenAuthenticationMethod) ToPointer() *IndexerDiscoveryConfigsAuthTokenAuthenticationMethod {
	return &e
}
func (e *IndexerDiscoveryConfigsAuthTokenAuthenticationMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = IndexerDiscoveryConfigsAuthTokenAuthenticationMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for IndexerDiscoveryConfigsAuthTokenAuthenticationMethod: %v", v)
	}
}

type CreateOutputAuthToken struct {
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *IndexerDiscoveryConfigsAuthTokenAuthenticationMethod `default:"manual" json:"authType"`
}

func (c CreateOutputAuthToken) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputAuthToken) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *CreateOutputAuthToken) GetAuthType() *IndexerDiscoveryConfigsAuthTokenAuthenticationMethod {
	if o == nil {
		return nil
	}
	return o.AuthType
}

// IndexerDiscoveryConfigsAuthenticationMethod - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type IndexerDiscoveryConfigsAuthenticationMethod string

const (
	IndexerDiscoveryConfigsAuthenticationMethodManual IndexerDiscoveryConfigsAuthenticationMethod = "manual"
	IndexerDiscoveryConfigsAuthenticationMethodSecret IndexerDiscoveryConfigsAuthenticationMethod = "secret"
)

func (e IndexerDiscoveryConfigsAuthenticationMethod) ToPointer() *IndexerDiscoveryConfigsAuthenticationMethod {
	return &e
}
func (e *IndexerDiscoveryConfigsAuthenticationMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = IndexerDiscoveryConfigsAuthenticationMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for IndexerDiscoveryConfigsAuthenticationMethod: %v", v)
	}
}

// IndexerDiscoveryConfigs - List of configurations to set up indexer discovery in Splunk Indexer clustering environment.
type IndexerDiscoveryConfigs struct {
	// Clustering site of the indexers from where indexers need to be discovered. In case of single site cluster, it defaults to 'default' site.
	Site *string `default:"default" json:"site"`
	// Full URI of Splunk cluster manager (scheme://host:port). Example: https://managerAddress:8089
	MasterURI string `json:"masterUri"`
	// Time interval, in seconds, between two consecutive indexer list fetches from cluster manager
	RefreshIntervalSec *float64 `default:"300" json:"refreshIntervalSec"`
	// During indexer discovery, reject cluster manager certificates that are not authorized by the system's CA. Disable to allow untrusted (for example, self-signed) certificates.
	RejectUnauthorized *bool `default:"false" json:"rejectUnauthorized"`
	// Tokens required to authenticate to cluster manager for indexer discovery
	AuthTokens []CreateOutputAuthToken `json:"authTokens,omitempty"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *IndexerDiscoveryConfigsAuthenticationMethod `default:"manual" json:"authType"`
	// Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (i IndexerDiscoveryConfigs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(i, "", false)
}

func (i *IndexerDiscoveryConfigs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &i, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *IndexerDiscoveryConfigs) GetSite() *string {
	if o == nil {
		return nil
	}
	return o.Site
}

func (o *IndexerDiscoveryConfigs) GetMasterURI() string {
	if o == nil {
		return ""
	}
	return o.MasterURI
}

func (o *IndexerDiscoveryConfigs) GetRefreshIntervalSec() *float64 {
	if o == nil {
		return nil
	}
	return o.RefreshIntervalSec
}

func (o *IndexerDiscoveryConfigs) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *IndexerDiscoveryConfigs) GetAuthTokens() []CreateOutputAuthToken {
	if o == nil {
		return nil
	}
	return o.AuthTokens
}

func (o *IndexerDiscoveryConfigs) GetAuthType() *IndexerDiscoveryConfigsAuthenticationMethod {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *IndexerDiscoveryConfigs) GetAuthToken() *string {
	if o == nil {
		return nil
	}
	return o.AuthToken
}

func (o *IndexerDiscoveryConfigs) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

// TLS - Whether to inherit TLS configs from group setting or disable TLS
type TLS string

const (
	TLSInherit TLS = "inherit"
	TLSOff     TLS = "off"
)

func (e TLS) ToPointer() *TLS {
	return &e
}
func (e *TLS) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "inherit":
		fallthrough
	case "off":
		*e = TLS(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TLS: %v", v)
	}
}

type HostSplunkLb struct {
	// The hostname of the receiver
	Host string `json:"host"`
	// The port to connect to on the provided host
	Port *float64 `default:"9997" json:"port"`
	// Whether to inherit TLS configs from group setting or disable TLS
	TLS *TLS `default:"inherit" json:"tls"`
	// Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.
	Servername *string `json:"servername,omitempty"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (h HostSplunkLb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostSplunkLb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *HostSplunkLb) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *HostSplunkLb) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *HostSplunkLb) GetTLS() *TLS {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *HostSplunkLb) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *HostSplunkLb) GetWeight() *float64 {
	if o == nil {
		return nil
	}
	return o.Weight
}

// PqCompressCompressionSplunkLb - Codec to use to compress the persisted data
type PqCompressCompressionSplunkLb string

const (
	PqCompressCompressionSplunkLbNone PqCompressCompressionSplunkLb = "none"
	PqCompressCompressionSplunkLbGzip PqCompressCompressionSplunkLb = "gzip"
)

func (e PqCompressCompressionSplunkLb) ToPointer() *PqCompressCompressionSplunkLb {
	return &e
}
func (e *PqCompressCompressionSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionSplunkLb: %v", v)
	}
}

// QueueFullBehaviorSplunkLb - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSplunkLb string

const (
	QueueFullBehaviorSplunkLbBlock QueueFullBehaviorSplunkLb = "block"
	QueueFullBehaviorSplunkLbDrop  QueueFullBehaviorSplunkLb = "drop"
)

func (e QueueFullBehaviorSplunkLb) ToPointer() *QueueFullBehaviorSplunkLb {
	return &e
}
func (e *QueueFullBehaviorSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSplunkLb: %v", v)
	}
}

// ModeSplunkLb - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSplunkLb string

const (
	ModeSplunkLbError        ModeSplunkLb = "error"
	ModeSplunkLbBackpressure ModeSplunkLb = "backpressure"
	ModeSplunkLbAlways       ModeSplunkLb = "always"
)

func (e ModeSplunkLb) ToPointer() *ModeSplunkLb {
	return &e
}
func (e *ModeSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeSplunkLb: %v", v)
	}
}

type PqControlsSplunkLb struct {
}

type OutputSplunkLb struct {
	// Unique ID for this output
	ID   string       `json:"id"`
	Type TypeSplunkLb `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.
	MaxConcurrentSenders *float64 `default:"0" json:"maxConcurrentSenders"`
	// How to serialize nested fields into index-time fields
	NestedFields *NestedFieldSerializationSplunkLb `default:"none" json:"nestedFields"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64                       `default:"60000" json:"writeTimeout"`
	TLS          *TLSSettingsClientSideSplunkLb `json:"tls,omitempty"`
	// Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.
	EnableMultiMetrics *bool `default:"false" json:"enableMultiMetrics"`
	// Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.
	EnableACK *bool `default:"true" json:"enableACK"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool `default:"false" json:"logFailedRequests"`
	// The highest S2S protocol version to advertise during handshake
	MaxS2Sversion *MaxS2SVersionSplunkLb `default:"v3" json:"maxS2Sversion"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSplunkLb `default:"block" json:"onBackpressure"`
	// Automatically discover indexers in indexer clustering environment.
	IndexerDiscovery *bool `default:"false" json:"indexerDiscovery"`
	// How long (in milliseconds) each LB endpoint can report blocked before the Destination reports unhealthy, blocking the sender. (Grace period for fluctuations.) Use 0 to disable; max 1 minute.
	SenderUnhealthyTimeAllowance *float64 `default:"100" json:"senderUnhealthyTimeAllowance"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType    *AuthenticationMethodSplunkLb `default:"manual" json:"authType"`
	Description *string                       `json:"description,omitempty"`
	// Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.
	MaxFailedHealthChecks *float64 `default:"1" json:"maxFailedHealthChecks"`
	// Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
	Compress *CompressCompressionSplunkLb `default:"disabled" json:"compress"`
	// List of configurations to set up indexer discovery in Splunk Indexer clustering environment.
	IndexerDiscoveryConfigs *IndexerDiscoveryConfigs `json:"indexerDiscoveryConfigs,omitempty"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool `default:"false" json:"excludeSelf"`
	// Set of Splunk indexers to load-balance data to.
	Hosts []HostSplunkLb `json:"hosts"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionSplunkLb `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSplunkLb `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeSplunkLb       `default:"error" json:"pqMode"`
	PqControls *PqControlsSplunkLb `json:"pqControls,omitempty"`
	// Shared secret token to use when establishing a connection to a Splunk indexer.
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (o OutputSplunkLb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSplunkLb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSplunkLb) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSplunkLb) GetType() TypeSplunkLb {
	if o == nil {
		return TypeSplunkLb("")
	}
	return o.Type
}

func (o *OutputSplunkLb) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSplunkLb) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSplunkLb) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSplunkLb) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSplunkLb) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputSplunkLb) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputSplunkLb) GetMaxConcurrentSenders() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentSenders
}

func (o *OutputSplunkLb) GetNestedFields() *NestedFieldSerializationSplunkLb {
	if o == nil {
		return nil
	}
	return o.NestedFields
}

func (o *OutputSplunkLb) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputSplunkLb) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputSplunkLb) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputSplunkLb) GetTLS() *TLSSettingsClientSideSplunkLb {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputSplunkLb) GetEnableMultiMetrics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableMultiMetrics
}

func (o *OutputSplunkLb) GetEnableACK() *bool {
	if o == nil {
		return nil
	}
	return o.EnableACK
}

func (o *OutputSplunkLb) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputSplunkLb) GetMaxS2Sversion() *MaxS2SVersionSplunkLb {
	if o == nil {
		return nil
	}
	return o.MaxS2Sversion
}

func (o *OutputSplunkLb) GetOnBackpressure() *BackpressureBehaviorSplunkLb {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSplunkLb) GetIndexerDiscovery() *bool {
	if o == nil {
		return nil
	}
	return o.IndexerDiscovery
}

func (o *OutputSplunkLb) GetSenderUnhealthyTimeAllowance() *float64 {
	if o == nil {
		return nil
	}
	return o.SenderUnhealthyTimeAllowance
}

func (o *OutputSplunkLb) GetAuthType() *AuthenticationMethodSplunkLb {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSplunkLb) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSplunkLb) GetMaxFailedHealthChecks() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFailedHealthChecks
}

func (o *OutputSplunkLb) GetCompress() *CompressCompressionSplunkLb {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSplunkLb) GetIndexerDiscoveryConfigs() *IndexerDiscoveryConfigs {
	if o == nil {
		return nil
	}
	return o.IndexerDiscoveryConfigs
}

func (o *OutputSplunkLb) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputSplunkLb) GetHosts() []HostSplunkLb {
	if o == nil {
		return []HostSplunkLb{}
	}
	return o.Hosts
}

func (o *OutputSplunkLb) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSplunkLb) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSplunkLb) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSplunkLb) GetPqCompress() *PqCompressCompressionSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSplunkLb) GetPqOnBackpressure() *QueueFullBehaviorSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSplunkLb) GetPqMode() *ModeSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSplunkLb) GetPqControls() *PqControlsSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSplunkLb) GetAuthToken() *string {
	if o == nil {
		return nil
	}
	return o.AuthToken
}

func (o *OutputSplunkLb) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

type CreateOutputTypeSplunk string

const (
	CreateOutputTypeSplunkSplunk CreateOutputTypeSplunk = "splunk"
)

func (e CreateOutputTypeSplunk) ToPointer() *CreateOutputTypeSplunk {
	return &e
}
func (e *CreateOutputTypeSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "splunk":
		*e = CreateOutputTypeSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeSplunk: %v", v)
	}
}

// NestedFieldSerializationSplunk - How to serialize nested fields into index-time fields
type NestedFieldSerializationSplunk string

const (
	NestedFieldSerializationSplunkJSON NestedFieldSerializationSplunk = "json"
	NestedFieldSerializationSplunkNone NestedFieldSerializationSplunk = "none"
)

func (e NestedFieldSerializationSplunk) ToPointer() *NestedFieldSerializationSplunk {
	return &e
}
func (e *NestedFieldSerializationSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "none":
		*e = NestedFieldSerializationSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for NestedFieldSerializationSplunk: %v", v)
	}
}

type CreateOutputMinimumTLSVersionSplunk string

const (
	CreateOutputMinimumTLSVersionSplunkTlSv1  CreateOutputMinimumTLSVersionSplunk = "TLSv1"
	CreateOutputMinimumTLSVersionSplunkTlSv11 CreateOutputMinimumTLSVersionSplunk = "TLSv1.1"
	CreateOutputMinimumTLSVersionSplunkTlSv12 CreateOutputMinimumTLSVersionSplunk = "TLSv1.2"
	CreateOutputMinimumTLSVersionSplunkTlSv13 CreateOutputMinimumTLSVersionSplunk = "TLSv1.3"
)

func (e CreateOutputMinimumTLSVersionSplunk) ToPointer() *CreateOutputMinimumTLSVersionSplunk {
	return &e
}
func (e *CreateOutputMinimumTLSVersionSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputMinimumTLSVersionSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputMinimumTLSVersionSplunk: %v", v)
	}
}

type CreateOutputMaximumTLSVersionSplunk string

const (
	CreateOutputMaximumTLSVersionSplunkTlSv1  CreateOutputMaximumTLSVersionSplunk = "TLSv1"
	CreateOutputMaximumTLSVersionSplunkTlSv11 CreateOutputMaximumTLSVersionSplunk = "TLSv1.1"
	CreateOutputMaximumTLSVersionSplunkTlSv12 CreateOutputMaximumTLSVersionSplunk = "TLSv1.2"
	CreateOutputMaximumTLSVersionSplunkTlSv13 CreateOutputMaximumTLSVersionSplunk = "TLSv1.3"
)

func (e CreateOutputMaximumTLSVersionSplunk) ToPointer() *CreateOutputMaximumTLSVersionSplunk {
	return &e
}
func (e *CreateOutputMaximumTLSVersionSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = CreateOutputMaximumTLSVersionSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputMaximumTLSVersionSplunk: %v", v)
	}
}

type TLSSettingsClientSideSplunk struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                              `json:"passphrase,omitempty"`
	MinVersion *CreateOutputMinimumTLSVersionSplunk `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputMaximumTLSVersionSplunk `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideSplunk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideSplunk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSideSplunk) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSideSplunk) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *TLSSettingsClientSideSplunk) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *TLSSettingsClientSideSplunk) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *TLSSettingsClientSideSplunk) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *TLSSettingsClientSideSplunk) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *TLSSettingsClientSideSplunk) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *TLSSettingsClientSideSplunk) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *TLSSettingsClientSideSplunk) GetMinVersion() *CreateOutputMinimumTLSVersionSplunk {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *TLSSettingsClientSideSplunk) GetMaxVersion() *CreateOutputMaximumTLSVersionSplunk {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// CreateOutputMaxS2SVersionSplunk - The highest S2S protocol version to advertise during handshake
type CreateOutputMaxS2SVersionSplunk string

const (
	CreateOutputMaxS2SVersionSplunkV3 CreateOutputMaxS2SVersionSplunk = "v3"
	CreateOutputMaxS2SVersionSplunkV4 CreateOutputMaxS2SVersionSplunk = "v4"
)

func (e CreateOutputMaxS2SVersionSplunk) ToPointer() *CreateOutputMaxS2SVersionSplunk {
	return &e
}
func (e *CreateOutputMaxS2SVersionSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v3":
		fallthrough
	case "v4":
		*e = CreateOutputMaxS2SVersionSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputMaxS2SVersionSplunk: %v", v)
	}
}

// BackpressureBehaviorSplunk - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSplunk string

const (
	BackpressureBehaviorSplunkBlock BackpressureBehaviorSplunk = "block"
	BackpressureBehaviorSplunkDrop  BackpressureBehaviorSplunk = "drop"
	BackpressureBehaviorSplunkQueue BackpressureBehaviorSplunk = "queue"
)

func (e BackpressureBehaviorSplunk) ToPointer() *BackpressureBehaviorSplunk {
	return &e
}
func (e *BackpressureBehaviorSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSplunk: %v", v)
	}
}

// AuthenticationMethodSplunk - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodSplunk string

const (
	AuthenticationMethodSplunkManual AuthenticationMethodSplunk = "manual"
	AuthenticationMethodSplunkSecret AuthenticationMethodSplunk = "secret"
)

func (e AuthenticationMethodSplunk) ToPointer() *AuthenticationMethodSplunk {
	return &e
}
func (e *AuthenticationMethodSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodSplunk: %v", v)
	}
}

// CreateOutputCompressCompressionSplunk - Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
type CreateOutputCompressCompressionSplunk string

const (
	CreateOutputCompressCompressionSplunkDisabled CreateOutputCompressCompressionSplunk = "disabled"
	CreateOutputCompressCompressionSplunkAuto     CreateOutputCompressCompressionSplunk = "auto"
	CreateOutputCompressCompressionSplunkAlways   CreateOutputCompressCompressionSplunk = "always"
)

func (e CreateOutputCompressCompressionSplunk) ToPointer() *CreateOutputCompressCompressionSplunk {
	return &e
}
func (e *CreateOutputCompressCompressionSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "disabled":
		fallthrough
	case "auto":
		fallthrough
	case "always":
		*e = CreateOutputCompressCompressionSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputCompressCompressionSplunk: %v", v)
	}
}

// PqCompressCompressionSplunk - Codec to use to compress the persisted data
type PqCompressCompressionSplunk string

const (
	PqCompressCompressionSplunkNone PqCompressCompressionSplunk = "none"
	PqCompressCompressionSplunkGzip PqCompressCompressionSplunk = "gzip"
)

func (e PqCompressCompressionSplunk) ToPointer() *PqCompressCompressionSplunk {
	return &e
}
func (e *PqCompressCompressionSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionSplunk: %v", v)
	}
}

// QueueFullBehaviorSplunk - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSplunk string

const (
	QueueFullBehaviorSplunkBlock QueueFullBehaviorSplunk = "block"
	QueueFullBehaviorSplunkDrop  QueueFullBehaviorSplunk = "drop"
)

func (e QueueFullBehaviorSplunk) ToPointer() *QueueFullBehaviorSplunk {
	return &e
}
func (e *QueueFullBehaviorSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSplunk: %v", v)
	}
}

// CreateOutputModeSplunk - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeSplunk string

const (
	CreateOutputModeSplunkError        CreateOutputModeSplunk = "error"
	CreateOutputModeSplunkBackpressure CreateOutputModeSplunk = "backpressure"
	CreateOutputModeSplunkAlways       CreateOutputModeSplunk = "always"
)

func (e CreateOutputModeSplunk) ToPointer() *CreateOutputModeSplunk {
	return &e
}
func (e *CreateOutputModeSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = CreateOutputModeSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputModeSplunk: %v", v)
	}
}

type PqControlsSplunk struct {
}

type OutputSplunk struct {
	// Unique ID for this output
	ID   string                  `json:"id"`
	Type *CreateOutputTypeSplunk `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The hostname of the receiver
	Host string `json:"host"`
	// The port to connect to on the provided host
	Port *float64 `default:"9997" json:"port"`
	// How to serialize nested fields into index-time fields
	NestedFields *NestedFieldSerializationSplunk `default:"none" json:"nestedFields"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64                     `default:"60000" json:"writeTimeout"`
	TLS          *TLSSettingsClientSideSplunk `json:"tls,omitempty"`
	// Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.
	EnableMultiMetrics *bool `default:"false" json:"enableMultiMetrics"`
	// Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.
	EnableACK *bool `default:"true" json:"enableACK"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool `default:"false" json:"logFailedRequests"`
	// The highest S2S protocol version to advertise during handshake
	MaxS2Sversion *CreateOutputMaxS2SVersionSplunk `default:"v3" json:"maxS2Sversion"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSplunk `default:"block" json:"onBackpressure"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType    *AuthenticationMethodSplunk `default:"manual" json:"authType"`
	Description *string                     `json:"description,omitempty"`
	// Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.
	MaxFailedHealthChecks *float64 `default:"1" json:"maxFailedHealthChecks"`
	// Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
	Compress *CreateOutputCompressCompressionSplunk `default:"disabled" json:"compress"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionSplunk `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSplunk `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *CreateOutputModeSplunk `default:"error" json:"pqMode"`
	PqControls *PqControlsSplunk       `json:"pqControls,omitempty"`
	// Shared secret token to use when establishing a connection to a Splunk indexer.
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (o OutputSplunk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSplunk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSplunk) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSplunk) GetType() *CreateOutputTypeSplunk {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputSplunk) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSplunk) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSplunk) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSplunk) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSplunk) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputSplunk) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputSplunk) GetNestedFields() *NestedFieldSerializationSplunk {
	if o == nil {
		return nil
	}
	return o.NestedFields
}

func (o *OutputSplunk) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputSplunk) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputSplunk) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputSplunk) GetTLS() *TLSSettingsClientSideSplunk {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputSplunk) GetEnableMultiMetrics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableMultiMetrics
}

func (o *OutputSplunk) GetEnableACK() *bool {
	if o == nil {
		return nil
	}
	return o.EnableACK
}

func (o *OutputSplunk) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputSplunk) GetMaxS2Sversion() *CreateOutputMaxS2SVersionSplunk {
	if o == nil {
		return nil
	}
	return o.MaxS2Sversion
}

func (o *OutputSplunk) GetOnBackpressure() *BackpressureBehaviorSplunk {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSplunk) GetAuthType() *AuthenticationMethodSplunk {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSplunk) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSplunk) GetMaxFailedHealthChecks() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFailedHealthChecks
}

func (o *OutputSplunk) GetCompress() *CreateOutputCompressCompressionSplunk {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSplunk) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSplunk) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSplunk) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSplunk) GetPqCompress() *PqCompressCompressionSplunk {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSplunk) GetPqOnBackpressure() *QueueFullBehaviorSplunk {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSplunk) GetPqMode() *CreateOutputModeSplunk {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSplunk) GetPqControls() *PqControlsSplunk {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSplunk) GetAuthToken() *string {
	if o == nil {
		return nil
	}
	return o.AuthToken
}

func (o *OutputSplunk) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

type TypeSentinel string

const (
	TypeSentinelSentinel TypeSentinel = "sentinel"
)

func (e TypeSentinel) ToPointer() *TypeSentinel {
	return &e
}
func (e *TypeSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sentinel":
		*e = TypeSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSentinel: %v", v)
	}
}

type ExtraHTTPHeaderSentinel struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o *ExtraHTTPHeaderSentinel) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderSentinel) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeSentinel - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSentinel string

const (
	FailedRequestLoggingModeSentinelPayload           FailedRequestLoggingModeSentinel = "payload"
	FailedRequestLoggingModeSentinelPayloadAndHeaders FailedRequestLoggingModeSentinel = "payloadAndHeaders"
	FailedRequestLoggingModeSentinelNone              FailedRequestLoggingModeSentinel = "none"
)

func (e FailedRequestLoggingModeSentinel) ToPointer() *FailedRequestLoggingModeSentinel {
	return &e
}
func (e *FailedRequestLoggingModeSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeSentinel: %v", v)
	}
}

type ResponseRetrySettingSentinel struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingSentinel) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingSentinel) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingSentinel) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingSentinel) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsSentinel struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsSentinel) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsSentinel) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsSentinel) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsSentinel) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorSentinel - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSentinel string

const (
	BackpressureBehaviorSentinelBlock BackpressureBehaviorSentinel = "block"
	BackpressureBehaviorSentinelDrop  BackpressureBehaviorSentinel = "drop"
	BackpressureBehaviorSentinelQueue BackpressureBehaviorSentinel = "queue"
)

func (e BackpressureBehaviorSentinel) ToPointer() *BackpressureBehaviorSentinel {
	return &e
}
func (e *BackpressureBehaviorSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSentinel: %v", v)
	}
}

type AuthType string

const (
	AuthTypeOauth AuthType = "oauth"
)

func (e AuthType) ToPointer() *AuthType {
	return &e
}
func (e *AuthType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "oauth":
		*e = AuthType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthType: %v", v)
	}
}

// EndpointConfiguration - Enter the data collection endpoint URL or the individual ID
type EndpointConfiguration string

const (
	EndpointConfigurationURL EndpointConfiguration = "url"
	EndpointConfigurationID  EndpointConfiguration = "ID"
)

func (e EndpointConfiguration) ToPointer() *EndpointConfiguration {
	return &e
}
func (e *EndpointConfiguration) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "url":
		fallthrough
	case "ID":
		*e = EndpointConfiguration(v)
		return nil
	default:
		return fmt.Errorf("invalid value for EndpointConfiguration: %v", v)
	}
}

type FormatSentinel string

const (
	FormatSentinelNdjson    FormatSentinel = "ndjson"
	FormatSentinelJSONArray FormatSentinel = "json_array"
	FormatSentinelCustom    FormatSentinel = "custom"
	FormatSentinelAdvanced  FormatSentinel = "advanced"
)

func (e FormatSentinel) ToPointer() *FormatSentinel {
	return &e
}
func (e *FormatSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "ndjson":
		fallthrough
	case "json_array":
		fallthrough
	case "custom":
		fallthrough
	case "advanced":
		*e = FormatSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FormatSentinel: %v", v)
	}
}

// CompressionSentinel - Codec to use to compress the persisted data
type CompressionSentinel string

const (
	CompressionSentinelNone CompressionSentinel = "none"
	CompressionSentinelGzip CompressionSentinel = "gzip"
)

func (e CompressionSentinel) ToPointer() *CompressionSentinel {
	return &e
}
func (e *CompressionSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionSentinel: %v", v)
	}
}

// QueueFullBehaviorSentinel - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSentinel string

const (
	QueueFullBehaviorSentinelBlock QueueFullBehaviorSentinel = "block"
	QueueFullBehaviorSentinelDrop  QueueFullBehaviorSentinel = "drop"
)

func (e QueueFullBehaviorSentinel) ToPointer() *QueueFullBehaviorSentinel {
	return &e
}
func (e *QueueFullBehaviorSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSentinel: %v", v)
	}
}

// ModeSentinel - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSentinel string

const (
	ModeSentinelError        ModeSentinel = "error"
	ModeSentinelBackpressure ModeSentinel = "backpressure"
	ModeSentinelAlways       ModeSentinel = "always"
)

func (e ModeSentinel) ToPointer() *ModeSentinel {
	return &e
}
func (e *ModeSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeSentinel: %v", v)
	}
}

type PqControlsSentinel struct {
}

type OutputSentinel struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type *TypeSentinel `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size (KB) of the request body (defaults to the API's maximum limit of 1000 KB)
	MaxPayloadSizeKB *float64 `default:"1000" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).
	ExtraHTTPHeaders []ExtraHTTPHeaderSentinel `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSentinel `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingSentinel `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSentinel  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSentinel `default:"block" json:"onBackpressure"`
	AuthType       *AuthType                     `json:"authType,omitempty"`
	// URL for OAuth
	LoginURL string `json:"loginUrl"`
	// Secret parameter value to pass in request body
	Secret string `json:"secret"`
	// JavaScript expression to compute the Client ID for the Azure application. Can be a constant.
	ClientID string `json:"client_id"`
	// Scope to pass in the OAuth request
	Scope *string `default:"https://monitor.azure.com/.default" json:"scope"`
	// Enter the data collection endpoint URL or the individual ID
	EndpointURLConfiguration *EndpointConfiguration `default:"url" json:"endpointURLConfiguration"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64        `json:"totalMemoryLimitKB,omitempty"`
	Description        *string         `json:"description,omitempty"`
	Format             *FormatSentinel `json:"format,omitempty"`
	// Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.
	CustomSourceExpression *string `default:"__httpOut" json:"customSourceExpression"`
	// Whether to drop events when the source expression evaluates to null
	CustomDropWhenNull *bool `default:"false" json:"customDropWhenNull"`
	// Delimiter string to insert between individual events. Defaults to newline character.
	CustomEventDelimiter *string `default:"\\n" json:"customEventDelimiter"`
	// Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.
	CustomContentType *string `default:"application/x-ndjson" json:"customContentType"`
	// Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.
	CustomPayloadExpression *string `default:"\\${events}" json:"customPayloadExpression"`
	// HTTP content-type header value
	AdvancedContentType *string `default:"application/json" json:"advancedContentType"`
	// Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
	FormatEventCode *string `json:"formatEventCode,omitempty"`
	// Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
	FormatPayloadCode *string `json:"formatPayloadCode,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSentinel `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSentinel `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeSentinel       `default:"error" json:"pqMode"`
	PqControls *PqControlsSentinel `json:"pqControls,omitempty"`
	// URL to send events to. Can be overwritten by an event's __url field.
	URL *string `json:"url,omitempty"`
	// Immutable ID for the Data Collection Rule (DCR)
	DcrID *string `json:"dcrID,omitempty"`
	// Data collection endpoint (DCE) URL. In the format: `https://<Endpoint-Name>-<Identifier>.<Region>.ingest.monitor.azure.com`
	DceEndpoint *string `json:"dceEndpoint,omitempty"`
	// The name of the stream (Sentinel table) in which to store the events
	StreamName *string `json:"streamName,omitempty"`
}

func (o OutputSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSentinel) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSentinel) GetType() *TypeSentinel {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputSentinel) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSentinel) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSentinel) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSentinel) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSentinel) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputSentinel) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSentinel) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSentinel) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSentinel) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSentinel) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSentinel) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSentinel) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSentinel) GetExtraHTTPHeaders() []ExtraHTTPHeaderSentinel {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSentinel) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputSentinel) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSentinel {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSentinel) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSentinel) GetResponseRetrySettings() []ResponseRetrySettingSentinel {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSentinel) GetTimeoutRetrySettings() *TimeoutRetrySettingsSentinel {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSentinel) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSentinel) GetOnBackpressure() *BackpressureBehaviorSentinel {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSentinel) GetAuthType() *AuthType {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSentinel) GetLoginURL() string {
	if o == nil {
		return ""
	}
	return o.LoginURL
}

func (o *OutputSentinel) GetSecret() string {
	if o == nil {
		return ""
	}
	return o.Secret
}

func (o *OutputSentinel) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputSentinel) GetScope() *string {
	if o == nil {
		return nil
	}
	return o.Scope
}

func (o *OutputSentinel) GetEndpointURLConfiguration() *EndpointConfiguration {
	if o == nil {
		return nil
	}
	return o.EndpointURLConfiguration
}

func (o *OutputSentinel) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputSentinel) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSentinel) GetFormat() *FormatSentinel {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputSentinel) GetCustomSourceExpression() *string {
	if o == nil {
		return nil
	}
	return o.CustomSourceExpression
}

func (o *OutputSentinel) GetCustomDropWhenNull() *bool {
	if o == nil {
		return nil
	}
	return o.CustomDropWhenNull
}

func (o *OutputSentinel) GetCustomEventDelimiter() *string {
	if o == nil {
		return nil
	}
	return o.CustomEventDelimiter
}

func (o *OutputSentinel) GetCustomContentType() *string {
	if o == nil {
		return nil
	}
	return o.CustomContentType
}

func (o *OutputSentinel) GetCustomPayloadExpression() *string {
	if o == nil {
		return nil
	}
	return o.CustomPayloadExpression
}

func (o *OutputSentinel) GetAdvancedContentType() *string {
	if o == nil {
		return nil
	}
	return o.AdvancedContentType
}

func (o *OutputSentinel) GetFormatEventCode() *string {
	if o == nil {
		return nil
	}
	return o.FormatEventCode
}

func (o *OutputSentinel) GetFormatPayloadCode() *string {
	if o == nil {
		return nil
	}
	return o.FormatPayloadCode
}

func (o *OutputSentinel) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSentinel) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSentinel) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSentinel) GetPqCompress() *CompressionSentinel {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSentinel) GetPqOnBackpressure() *QueueFullBehaviorSentinel {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSentinel) GetPqMode() *ModeSentinel {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSentinel) GetPqControls() *PqControlsSentinel {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSentinel) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputSentinel) GetDcrID() *string {
	if o == nil {
		return nil
	}
	return o.DcrID
}

func (o *OutputSentinel) GetDceEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.DceEndpoint
}

func (o *OutputSentinel) GetStreamName() *string {
	if o == nil {
		return nil
	}
	return o.StreamName
}

type TypeDefault string

const (
	TypeDefaultDefault TypeDefault = "default"
)

func (e TypeDefault) ToPointer() *TypeDefault {
	return &e
}
func (e *TypeDefault) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "default":
		*e = TypeDefault(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDefault: %v", v)
	}
}

type OutputDefault struct {
	// Unique ID for this output
	ID   string      `json:"id"`
	Type TypeDefault `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// ID of the default output. This will be used whenever a nonexistent/deleted output is referenced.
	DefaultID string `json:"defaultId"`
}

func (o *OutputDefault) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDefault) GetType() TypeDefault {
	if o == nil {
		return TypeDefault("")
	}
	return o.Type
}

func (o *OutputDefault) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDefault) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDefault) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDefault) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDefault) GetDefaultID() string {
	if o == nil {
		return ""
	}
	return o.DefaultID
}

type CreateOutputRequestType string

const (
	CreateOutputRequestTypeOutputDefault                CreateOutputRequestType = "OutputDefault"
	CreateOutputRequestTypeOutputWebhook                CreateOutputRequestType = "OutputWebhook"
	CreateOutputRequestTypeOutputSentinel               CreateOutputRequestType = "OutputSentinel"
	CreateOutputRequestTypeOutputDevnull                CreateOutputRequestType = "OutputDevnull"
	CreateOutputRequestTypeOutputSyslog                 CreateOutputRequestType = "OutputSyslog"
	CreateOutputRequestTypeOutputSplunk                 CreateOutputRequestType = "OutputSplunk"
	CreateOutputRequestTypeOutputSplunkLb               CreateOutputRequestType = "OutputSplunkLb"
	CreateOutputRequestTypeOutputSplunkHec              CreateOutputRequestType = "OutputSplunkHec"
	CreateOutputRequestTypeOutputTcpjson                CreateOutputRequestType = "OutputTcpjson"
	CreateOutputRequestTypeOutputWavefront              CreateOutputRequestType = "OutputWavefront"
	CreateOutputRequestTypeOutputSignalfx               CreateOutputRequestType = "OutputSignalfx"
	CreateOutputRequestTypeOutputFilesystem             CreateOutputRequestType = "OutputFilesystem"
	CreateOutputRequestTypeOutputS3                     CreateOutputRequestType = "OutputS3"
	CreateOutputRequestTypeOutputAzureBlob              CreateOutputRequestType = "OutputAzureBlob"
	CreateOutputRequestTypeOutputAzureDataExplorer      CreateOutputRequestType = "OutputAzureDataExplorer"
	CreateOutputRequestTypeOutputAzureLogs              CreateOutputRequestType = "OutputAzureLogs"
	CreateOutputRequestTypeOutputKinesis                CreateOutputRequestType = "OutputKinesis"
	CreateOutputRequestTypeOutputHoneycomb              CreateOutputRequestType = "OutputHoneycomb"
	CreateOutputRequestTypeOutputAzureEventhub          CreateOutputRequestType = "OutputAzureEventhub"
	CreateOutputRequestTypeOutputGoogleChronicle        CreateOutputRequestType = "OutputGoogleChronicle"
	CreateOutputRequestTypeOutputGoogleCloudStorage     CreateOutputRequestType = "OutputGoogleCloudStorage"
	CreateOutputRequestTypeOutputGoogleCloudLogging     CreateOutputRequestType = "OutputGoogleCloudLogging"
	CreateOutputRequestTypeOutputGooglePubsub           CreateOutputRequestType = "OutputGooglePubsub"
	CreateOutputRequestTypeOutputExabeam                CreateOutputRequestType = "OutputExabeam"
	CreateOutputRequestTypeOutputKafka                  CreateOutputRequestType = "OutputKafka"
	CreateOutputRequestTypeOutputConfluentCloud         CreateOutputRequestType = "OutputConfluentCloud"
	CreateOutputRequestTypeOutputMsk                    CreateOutputRequestType = "OutputMsk"
	CreateOutputRequestTypeOutputElastic                CreateOutputRequestType = "OutputElastic"
	CreateOutputRequestTypeOutputElasticCloud           CreateOutputRequestType = "OutputElasticCloud"
	CreateOutputRequestTypeOutputNewrelic               CreateOutputRequestType = "OutputNewrelic"
	CreateOutputRequestTypeOutputNewrelicEvents         CreateOutputRequestType = "OutputNewrelicEvents"
	CreateOutputRequestTypeOutputInfluxdb               CreateOutputRequestType = "OutputInfluxdb"
	CreateOutputRequestTypeOutputCloudwatch             CreateOutputRequestType = "OutputCloudwatch"
	CreateOutputRequestTypeOutputMinio                  CreateOutputRequestType = "OutputMinio"
	CreateOutputRequestTypeOutputStatsd                 CreateOutputRequestType = "OutputStatsd"
	CreateOutputRequestTypeOutputStatsdExt              CreateOutputRequestType = "OutputStatsdExt"
	CreateOutputRequestTypeOutputGraphite               CreateOutputRequestType = "OutputGraphite"
	CreateOutputRequestTypeOutputRouter                 CreateOutputRequestType = "OutputRouter"
	CreateOutputRequestTypeOutputSns                    CreateOutputRequestType = "OutputSns"
	CreateOutputRequestTypeOutputSqs                    CreateOutputRequestType = "OutputSqs"
	CreateOutputRequestTypeOutputSnmp                   CreateOutputRequestType = "OutputSnmp"
	CreateOutputRequestTypeOutputSumoLogic              CreateOutputRequestType = "OutputSumoLogic"
	CreateOutputRequestTypeOutputDatadog                CreateOutputRequestType = "OutputDatadog"
	CreateOutputRequestTypeOutputGrafanaCloud           CreateOutputRequestType = "OutputGrafanaCloud"
	CreateOutputRequestTypeOutputLoki                   CreateOutputRequestType = "OutputLoki"
	CreateOutputRequestTypeOutputPrometheus             CreateOutputRequestType = "OutputPrometheus"
	CreateOutputRequestTypeOutputRing                   CreateOutputRequestType = "OutputRing"
	CreateOutputRequestTypeOutputOpenTelemetry          CreateOutputRequestType = "OutputOpenTelemetry"
	CreateOutputRequestTypeOutputServiceNow             CreateOutputRequestType = "OutputServiceNow"
	CreateOutputRequestTypeOutputDataset                CreateOutputRequestType = "OutputDataset"
	CreateOutputRequestTypeOutputCriblTCP               CreateOutputRequestType = "OutputCriblTcp"
	CreateOutputRequestTypeOutputCriblHTTP              CreateOutputRequestType = "OutputCriblHttp"
	CreateOutputRequestTypeOutputHumioHec               CreateOutputRequestType = "OutputHumioHec"
	CreateOutputRequestTypeOutputCrowdstrikeNextGenSiem CreateOutputRequestType = "OutputCrowdstrikeNextGenSiem"
	CreateOutputRequestTypeOutputDlS3                   CreateOutputRequestType = "OutputDlS3"
	CreateOutputRequestTypeOutputSecurityLake           CreateOutputRequestType = "OutputSecurityLake"
	CreateOutputRequestTypeOutputCriblLake              CreateOutputRequestType = "OutputCriblLake"
	CreateOutputRequestTypeOutputDiskSpool              CreateOutputRequestType = "OutputDiskSpool"
	CreateOutputRequestTypeOutputClickHouse             CreateOutputRequestType = "OutputClickHouse"
	CreateOutputRequestTypeOutputXsiam                  CreateOutputRequestType = "OutputXsiam"
	CreateOutputRequestTypeOutputNetflow                CreateOutputRequestType = "OutputNetflow"
	CreateOutputRequestTypeOutputDynatraceHTTP          CreateOutputRequestType = "OutputDynatraceHttp"
	CreateOutputRequestTypeOutputDynatraceOtlp          CreateOutputRequestType = "OutputDynatraceOtlp"
	CreateOutputRequestTypeOutputSentinelOneAiSiem      CreateOutputRequestType = "OutputSentinelOneAiSiem"
)

// CreateOutputRequest - New Destination object
type CreateOutputRequest struct {
	OutputDefault                *OutputDefault                 `queryParam:"inline"`
	OutputWebhook                *components.OutputWebhook      `queryParam:"inline"`
	OutputSentinel               *OutputSentinel                `queryParam:"inline"`
	OutputDevnull                *components.OutputDevnull      `queryParam:"inline"`
	OutputSyslog                 *components.OutputSyslog       `queryParam:"inline"`
	OutputSplunk                 *OutputSplunk                  `queryParam:"inline"`
	OutputSplunkLb               *OutputSplunkLb                `queryParam:"inline"`
	OutputSplunkHec              *components.OutputSplunkHec    `queryParam:"inline"`
	OutputTcpjson                *components.OutputTcpjson      `queryParam:"inline"`
	OutputWavefront              *OutputWavefront               `queryParam:"inline"`
	OutputSignalfx               *OutputSignalfx                `queryParam:"inline"`
	OutputFilesystem             *OutputFilesystem              `queryParam:"inline"`
	OutputS3                     *OutputS3                      `queryParam:"inline"`
	OutputAzureBlob              *OutputAzureBlob               `queryParam:"inline"`
	OutputAzureDataExplorer      *OutputAzureDataExplorer       `queryParam:"inline"`
	OutputAzureLogs              *OutputAzureLogs               `queryParam:"inline"`
	OutputKinesis                *OutputKinesis                 `queryParam:"inline"`
	OutputHoneycomb              *OutputHoneycomb               `queryParam:"inline"`
	OutputAzureEventhub          *OutputAzureEventhub           `queryParam:"inline"`
	OutputGoogleChronicle        *OutputGoogleChronicle         `queryParam:"inline"`
	OutputGoogleCloudStorage     *OutputGoogleCloudStorage      `queryParam:"inline"`
	OutputGoogleCloudLogging     *OutputGoogleCloudLogging      `queryParam:"inline"`
	OutputGooglePubsub           *OutputGooglePubsub            `queryParam:"inline"`
	OutputExabeam                *OutputExabeam                 `queryParam:"inline"`
	OutputKafka                  *OutputKafka                   `queryParam:"inline"`
	OutputConfluentCloud         *OutputConfluentCloud          `queryParam:"inline"`
	OutputMsk                    *OutputMsk                     `queryParam:"inline"`
	OutputElastic                *OutputElastic                 `queryParam:"inline"`
	OutputElasticCloud           *OutputElasticCloud            `queryParam:"inline"`
	OutputNewrelic               *components.OutputNewrelic     `queryParam:"inline"`
	OutputNewrelicEvents         *OutputNewrelicEvents          `queryParam:"inline"`
	OutputInfluxdb               *OutputInfluxdb                `queryParam:"inline"`
	OutputCloudwatch             *OutputCloudwatch              `queryParam:"inline"`
	OutputMinio                  *OutputMinio                   `queryParam:"inline"`
	OutputStatsd                 *OutputStatsd                  `queryParam:"inline"`
	OutputStatsdExt              *OutputStatsdExt               `queryParam:"inline"`
	OutputGraphite               *OutputGraphite                `queryParam:"inline"`
	OutputRouter                 *OutputRouter                  `queryParam:"inline"`
	OutputSns                    *OutputSns                     `queryParam:"inline"`
	OutputSqs                    *OutputSqs                     `queryParam:"inline"`
	OutputSnmp                   *OutputSnmp                    `queryParam:"inline"`
	OutputSumoLogic              *OutputSumoLogic               `queryParam:"inline"`
	OutputDatadog                *components.OutputDatadog      `queryParam:"inline"`
	OutputGrafanaCloud           *components.OutputGrafanaCloud `queryParam:"inline"`
	OutputLoki                   *OutputLoki                    `queryParam:"inline"`
	OutputPrometheus             *OutputPrometheus              `queryParam:"inline"`
	OutputRing                   *components.OutputRing         `queryParam:"inline"`
	OutputOpenTelemetry          *OutputOpenTelemetry           `queryParam:"inline"`
	OutputServiceNow             *OutputServiceNow              `queryParam:"inline"`
	OutputDataset                *components.OutputDataset      `queryParam:"inline"`
	OutputCriblTCP               *components.OutputCriblTCP     `queryParam:"inline"`
	OutputCriblHTTP              *components.OutputCriblHTTP    `queryParam:"inline"`
	OutputHumioHec               *OutputHumioHec                `queryParam:"inline"`
	OutputCrowdstrikeNextGenSiem *OutputCrowdstrikeNextGenSiem  `queryParam:"inline"`
	OutputDlS3                   *OutputDlS3                    `queryParam:"inline"`
	OutputSecurityLake           *OutputSecurityLake            `queryParam:"inline"`
	OutputCriblLake              *components.OutputCriblLake    `queryParam:"inline"`
	OutputDiskSpool              *components.OutputDiskSpool    `queryParam:"inline"`
	OutputClickHouse             *OutputClickHouse              `queryParam:"inline"`
	OutputXsiam                  *components.OutputXsiam        `queryParam:"inline"`
	OutputNetflow                *OutputNetflow                 `queryParam:"inline"`
	OutputDynatraceHTTP          *OutputDynatraceHTTP           `queryParam:"inline"`
	OutputDynatraceOtlp          *OutputDynatraceOtlp           `queryParam:"inline"`
	OutputSentinelOneAiSiem      *OutputSentinelOneAiSiem       `queryParam:"inline"`

	Type CreateOutputRequestType
}

func CreateCreateOutputRequestOutputDefault(outputDefault OutputDefault) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputDefault

	return CreateOutputRequest{
		OutputDefault: &outputDefault,
		Type:          typ,
	}
}

func CreateCreateOutputRequestOutputWebhook(outputWebhook components.OutputWebhook) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputWebhook

	return CreateOutputRequest{
		OutputWebhook: &outputWebhook,
		Type:          typ,
	}
}

func CreateCreateOutputRequestOutputSentinel(outputSentinel OutputSentinel) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputSentinel

	return CreateOutputRequest{
		OutputSentinel: &outputSentinel,
		Type:           typ,
	}
}

func CreateCreateOutputRequestOutputDevnull(outputDevnull components.OutputDevnull) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputDevnull

	return CreateOutputRequest{
		OutputDevnull: &outputDevnull,
		Type:          typ,
	}
}

func CreateCreateOutputRequestOutputSyslog(outputSyslog components.OutputSyslog) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputSyslog

	return CreateOutputRequest{
		OutputSyslog: &outputSyslog,
		Type:         typ,
	}
}

func CreateCreateOutputRequestOutputSplunk(outputSplunk OutputSplunk) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputSplunk

	return CreateOutputRequest{
		OutputSplunk: &outputSplunk,
		Type:         typ,
	}
}

func CreateCreateOutputRequestOutputSplunkLb(outputSplunkLb OutputSplunkLb) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputSplunkLb

	return CreateOutputRequest{
		OutputSplunkLb: &outputSplunkLb,
		Type:           typ,
	}
}

func CreateCreateOutputRequestOutputSplunkHec(outputSplunkHec components.OutputSplunkHec) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputSplunkHec

	return CreateOutputRequest{
		OutputSplunkHec: &outputSplunkHec,
		Type:            typ,
	}
}

func CreateCreateOutputRequestOutputTcpjson(outputTcpjson components.OutputTcpjson) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputTcpjson

	return CreateOutputRequest{
		OutputTcpjson: &outputTcpjson,
		Type:          typ,
	}
}

func CreateCreateOutputRequestOutputWavefront(outputWavefront OutputWavefront) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputWavefront

	return CreateOutputRequest{
		OutputWavefront: &outputWavefront,
		Type:            typ,
	}
}

func CreateCreateOutputRequestOutputSignalfx(outputSignalfx OutputSignalfx) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputSignalfx

	return CreateOutputRequest{
		OutputSignalfx: &outputSignalfx,
		Type:           typ,
	}
}

func CreateCreateOutputRequestOutputFilesystem(outputFilesystem OutputFilesystem) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputFilesystem

	return CreateOutputRequest{
		OutputFilesystem: &outputFilesystem,
		Type:             typ,
	}
}

func CreateCreateOutputRequestOutputS3(outputS3 OutputS3) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputS3

	return CreateOutputRequest{
		OutputS3: &outputS3,
		Type:     typ,
	}
}

func CreateCreateOutputRequestOutputAzureBlob(outputAzureBlob OutputAzureBlob) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputAzureBlob

	return CreateOutputRequest{
		OutputAzureBlob: &outputAzureBlob,
		Type:            typ,
	}
}

func CreateCreateOutputRequestOutputAzureDataExplorer(outputAzureDataExplorer OutputAzureDataExplorer) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputAzureDataExplorer

	return CreateOutputRequest{
		OutputAzureDataExplorer: &outputAzureDataExplorer,
		Type:                    typ,
	}
}

func CreateCreateOutputRequestOutputAzureLogs(outputAzureLogs OutputAzureLogs) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputAzureLogs

	return CreateOutputRequest{
		OutputAzureLogs: &outputAzureLogs,
		Type:            typ,
	}
}

func CreateCreateOutputRequestOutputKinesis(outputKinesis OutputKinesis) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputKinesis

	return CreateOutputRequest{
		OutputKinesis: &outputKinesis,
		Type:          typ,
	}
}

func CreateCreateOutputRequestOutputHoneycomb(outputHoneycomb OutputHoneycomb) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputHoneycomb

	return CreateOutputRequest{
		OutputHoneycomb: &outputHoneycomb,
		Type:            typ,
	}
}

func CreateCreateOutputRequestOutputAzureEventhub(outputAzureEventhub OutputAzureEventhub) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputAzureEventhub

	return CreateOutputRequest{
		OutputAzureEventhub: &outputAzureEventhub,
		Type:                typ,
	}
}

func CreateCreateOutputRequestOutputGoogleChronicle(outputGoogleChronicle OutputGoogleChronicle) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputGoogleChronicle

	return CreateOutputRequest{
		OutputGoogleChronicle: &outputGoogleChronicle,
		Type:                  typ,
	}
}

func CreateCreateOutputRequestOutputGoogleCloudStorage(outputGoogleCloudStorage OutputGoogleCloudStorage) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputGoogleCloudStorage

	return CreateOutputRequest{
		OutputGoogleCloudStorage: &outputGoogleCloudStorage,
		Type:                     typ,
	}
}

func CreateCreateOutputRequestOutputGoogleCloudLogging(outputGoogleCloudLogging OutputGoogleCloudLogging) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputGoogleCloudLogging

	return CreateOutputRequest{
		OutputGoogleCloudLogging: &outputGoogleCloudLogging,
		Type:                     typ,
	}
}

func CreateCreateOutputRequestOutputGooglePubsub(outputGooglePubsub OutputGooglePubsub) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputGooglePubsub

	return CreateOutputRequest{
		OutputGooglePubsub: &outputGooglePubsub,
		Type:               typ,
	}
}

func CreateCreateOutputRequestOutputExabeam(outputExabeam OutputExabeam) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputExabeam

	return CreateOutputRequest{
		OutputExabeam: &outputExabeam,
		Type:          typ,
	}
}

func CreateCreateOutputRequestOutputKafka(outputKafka OutputKafka) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputKafka

	return CreateOutputRequest{
		OutputKafka: &outputKafka,
		Type:        typ,
	}
}

func CreateCreateOutputRequestOutputConfluentCloud(outputConfluentCloud OutputConfluentCloud) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputConfluentCloud

	return CreateOutputRequest{
		OutputConfluentCloud: &outputConfluentCloud,
		Type:                 typ,
	}
}

func CreateCreateOutputRequestOutputMsk(outputMsk OutputMsk) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputMsk

	return CreateOutputRequest{
		OutputMsk: &outputMsk,
		Type:      typ,
	}
}

func CreateCreateOutputRequestOutputElastic(outputElastic OutputElastic) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputElastic

	return CreateOutputRequest{
		OutputElastic: &outputElastic,
		Type:          typ,
	}
}

func CreateCreateOutputRequestOutputElasticCloud(outputElasticCloud OutputElasticCloud) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputElasticCloud

	return CreateOutputRequest{
		OutputElasticCloud: &outputElasticCloud,
		Type:               typ,
	}
}

func CreateCreateOutputRequestOutputNewrelic(outputNewrelic components.OutputNewrelic) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputNewrelic

	return CreateOutputRequest{
		OutputNewrelic: &outputNewrelic,
		Type:           typ,
	}
}

func CreateCreateOutputRequestOutputNewrelicEvents(outputNewrelicEvents OutputNewrelicEvents) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputNewrelicEvents

	return CreateOutputRequest{
		OutputNewrelicEvents: &outputNewrelicEvents,
		Type:                 typ,
	}
}

func CreateCreateOutputRequestOutputInfluxdb(outputInfluxdb OutputInfluxdb) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputInfluxdb

	return CreateOutputRequest{
		OutputInfluxdb: &outputInfluxdb,
		Type:           typ,
	}
}

func CreateCreateOutputRequestOutputCloudwatch(outputCloudwatch OutputCloudwatch) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputCloudwatch

	return CreateOutputRequest{
		OutputCloudwatch: &outputCloudwatch,
		Type:             typ,
	}
}

func CreateCreateOutputRequestOutputMinio(outputMinio OutputMinio) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputMinio

	return CreateOutputRequest{
		OutputMinio: &outputMinio,
		Type:        typ,
	}
}

func CreateCreateOutputRequestOutputStatsd(outputStatsd OutputStatsd) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputStatsd

	return CreateOutputRequest{
		OutputStatsd: &outputStatsd,
		Type:         typ,
	}
}

func CreateCreateOutputRequestOutputStatsdExt(outputStatsdExt OutputStatsdExt) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputStatsdExt

	return CreateOutputRequest{
		OutputStatsdExt: &outputStatsdExt,
		Type:            typ,
	}
}

func CreateCreateOutputRequestOutputGraphite(outputGraphite OutputGraphite) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputGraphite

	return CreateOutputRequest{
		OutputGraphite: &outputGraphite,
		Type:           typ,
	}
}

func CreateCreateOutputRequestOutputRouter(outputRouter OutputRouter) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputRouter

	return CreateOutputRequest{
		OutputRouter: &outputRouter,
		Type:         typ,
	}
}

func CreateCreateOutputRequestOutputSns(outputSns OutputSns) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputSns

	return CreateOutputRequest{
		OutputSns: &outputSns,
		Type:      typ,
	}
}

func CreateCreateOutputRequestOutputSqs(outputSqs OutputSqs) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputSqs

	return CreateOutputRequest{
		OutputSqs: &outputSqs,
		Type:      typ,
	}
}

func CreateCreateOutputRequestOutputSnmp(outputSnmp OutputSnmp) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputSnmp

	return CreateOutputRequest{
		OutputSnmp: &outputSnmp,
		Type:       typ,
	}
}

func CreateCreateOutputRequestOutputSumoLogic(outputSumoLogic OutputSumoLogic) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputSumoLogic

	return CreateOutputRequest{
		OutputSumoLogic: &outputSumoLogic,
		Type:            typ,
	}
}

func CreateCreateOutputRequestOutputDatadog(outputDatadog components.OutputDatadog) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputDatadog

	return CreateOutputRequest{
		OutputDatadog: &outputDatadog,
		Type:          typ,
	}
}

func CreateCreateOutputRequestOutputGrafanaCloud(outputGrafanaCloud components.OutputGrafanaCloud) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputGrafanaCloud

	return CreateOutputRequest{
		OutputGrafanaCloud: &outputGrafanaCloud,
		Type:               typ,
	}
}

func CreateCreateOutputRequestOutputLoki(outputLoki OutputLoki) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputLoki

	return CreateOutputRequest{
		OutputLoki: &outputLoki,
		Type:       typ,
	}
}

func CreateCreateOutputRequestOutputPrometheus(outputPrometheus OutputPrometheus) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputPrometheus

	return CreateOutputRequest{
		OutputPrometheus: &outputPrometheus,
		Type:             typ,
	}
}

func CreateCreateOutputRequestOutputRing(outputRing components.OutputRing) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputRing

	return CreateOutputRequest{
		OutputRing: &outputRing,
		Type:       typ,
	}
}

func CreateCreateOutputRequestOutputOpenTelemetry(outputOpenTelemetry OutputOpenTelemetry) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputOpenTelemetry

	return CreateOutputRequest{
		OutputOpenTelemetry: &outputOpenTelemetry,
		Type:                typ,
	}
}

func CreateCreateOutputRequestOutputServiceNow(outputServiceNow OutputServiceNow) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputServiceNow

	return CreateOutputRequest{
		OutputServiceNow: &outputServiceNow,
		Type:             typ,
	}
}

func CreateCreateOutputRequestOutputDataset(outputDataset components.OutputDataset) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputDataset

	return CreateOutputRequest{
		OutputDataset: &outputDataset,
		Type:          typ,
	}
}

func CreateCreateOutputRequestOutputCriblTCP(outputCriblTCP components.OutputCriblTCP) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputCriblTCP

	return CreateOutputRequest{
		OutputCriblTCP: &outputCriblTCP,
		Type:           typ,
	}
}

func CreateCreateOutputRequestOutputCriblHTTP(outputCriblHTTP components.OutputCriblHTTP) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputCriblHTTP

	return CreateOutputRequest{
		OutputCriblHTTP: &outputCriblHTTP,
		Type:            typ,
	}
}

func CreateCreateOutputRequestOutputHumioHec(outputHumioHec OutputHumioHec) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputHumioHec

	return CreateOutputRequest{
		OutputHumioHec: &outputHumioHec,
		Type:           typ,
	}
}

func CreateCreateOutputRequestOutputCrowdstrikeNextGenSiem(outputCrowdstrikeNextGenSiem OutputCrowdstrikeNextGenSiem) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputCrowdstrikeNextGenSiem

	return CreateOutputRequest{
		OutputCrowdstrikeNextGenSiem: &outputCrowdstrikeNextGenSiem,
		Type:                         typ,
	}
}

func CreateCreateOutputRequestOutputDlS3(outputDlS3 OutputDlS3) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputDlS3

	return CreateOutputRequest{
		OutputDlS3: &outputDlS3,
		Type:       typ,
	}
}

func CreateCreateOutputRequestOutputSecurityLake(outputSecurityLake OutputSecurityLake) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputSecurityLake

	return CreateOutputRequest{
		OutputSecurityLake: &outputSecurityLake,
		Type:               typ,
	}
}

func CreateCreateOutputRequestOutputCriblLake(outputCriblLake components.OutputCriblLake) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputCriblLake

	return CreateOutputRequest{
		OutputCriblLake: &outputCriblLake,
		Type:            typ,
	}
}

func CreateCreateOutputRequestOutputDiskSpool(outputDiskSpool components.OutputDiskSpool) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputDiskSpool

	return CreateOutputRequest{
		OutputDiskSpool: &outputDiskSpool,
		Type:            typ,
	}
}

func CreateCreateOutputRequestOutputClickHouse(outputClickHouse OutputClickHouse) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputClickHouse

	return CreateOutputRequest{
		OutputClickHouse: &outputClickHouse,
		Type:             typ,
	}
}

func CreateCreateOutputRequestOutputXsiam(outputXsiam components.OutputXsiam) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputXsiam

	return CreateOutputRequest{
		OutputXsiam: &outputXsiam,
		Type:        typ,
	}
}

func CreateCreateOutputRequestOutputNetflow(outputNetflow OutputNetflow) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputNetflow

	return CreateOutputRequest{
		OutputNetflow: &outputNetflow,
		Type:          typ,
	}
}

func CreateCreateOutputRequestOutputDynatraceHTTP(outputDynatraceHTTP OutputDynatraceHTTP) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputDynatraceHTTP

	return CreateOutputRequest{
		OutputDynatraceHTTP: &outputDynatraceHTTP,
		Type:                typ,
	}
}

func CreateCreateOutputRequestOutputDynatraceOtlp(outputDynatraceOtlp OutputDynatraceOtlp) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputDynatraceOtlp

	return CreateOutputRequest{
		OutputDynatraceOtlp: &outputDynatraceOtlp,
		Type:                typ,
	}
}

func CreateCreateOutputRequestOutputSentinelOneAiSiem(outputSentinelOneAiSiem OutputSentinelOneAiSiem) CreateOutputRequest {
	typ := CreateOutputRequestTypeOutputSentinelOneAiSiem

	return CreateOutputRequest{
		OutputSentinelOneAiSiem: &outputSentinelOneAiSiem,
		Type:                    typ,
	}
}

func (u *CreateOutputRequest) UnmarshalJSON(data []byte) error {

	var outputDevnull components.OutputDevnull = components.OutputDevnull{}
	if err := utils.UnmarshalJSON(data, &outputDevnull, "", true, true); err == nil {
		u.OutputDevnull = &outputDevnull
		u.Type = CreateOutputRequestTypeOutputDevnull
		return nil
	}

	var outputDefault OutputDefault = OutputDefault{}
	if err := utils.UnmarshalJSON(data, &outputDefault, "", true, true); err == nil {
		u.OutputDefault = &outputDefault
		u.Type = CreateOutputRequestTypeOutputDefault
		return nil
	}

	var outputRouter OutputRouter = OutputRouter{}
	if err := utils.UnmarshalJSON(data, &outputRouter, "", true, true); err == nil {
		u.OutputRouter = &outputRouter
		u.Type = CreateOutputRequestTypeOutputRouter
		return nil
	}

	var outputSnmp OutputSnmp = OutputSnmp{}
	if err := utils.UnmarshalJSON(data, &outputSnmp, "", true, true); err == nil {
		u.OutputSnmp = &outputSnmp
		u.Type = CreateOutputRequestTypeOutputSnmp
		return nil
	}

	var outputNetflow OutputNetflow = OutputNetflow{}
	if err := utils.UnmarshalJSON(data, &outputNetflow, "", true, true); err == nil {
		u.OutputNetflow = &outputNetflow
		u.Type = CreateOutputRequestTypeOutputNetflow
		return nil
	}

	var outputDiskSpool components.OutputDiskSpool = components.OutputDiskSpool{}
	if err := utils.UnmarshalJSON(data, &outputDiskSpool, "", true, true); err == nil {
		u.OutputDiskSpool = &outputDiskSpool
		u.Type = CreateOutputRequestTypeOutputDiskSpool
		return nil
	}

	var outputRing components.OutputRing = components.OutputRing{}
	if err := utils.UnmarshalJSON(data, &outputRing, "", true, true); err == nil {
		u.OutputRing = &outputRing
		u.Type = CreateOutputRequestTypeOutputRing
		return nil
	}

	var outputGraphite OutputGraphite = OutputGraphite{}
	if err := utils.UnmarshalJSON(data, &outputGraphite, "", true, true); err == nil {
		u.OutputGraphite = &outputGraphite
		u.Type = CreateOutputRequestTypeOutputGraphite
		return nil
	}

	var outputStatsd OutputStatsd = OutputStatsd{}
	if err := utils.UnmarshalJSON(data, &outputStatsd, "", true, true); err == nil {
		u.OutputStatsd = &outputStatsd
		u.Type = CreateOutputRequestTypeOutputStatsd
		return nil
	}

	var outputStatsdExt OutputStatsdExt = OutputStatsdExt{}
	if err := utils.UnmarshalJSON(data, &outputStatsdExt, "", true, true); err == nil {
		u.OutputStatsdExt = &outputStatsdExt
		u.Type = CreateOutputRequestTypeOutputStatsdExt
		return nil
	}

	var outputGooglePubsub OutputGooglePubsub = OutputGooglePubsub{}
	if err := utils.UnmarshalJSON(data, &outputGooglePubsub, "", true, true); err == nil {
		u.OutputGooglePubsub = &outputGooglePubsub
		u.Type = CreateOutputRequestTypeOutputGooglePubsub
		return nil
	}

	var outputCriblTCP components.OutputCriblTCP = components.OutputCriblTCP{}
	if err := utils.UnmarshalJSON(data, &outputCriblTCP, "", true, true); err == nil {
		u.OutputCriblTCP = &outputCriblTCP
		u.Type = CreateOutputRequestTypeOutputCriblTCP
		return nil
	}

	var outputSplunk OutputSplunk = OutputSplunk{}
	if err := utils.UnmarshalJSON(data, &outputSplunk, "", true, true); err == nil {
		u.OutputSplunk = &outputSplunk
		u.Type = CreateOutputRequestTypeOutputSplunk
		return nil
	}

	var outputSns OutputSns = OutputSns{}
	if err := utils.UnmarshalJSON(data, &outputSns, "", true, true); err == nil {
		u.OutputSns = &outputSns
		u.Type = CreateOutputRequestTypeOutputSns
		return nil
	}

	var outputCloudwatch OutputCloudwatch = OutputCloudwatch{}
	if err := utils.UnmarshalJSON(data, &outputCloudwatch, "", true, true); err == nil {
		u.OutputCloudwatch = &outputCloudwatch
		u.Type = CreateOutputRequestTypeOutputCloudwatch
		return nil
	}

	var outputSyslog components.OutputSyslog = components.OutputSyslog{}
	if err := utils.UnmarshalJSON(data, &outputSyslog, "", true, true); err == nil {
		u.OutputSyslog = &outputSyslog
		u.Type = CreateOutputRequestTypeOutputSyslog
		return nil
	}

	var outputAzureEventhub OutputAzureEventhub = OutputAzureEventhub{}
	if err := utils.UnmarshalJSON(data, &outputAzureEventhub, "", true, true); err == nil {
		u.OutputAzureEventhub = &outputAzureEventhub
		u.Type = CreateOutputRequestTypeOutputAzureEventhub
		return nil
	}

	var outputWavefront OutputWavefront = OutputWavefront{}
	if err := utils.UnmarshalJSON(data, &outputWavefront, "", true, true); err == nil {
		u.OutputWavefront = &outputWavefront
		u.Type = CreateOutputRequestTypeOutputWavefront
		return nil
	}

	var outputSignalfx OutputSignalfx = OutputSignalfx{}
	if err := utils.UnmarshalJSON(data, &outputSignalfx, "", true, true); err == nil {
		u.OutputSignalfx = &outputSignalfx
		u.Type = CreateOutputRequestTypeOutputSignalfx
		return nil
	}

	var outputHoneycomb OutputHoneycomb = OutputHoneycomb{}
	if err := utils.UnmarshalJSON(data, &outputHoneycomb, "", true, true); err == nil {
		u.OutputHoneycomb = &outputHoneycomb
		u.Type = CreateOutputRequestTypeOutputHoneycomb
		return nil
	}

	var outputTcpjson components.OutputTcpjson = components.OutputTcpjson{}
	if err := utils.UnmarshalJSON(data, &outputTcpjson, "", true, true); err == nil {
		u.OutputTcpjson = &outputTcpjson
		u.Type = CreateOutputRequestTypeOutputTcpjson
		return nil
	}

	var outputSumoLogic OutputSumoLogic = OutputSumoLogic{}
	if err := utils.UnmarshalJSON(data, &outputSumoLogic, "", true, true); err == nil {
		u.OutputSumoLogic = &outputSumoLogic
		u.Type = CreateOutputRequestTypeOutputSumoLogic
		return nil
	}

	var outputHumioHec OutputHumioHec = OutputHumioHec{}
	if err := utils.UnmarshalJSON(data, &outputHumioHec, "", true, true); err == nil {
		u.OutputHumioHec = &outputHumioHec
		u.Type = CreateOutputRequestTypeOutputHumioHec
		return nil
	}

	var outputElasticCloud OutputElasticCloud = OutputElasticCloud{}
	if err := utils.UnmarshalJSON(data, &outputElasticCloud, "", true, true); err == nil {
		u.OutputElasticCloud = &outputElasticCloud
		u.Type = CreateOutputRequestTypeOutputElasticCloud
		return nil
	}

	var outputCrowdstrikeNextGenSiem OutputCrowdstrikeNextGenSiem = OutputCrowdstrikeNextGenSiem{}
	if err := utils.UnmarshalJSON(data, &outputCrowdstrikeNextGenSiem, "", true, true); err == nil {
		u.OutputCrowdstrikeNextGenSiem = &outputCrowdstrikeNextGenSiem
		u.Type = CreateOutputRequestTypeOutputCrowdstrikeNextGenSiem
		return nil
	}

	var outputKinesis OutputKinesis = OutputKinesis{}
	if err := utils.UnmarshalJSON(data, &outputKinesis, "", true, true); err == nil {
		u.OutputKinesis = &outputKinesis
		u.Type = CreateOutputRequestTypeOutputKinesis
		return nil
	}

	var outputConfluentCloud OutputConfluentCloud = OutputConfluentCloud{}
	if err := utils.UnmarshalJSON(data, &outputConfluentCloud, "", true, true); err == nil {
		u.OutputConfluentCloud = &outputConfluentCloud
		u.Type = CreateOutputRequestTypeOutputConfluentCloud
		return nil
	}

	var outputKafka OutputKafka = OutputKafka{}
	if err := utils.UnmarshalJSON(data, &outputKafka, "", true, true); err == nil {
		u.OutputKafka = &outputKafka
		u.Type = CreateOutputRequestTypeOutputKafka
		return nil
	}

	var outputExabeam OutputExabeam = OutputExabeam{}
	if err := utils.UnmarshalJSON(data, &outputExabeam, "", true, true); err == nil {
		u.OutputExabeam = &outputExabeam
		u.Type = CreateOutputRequestTypeOutputExabeam
		return nil
	}

	var outputNewrelicEvents OutputNewrelicEvents = OutputNewrelicEvents{}
	if err := utils.UnmarshalJSON(data, &outputNewrelicEvents, "", true, true); err == nil {
		u.OutputNewrelicEvents = &outputNewrelicEvents
		u.Type = CreateOutputRequestTypeOutputNewrelicEvents
		return nil
	}

	var outputAzureLogs OutputAzureLogs = OutputAzureLogs{}
	if err := utils.UnmarshalJSON(data, &outputAzureLogs, "", true, true); err == nil {
		u.OutputAzureLogs = &outputAzureLogs
		u.Type = CreateOutputRequestTypeOutputAzureLogs
		return nil
	}

	var outputSplunkLb OutputSplunkLb = OutputSplunkLb{}
	if err := utils.UnmarshalJSON(data, &outputSplunkLb, "", true, true); err == nil {
		u.OutputSplunkLb = &outputSplunkLb
		u.Type = CreateOutputRequestTypeOutputSplunkLb
		return nil
	}

	var outputSqs OutputSqs = OutputSqs{}
	if err := utils.UnmarshalJSON(data, &outputSqs, "", true, true); err == nil {
		u.OutputSqs = &outputSqs
		u.Type = CreateOutputRequestTypeOutputSqs
		return nil
	}

	var outputNewrelic components.OutputNewrelic = components.OutputNewrelic{}
	if err := utils.UnmarshalJSON(data, &outputNewrelic, "", true, true); err == nil {
		u.OutputNewrelic = &outputNewrelic
		u.Type = CreateOutputRequestTypeOutputNewrelic
		return nil
	}

	var outputCriblHTTP components.OutputCriblHTTP = components.OutputCriblHTTP{}
	if err := utils.UnmarshalJSON(data, &outputCriblHTTP, "", true, true); err == nil {
		u.OutputCriblHTTP = &outputCriblHTTP
		u.Type = CreateOutputRequestTypeOutputCriblHTTP
		return nil
	}

	var outputXsiam components.OutputXsiam = components.OutputXsiam{}
	if err := utils.UnmarshalJSON(data, &outputXsiam, "", true, true); err == nil {
		u.OutputXsiam = &outputXsiam
		u.Type = CreateOutputRequestTypeOutputXsiam
		return nil
	}

	var outputFilesystem OutputFilesystem = OutputFilesystem{}
	if err := utils.UnmarshalJSON(data, &outputFilesystem, "", true, true); err == nil {
		u.OutputFilesystem = &outputFilesystem
		u.Type = CreateOutputRequestTypeOutputFilesystem
		return nil
	}

	var outputDataset components.OutputDataset = components.OutputDataset{}
	if err := utils.UnmarshalJSON(data, &outputDataset, "", true, true); err == nil {
		u.OutputDataset = &outputDataset
		u.Type = CreateOutputRequestTypeOutputDataset
		return nil
	}

	var outputLoki OutputLoki = OutputLoki{}
	if err := utils.UnmarshalJSON(data, &outputLoki, "", true, true); err == nil {
		u.OutputLoki = &outputLoki
		u.Type = CreateOutputRequestTypeOutputLoki
		return nil
	}

	var outputSplunkHec components.OutputSplunkHec = components.OutputSplunkHec{}
	if err := utils.UnmarshalJSON(data, &outputSplunkHec, "", true, true); err == nil {
		u.OutputSplunkHec = &outputSplunkHec
		u.Type = CreateOutputRequestTypeOutputSplunkHec
		return nil
	}

	var outputDynatraceHTTP OutputDynatraceHTTP = OutputDynatraceHTTP{}
	if err := utils.UnmarshalJSON(data, &outputDynatraceHTTP, "", true, true); err == nil {
		u.OutputDynatraceHTTP = &outputDynatraceHTTP
		u.Type = CreateOutputRequestTypeOutputDynatraceHTTP
		return nil
	}

	var outputServiceNow OutputServiceNow = OutputServiceNow{}
	if err := utils.UnmarshalJSON(data, &outputServiceNow, "", true, true); err == nil {
		u.OutputServiceNow = &outputServiceNow
		u.Type = CreateOutputRequestTypeOutputServiceNow
		return nil
	}

	var outputDynatraceOtlp OutputDynatraceOtlp = OutputDynatraceOtlp{}
	if err := utils.UnmarshalJSON(data, &outputDynatraceOtlp, "", true, true); err == nil {
		u.OutputDynatraceOtlp = &outputDynatraceOtlp
		u.Type = CreateOutputRequestTypeOutputDynatraceOtlp
		return nil
	}

	var outputElastic OutputElastic = OutputElastic{}
	if err := utils.UnmarshalJSON(data, &outputElastic, "", true, true); err == nil {
		u.OutputElastic = &outputElastic
		u.Type = CreateOutputRequestTypeOutputElastic
		return nil
	}

	var outputGoogleChronicle OutputGoogleChronicle = OutputGoogleChronicle{}
	if err := utils.UnmarshalJSON(data, &outputGoogleChronicle, "", true, true); err == nil {
		u.OutputGoogleChronicle = &outputGoogleChronicle
		u.Type = CreateOutputRequestTypeOutputGoogleChronicle
		return nil
	}

	var outputCriblLake components.OutputCriblLake = components.OutputCriblLake{}
	if err := utils.UnmarshalJSON(data, &outputCriblLake, "", true, true); err == nil {
		u.OutputCriblLake = &outputCriblLake
		u.Type = CreateOutputRequestTypeOutputCriblLake
		return nil
	}

	var outputDatadog components.OutputDatadog = components.OutputDatadog{}
	if err := utils.UnmarshalJSON(data, &outputDatadog, "", true, true); err == nil {
		u.OutputDatadog = &outputDatadog
		u.Type = CreateOutputRequestTypeOutputDatadog
		return nil
	}

	var outputPrometheus OutputPrometheus = OutputPrometheus{}
	if err := utils.UnmarshalJSON(data, &outputPrometheus, "", true, true); err == nil {
		u.OutputPrometheus = &outputPrometheus
		u.Type = CreateOutputRequestTypeOutputPrometheus
		return nil
	}

	var outputMsk OutputMsk = OutputMsk{}
	if err := utils.UnmarshalJSON(data, &outputMsk, "", true, true); err == nil {
		u.OutputMsk = &outputMsk
		u.Type = CreateOutputRequestTypeOutputMsk
		return nil
	}

	var outputSentinelOneAiSiem OutputSentinelOneAiSiem = OutputSentinelOneAiSiem{}
	if err := utils.UnmarshalJSON(data, &outputSentinelOneAiSiem, "", true, true); err == nil {
		u.OutputSentinelOneAiSiem = &outputSentinelOneAiSiem
		u.Type = CreateOutputRequestTypeOutputSentinelOneAiSiem
		return nil
	}

	var outputSentinel OutputSentinel = OutputSentinel{}
	if err := utils.UnmarshalJSON(data, &outputSentinel, "", true, true); err == nil {
		u.OutputSentinel = &outputSentinel
		u.Type = CreateOutputRequestTypeOutputSentinel
		return nil
	}

	var outputInfluxdb OutputInfluxdb = OutputInfluxdb{}
	if err := utils.UnmarshalJSON(data, &outputInfluxdb, "", true, true); err == nil {
		u.OutputInfluxdb = &outputInfluxdb
		u.Type = CreateOutputRequestTypeOutputInfluxdb
		return nil
	}

	var outputGoogleCloudStorage OutputGoogleCloudStorage = OutputGoogleCloudStorage{}
	if err := utils.UnmarshalJSON(data, &outputGoogleCloudStorage, "", true, true); err == nil {
		u.OutputGoogleCloudStorage = &outputGoogleCloudStorage
		u.Type = CreateOutputRequestTypeOutputGoogleCloudStorage
		return nil
	}

	var outputAzureBlob OutputAzureBlob = OutputAzureBlob{}
	if err := utils.UnmarshalJSON(data, &outputAzureBlob, "", true, true); err == nil {
		u.OutputAzureBlob = &outputAzureBlob
		u.Type = CreateOutputRequestTypeOutputAzureBlob
		return nil
	}

	var outputOpenTelemetry OutputOpenTelemetry = OutputOpenTelemetry{}
	if err := utils.UnmarshalJSON(data, &outputOpenTelemetry, "", true, true); err == nil {
		u.OutputOpenTelemetry = &outputOpenTelemetry
		u.Type = CreateOutputRequestTypeOutputOpenTelemetry
		return nil
	}

	var outputMinio OutputMinio = OutputMinio{}
	if err := utils.UnmarshalJSON(data, &outputMinio, "", true, true); err == nil {
		u.OutputMinio = &outputMinio
		u.Type = CreateOutputRequestTypeOutputMinio
		return nil
	}

	var outputClickHouse OutputClickHouse = OutputClickHouse{}
	if err := utils.UnmarshalJSON(data, &outputClickHouse, "", true, true); err == nil {
		u.OutputClickHouse = &outputClickHouse
		u.Type = CreateOutputRequestTypeOutputClickHouse
		return nil
	}

	var outputSecurityLake OutputSecurityLake = OutputSecurityLake{}
	if err := utils.UnmarshalJSON(data, &outputSecurityLake, "", true, true); err == nil {
		u.OutputSecurityLake = &outputSecurityLake
		u.Type = CreateOutputRequestTypeOutputSecurityLake
		return nil
	}

	var outputDlS3 OutputDlS3 = OutputDlS3{}
	if err := utils.UnmarshalJSON(data, &outputDlS3, "", true, true); err == nil {
		u.OutputDlS3 = &outputDlS3
		u.Type = CreateOutputRequestTypeOutputDlS3
		return nil
	}

	var outputS3 OutputS3 = OutputS3{}
	if err := utils.UnmarshalJSON(data, &outputS3, "", true, true); err == nil {
		u.OutputS3 = &outputS3
		u.Type = CreateOutputRequestTypeOutputS3
		return nil
	}

	var outputAzureDataExplorer OutputAzureDataExplorer = OutputAzureDataExplorer{}
	if err := utils.UnmarshalJSON(data, &outputAzureDataExplorer, "", true, true); err == nil {
		u.OutputAzureDataExplorer = &outputAzureDataExplorer
		u.Type = CreateOutputRequestTypeOutputAzureDataExplorer
		return nil
	}

	var outputWebhook components.OutputWebhook = components.OutputWebhook{}
	if err := utils.UnmarshalJSON(data, &outputWebhook, "", true, true); err == nil {
		u.OutputWebhook = &outputWebhook
		u.Type = CreateOutputRequestTypeOutputWebhook
		return nil
	}

	var outputGoogleCloudLogging OutputGoogleCloudLogging = OutputGoogleCloudLogging{}
	if err := utils.UnmarshalJSON(data, &outputGoogleCloudLogging, "", true, true); err == nil {
		u.OutputGoogleCloudLogging = &outputGoogleCloudLogging
		u.Type = CreateOutputRequestTypeOutputGoogleCloudLogging
		return nil
	}

	var outputGrafanaCloud components.OutputGrafanaCloud = components.OutputGrafanaCloud{}
	if err := utils.UnmarshalJSON(data, &outputGrafanaCloud, "", true, true); err == nil {
		u.OutputGrafanaCloud = &outputGrafanaCloud
		u.Type = CreateOutputRequestTypeOutputGrafanaCloud
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for CreateOutputRequest", string(data))
}

func (u CreateOutputRequest) MarshalJSON() ([]byte, error) {
	if u.OutputDefault != nil {
		return utils.MarshalJSON(u.OutputDefault, "", true)
	}

	if u.OutputWebhook != nil {
		return utils.MarshalJSON(u.OutputWebhook, "", true)
	}

	if u.OutputSentinel != nil {
		return utils.MarshalJSON(u.OutputSentinel, "", true)
	}

	if u.OutputDevnull != nil {
		return utils.MarshalJSON(u.OutputDevnull, "", true)
	}

	if u.OutputSyslog != nil {
		return utils.MarshalJSON(u.OutputSyslog, "", true)
	}

	if u.OutputSplunk != nil {
		return utils.MarshalJSON(u.OutputSplunk, "", true)
	}

	if u.OutputSplunkLb != nil {
		return utils.MarshalJSON(u.OutputSplunkLb, "", true)
	}

	if u.OutputSplunkHec != nil {
		return utils.MarshalJSON(u.OutputSplunkHec, "", true)
	}

	if u.OutputTcpjson != nil {
		return utils.MarshalJSON(u.OutputTcpjson, "", true)
	}

	if u.OutputWavefront != nil {
		return utils.MarshalJSON(u.OutputWavefront, "", true)
	}

	if u.OutputSignalfx != nil {
		return utils.MarshalJSON(u.OutputSignalfx, "", true)
	}

	if u.OutputFilesystem != nil {
		return utils.MarshalJSON(u.OutputFilesystem, "", true)
	}

	if u.OutputS3 != nil {
		return utils.MarshalJSON(u.OutputS3, "", true)
	}

	if u.OutputAzureBlob != nil {
		return utils.MarshalJSON(u.OutputAzureBlob, "", true)
	}

	if u.OutputAzureDataExplorer != nil {
		return utils.MarshalJSON(u.OutputAzureDataExplorer, "", true)
	}

	if u.OutputAzureLogs != nil {
		return utils.MarshalJSON(u.OutputAzureLogs, "", true)
	}

	if u.OutputKinesis != nil {
		return utils.MarshalJSON(u.OutputKinesis, "", true)
	}

	if u.OutputHoneycomb != nil {
		return utils.MarshalJSON(u.OutputHoneycomb, "", true)
	}

	if u.OutputAzureEventhub != nil {
		return utils.MarshalJSON(u.OutputAzureEventhub, "", true)
	}

	if u.OutputGoogleChronicle != nil {
		return utils.MarshalJSON(u.OutputGoogleChronicle, "", true)
	}

	if u.OutputGoogleCloudStorage != nil {
		return utils.MarshalJSON(u.OutputGoogleCloudStorage, "", true)
	}

	if u.OutputGoogleCloudLogging != nil {
		return utils.MarshalJSON(u.OutputGoogleCloudLogging, "", true)
	}

	if u.OutputGooglePubsub != nil {
		return utils.MarshalJSON(u.OutputGooglePubsub, "", true)
	}

	if u.OutputExabeam != nil {
		return utils.MarshalJSON(u.OutputExabeam, "", true)
	}

	if u.OutputKafka != nil {
		return utils.MarshalJSON(u.OutputKafka, "", true)
	}

	if u.OutputConfluentCloud != nil {
		return utils.MarshalJSON(u.OutputConfluentCloud, "", true)
	}

	if u.OutputMsk != nil {
		return utils.MarshalJSON(u.OutputMsk, "", true)
	}

	if u.OutputElastic != nil {
		return utils.MarshalJSON(u.OutputElastic, "", true)
	}

	if u.OutputElasticCloud != nil {
		return utils.MarshalJSON(u.OutputElasticCloud, "", true)
	}

	if u.OutputNewrelic != nil {
		return utils.MarshalJSON(u.OutputNewrelic, "", true)
	}

	if u.OutputNewrelicEvents != nil {
		return utils.MarshalJSON(u.OutputNewrelicEvents, "", true)
	}

	if u.OutputInfluxdb != nil {
		return utils.MarshalJSON(u.OutputInfluxdb, "", true)
	}

	if u.OutputCloudwatch != nil {
		return utils.MarshalJSON(u.OutputCloudwatch, "", true)
	}

	if u.OutputMinio != nil {
		return utils.MarshalJSON(u.OutputMinio, "", true)
	}

	if u.OutputStatsd != nil {
		return utils.MarshalJSON(u.OutputStatsd, "", true)
	}

	if u.OutputStatsdExt != nil {
		return utils.MarshalJSON(u.OutputStatsdExt, "", true)
	}

	if u.OutputGraphite != nil {
		return utils.MarshalJSON(u.OutputGraphite, "", true)
	}

	if u.OutputRouter != nil {
		return utils.MarshalJSON(u.OutputRouter, "", true)
	}

	if u.OutputSns != nil {
		return utils.MarshalJSON(u.OutputSns, "", true)
	}

	if u.OutputSqs != nil {
		return utils.MarshalJSON(u.OutputSqs, "", true)
	}

	if u.OutputSnmp != nil {
		return utils.MarshalJSON(u.OutputSnmp, "", true)
	}

	if u.OutputSumoLogic != nil {
		return utils.MarshalJSON(u.OutputSumoLogic, "", true)
	}

	if u.OutputDatadog != nil {
		return utils.MarshalJSON(u.OutputDatadog, "", true)
	}

	if u.OutputGrafanaCloud != nil {
		return utils.MarshalJSON(u.OutputGrafanaCloud, "", true)
	}

	if u.OutputLoki != nil {
		return utils.MarshalJSON(u.OutputLoki, "", true)
	}

	if u.OutputPrometheus != nil {
		return utils.MarshalJSON(u.OutputPrometheus, "", true)
	}

	if u.OutputRing != nil {
		return utils.MarshalJSON(u.OutputRing, "", true)
	}

	if u.OutputOpenTelemetry != nil {
		return utils.MarshalJSON(u.OutputOpenTelemetry, "", true)
	}

	if u.OutputServiceNow != nil {
		return utils.MarshalJSON(u.OutputServiceNow, "", true)
	}

	if u.OutputDataset != nil {
		return utils.MarshalJSON(u.OutputDataset, "", true)
	}

	if u.OutputCriblTCP != nil {
		return utils.MarshalJSON(u.OutputCriblTCP, "", true)
	}

	if u.OutputCriblHTTP != nil {
		return utils.MarshalJSON(u.OutputCriblHTTP, "", true)
	}

	if u.OutputHumioHec != nil {
		return utils.MarshalJSON(u.OutputHumioHec, "", true)
	}

	if u.OutputCrowdstrikeNextGenSiem != nil {
		return utils.MarshalJSON(u.OutputCrowdstrikeNextGenSiem, "", true)
	}

	if u.OutputDlS3 != nil {
		return utils.MarshalJSON(u.OutputDlS3, "", true)
	}

	if u.OutputSecurityLake != nil {
		return utils.MarshalJSON(u.OutputSecurityLake, "", true)
	}

	if u.OutputCriblLake != nil {
		return utils.MarshalJSON(u.OutputCriblLake, "", true)
	}

	if u.OutputDiskSpool != nil {
		return utils.MarshalJSON(u.OutputDiskSpool, "", true)
	}

	if u.OutputClickHouse != nil {
		return utils.MarshalJSON(u.OutputClickHouse, "", true)
	}

	if u.OutputXsiam != nil {
		return utils.MarshalJSON(u.OutputXsiam, "", true)
	}

	if u.OutputNetflow != nil {
		return utils.MarshalJSON(u.OutputNetflow, "", true)
	}

	if u.OutputDynatraceHTTP != nil {
		return utils.MarshalJSON(u.OutputDynatraceHTTP, "", true)
	}

	if u.OutputDynatraceOtlp != nil {
		return utils.MarshalJSON(u.OutputDynatraceOtlp, "", true)
	}

	if u.OutputSentinelOneAiSiem != nil {
		return utils.MarshalJSON(u.OutputSentinelOneAiSiem, "", true)
	}

	return nil, errors.New("could not marshal union type CreateOutputRequest: all fields are null")
}

// CreateOutputResponseBody - a list of Destination objects
type CreateOutputResponseBody struct {
	// number of items present in the items array
	Count *int64              `json:"count,omitempty"`
	Items []components.Output `json:"items,omitempty"`
}

func (o *CreateOutputResponseBody) GetCount() *int64 {
	if o == nil {
		return nil
	}
	return o.Count
}

func (o *CreateOutputResponseBody) GetItems() []components.Output {
	if o == nil {
		return nil
	}
	return o.Items
}

type CreateOutputResponse struct {
	HTTPMeta components.HTTPMetadata `json:"-"`
	// a list of Destination objects
	Object *CreateOutputResponseBody
}

func (o *CreateOutputResponse) GetHTTPMeta() components.HTTPMetadata {
	if o == nil {
		return components.HTTPMetadata{}
	}
	return o.HTTPMeta
}

func (o *CreateOutputResponse) GetObject() *CreateOutputResponseBody {
	if o == nil {
		return nil
	}
	return o.Object
}
